{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 2: Decision trees and machine learning fundamentals \n",
    "### Associated lectures: [Lectures 2 and 3](https://ubc-cs.github.io/cpsc330/README.html) \n",
    "\n",
    "**Due date: Monday Sep 20, 2021 at 11:59pm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "rubric={points:3}\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330/blob/master/docs/homework_instructions.md). In particular, **see the note about not pushing downloaded data to your repo**.\n",
    "\n",
    "You are welcome to broadly discuss questions with your classmates but your final answers must be your own. **We are not allowing group submission for this homework assignment.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the data set\n",
    " \n",
    "For this  assignment you'll be looking at Kaggle's [Spotify Song Attributes](https://www.kaggle.com/geomack/spotifyclassification/) dataset.\n",
    "The dataset contains a number of features of songs from 2017 and a binary variable `target` that represents whether the user liked the song (encoded as 1) or not (encoded as 0). See the documentation of all the features [here](https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/). \n",
    "\n",
    "This dataset is publicly available on Kaggle, and you will have to download it yourself. Follow the steps below to get the data CSV. \n",
    "\n",
    "1. If you do not have an account with [Kaggle](https://www.kaggle.com/), you will first need to create one (it's free).\n",
    "2. Login to your account and [download](https://www.kaggle.com/geomack/spotifyclassification/download) the dataset.\n",
    "3. Unzip the data file if needed, then rename it to `spotify.csv`, and move it to the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d4d478b6cdc9bf88",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### 1(a) \n",
    "rubric={points:2}\n",
    "\n",
    "Read in the data CSV and store it as a pandas dataframe named `spotify_df`. The first column of the .csv file should be set as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4f3f14b59fd7e6b8",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "spotify_df=pd.read_csv('data/spotify.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1(b)\n",
    "rubric={points:2}\n",
    "\n",
    "Run the following line of code to split the data. How many training and test examples do we have?\n",
    "\n",
    "> Note: we are setting the `random_state` so that everyone has the same split on their assignments. This will make it easier for the TAs to grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples (in rows): 1613 \n",
      "Test examples (in rows): 404\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(spotify_df, test_size=0.2, random_state=321)\n",
    "print(\"Training examples (in rows):\", len(df_train), \"\\nTest examples (in rows):\", len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1(c)\n",
    "rubric={points:3}\n",
    "\n",
    "- Print out the output of `describe()` **on the training split**. This will compute some summary statistics of the numeric columns.\n",
    "- Which feature has the smallest range? \n",
    "\n",
    "> Hint: You can subtract the min value from the max value of the column to get the range.\n",
    "\n",
    "Note that `describe` returns another DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       acousticness  danceability   duration_ms       energy  instrumentalness          key     liveness     loudness         mode  speechiness        tempo  time_signature      valence       target\n",
      "count   1613.000000   1613.000000  1.613000e+03  1613.000000       1613.000000  1613.000000  1613.000000  1613.000000  1613.000000  1613.000000  1613.000000     1613.000000  1613.000000  1613.000000\n",
      "mean       0.185067      0.620076  2.462533e+05     0.681315          0.134317     5.384377     0.191317    -7.095272     0.619343     0.092119   121.310311        3.975201     0.495891     0.512089\n",
      "std        0.255838      0.161152  8.056740e+04     0.206964          0.274217     3.653722     0.156071     3.678993     0.485699     0.088007    26.431574        0.247829     0.244267     0.500009\n",
      "min        0.000003      0.148000  1.604200e+04     0.015600          0.000000     0.000000     0.018800   -31.082000     0.000000     0.023100    47.859000        1.000000     0.037300     0.000000\n",
      "25%        0.010000      0.514000  1.999040e+05     0.564000          0.000000     2.000000     0.092200    -8.388000     0.000000     0.037500   100.221000        4.000000     0.298000     0.000000\n",
      "50%        0.062600      0.634000  2.298590e+05     0.712000          0.000088     6.000000     0.125000    -6.298000     1.000000     0.055300   121.241000        4.000000     0.492000     1.000000\n",
      "75%        0.260000      0.739000  2.703330e+05     0.841000          0.055900     9.000000     0.250000    -4.833000     1.000000     0.107000   136.894000        4.000000     0.689000     1.000000\n",
      "max        0.994000      0.984000  1.004627e+06     0.997000          0.976000    11.000000     0.969000    -0.718000     1.000000     0.622000   219.331000        5.000000     0.974000     1.000000\n",
      "\n",
      "Min range feature: speechiness\n"
     ]
    }
   ],
   "source": [
    "# citations for this question:\n",
    "# [1] iterating over columns in DataFrame: https://thispointer.com/pandas-loop-or-iterate-over-all-or-certain-columns-of-a-dataframe/\n",
    "\n",
    "df_train_describe = df_train.describe()\n",
    "print(df_train_describe.to_string())\n",
    "\n",
    "min_range = sys.maxsize\n",
    "min_range_feature = \"\"\n",
    "for (col_name, col_data) in df_train_describe.iteritems():  # [1]\n",
    "    curr_range = col_data[7] - col_data[3]\n",
    "    if curr_range < min_range:\n",
    "        min_range = curr_range\n",
    "        min_range_feature = col_name\n",
    "print(\"\\nMin range feature:\", min_range_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b33320bcf667584a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### 1(d) \n",
    "rubric={points:5}\n",
    "\n",
    "Let's focus on the following features:\n",
    "\n",
    "- danceability\n",
    "- tempo\n",
    "- energy\n",
    "- valence\n",
    "\n",
    "For each of these features (in order), produce a histogram that shows the distribution of the feature values in the training set, **separated for positive and negative examples**. \n",
    "By \"positive examples\" we mean target = 1 (user liked the song, positive sentiment) and by \"negative examples\" we mean target = 0 (used disliked the song, negative sentiment). As an example, here is what the histogram would look like for a different feature, loudness:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='loudness.png' width=\"400\">\n",
    "\n",
    "(You don't have to match all the details exactly, such as colour, but your histograms should look something like this, with a reasonable number of bins to see the shape of the distribution.) As shown above, there are two different histograms, one for target = 0 and one for target = 1, and they are overlaid on top of each other. The histogram above shows that extremely quiet songs tend to be disliked (more blue bars than orange on the left) and very loud songs also tend to be disliked (more blue than orange on the far right).\n",
    "\n",
    "To adhere to the [DRY (Don't Repeat Yourself)](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself) principle, make sure you use a `for` loop for your plotting, rather than repeating the plotting code 4 times. For this to work, I used `plt.show()` at the end of your loop, which draws the figure and resets the canvas for your next plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some code that separates out the dataset into positive and negative examples, to help you get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-6620b12eb0e6409ba13a69eec614f264\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-6620b12eb0e6409ba13a69eec614f264\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-6620b12eb0e6409ba13a69eec614f264\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"vconcat\": [{\"data\": {\"name\": \"data-2987e0c8c7dbebdb4e112953b8e41cc7\"}, \"mark\": {\"type\": \"area\", \"interpolate\": \"step\", \"opacity\": 0.5}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Feature target class\"}, \"x\": {\"type\": \"quantitative\", \"bin\": {\"maxbins\": 50}, \"field\": \"danceability\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"count\", \"stack\": null}}, \"title\": \"Histogram of danceability by target class\", \"transform\": [{\"fold\": [\"0\", \"1\"], \"as\": [\"Feature target class\", \"danceability\"]}]}, {\"data\": {\"name\": \"data-871d06e82555bd1fd3e3fd9b182c0c96\"}, \"mark\": {\"type\": \"area\", \"interpolate\": \"step\", \"opacity\": 0.5}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Feature target class\"}, \"x\": {\"type\": \"quantitative\", \"bin\": {\"maxbins\": 50}, \"field\": \"tempo\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"count\", \"stack\": null}}, \"title\": \"Histogram of tempo by target class\", \"transform\": [{\"fold\": [\"0\", \"1\"], \"as\": [\"Feature target class\", \"tempo\"]}]}, {\"data\": {\"name\": \"data-b63207ba6e5359e75461234396b8b8d8\"}, \"mark\": {\"type\": \"area\", \"interpolate\": \"step\", \"opacity\": 0.5}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Feature target class\"}, \"x\": {\"type\": \"quantitative\", \"bin\": {\"maxbins\": 50}, \"field\": \"energy\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"count\", \"stack\": null}}, \"title\": \"Histogram of energy by target class\", \"transform\": [{\"fold\": [\"0\", \"1\"], \"as\": [\"Feature target class\", \"energy\"]}]}, {\"data\": {\"name\": \"data-54e1b0c37e6a24880ae6994cca0f699a\"}, \"mark\": {\"type\": \"area\", \"interpolate\": \"step\", \"opacity\": 0.5}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Feature target class\"}, \"x\": {\"type\": \"quantitative\", \"bin\": {\"maxbins\": 50}, \"field\": \"valence\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"count\", \"stack\": null}}, \"title\": \"Histogram of valence by target class\", \"transform\": [{\"fold\": [\"0\", \"1\"], \"as\": [\"Feature target class\", \"valence\"]}]}], \"resolve\": {\"scale\": {\"color\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-2987e0c8c7dbebdb4e112953b8e41cc7\": [{\"0\": 0.459, \"1\": 0.932}, {\"0\": 0.482, \"1\": 0.746}, {\"0\": 0.673, \"1\": 0.731}, {\"0\": 0.592, \"1\": 0.411}, {\"0\": 0.52, \"1\": 0.873}, {\"0\": 0.217, \"1\": 0.575}, {\"0\": 0.385, \"1\": 0.363}, {\"0\": 0.48, \"1\": 0.541}, {\"0\": 0.529, \"1\": 0.711}, {\"0\": 0.242, \"1\": 0.555}, {\"0\": 0.227, \"1\": 0.662}, {\"0\": 0.295, \"1\": 0.857}, {\"0\": 0.754, \"1\": 0.42}, {\"0\": 0.804, \"1\": 0.727}, {\"0\": 0.483, \"1\": 0.582}, {\"0\": 0.85, \"1\": 0.68}, {\"0\": 0.612, \"1\": 0.402}, {\"0\": 0.502, \"1\": 0.766}, {\"0\": 0.588, \"1\": 0.879}, {\"0\": 0.859, \"1\": 0.839}, {\"0\": 0.639, \"1\": 0.577}, {\"0\": 0.404, \"1\": 0.712}, {\"0\": 0.51, \"1\": 0.276}, {\"0\": 0.699, \"1\": 0.691}, {\"0\": 0.596, \"1\": 0.284}, {\"0\": 0.776, \"1\": 0.811}, {\"0\": 0.607, \"1\": 0.699}, {\"0\": 0.762, \"1\": 0.736}, {\"0\": 0.772, \"1\": 0.367}, {\"0\": 0.663, \"1\": 0.351}, {\"0\": 0.434, \"1\": 0.827}, {\"0\": 0.398, \"1\": 0.806}, {\"0\": 0.56, \"1\": 0.352}, {\"0\": 0.55, \"1\": 0.684}, {\"0\": 0.519, \"1\": 0.515}, {\"0\": 0.761, \"1\": 0.777}, {\"0\": 0.766, \"1\": 0.451}, {\"0\": 0.589, \"1\": 0.864}, {\"0\": 0.32, \"1\": 0.688}, {\"0\": 0.432, \"1\": 0.737}, {\"0\": 0.657, \"1\": 0.854}, {\"0\": 0.5, \"1\": 0.682}, {\"0\": 0.888, \"1\": 0.772}, {\"0\": 0.842, \"1\": 0.906}, {\"0\": 0.428, \"1\": 0.683}, {\"0\": 0.583, \"1\": 0.854}, {\"0\": 0.525, \"1\": 0.675}, {\"0\": 0.695, \"1\": 0.669}, {\"0\": 0.905, \"1\": 0.22}, {\"0\": 0.571, \"1\": 0.64}, {\"0\": 0.658, \"1\": 0.836}, {\"0\": 0.456, \"1\": 0.651}, {\"0\": 0.37, \"1\": 0.658}, {\"0\": 0.447, \"1\": 0.673}, {\"0\": 0.683, \"1\": 0.445}, {\"0\": 0.501, \"1\": 0.701}, {\"0\": 0.687, \"1\": 0.259}, {\"0\": 0.508, \"1\": 0.735}, {\"0\": 0.748, \"1\": 0.714}, {\"0\": 0.772, \"1\": 0.578}, {\"0\": 0.571, \"1\": 0.521}, {\"0\": 0.317, \"1\": 0.827}, {\"0\": 0.389, \"1\": 0.529}, {\"0\": 0.401, \"1\": 0.539}, {\"0\": 0.771, \"1\": 0.347}, {\"0\": 0.733, \"1\": 0.43}, {\"0\": 0.49, \"1\": 0.85}, {\"0\": 0.638, \"1\": 0.856}, {\"0\": 0.616, \"1\": 0.622}, {\"0\": 0.71, \"1\": 0.775}, {\"0\": 0.697, \"1\": 0.653}, {\"0\": 0.726, \"1\": 0.277}, {\"0\": 0.688, \"1\": 0.246}, {\"0\": 0.612, \"1\": 0.562}, {\"0\": 0.352, \"1\": 0.718}, {\"0\": 0.605, \"1\": 0.719}, {\"0\": 0.794, \"1\": 0.874}, {\"0\": 0.433, \"1\": 0.648}, {\"0\": 0.674, \"1\": 0.753}, {\"0\": 0.815, \"1\": 0.654}, {\"0\": 0.411, \"1\": 0.61}, {\"0\": 0.504, \"1\": 0.812}, {\"0\": 0.567, \"1\": 0.506}, {\"0\": 0.852, \"1\": 0.716}, {\"0\": 0.537, \"1\": 0.683}, {\"0\": 0.349, \"1\": 0.386}, {\"0\": 0.475, \"1\": 0.525}, {\"0\": 0.692, \"1\": 0.775}, {\"0\": 0.633, \"1\": 0.552}, {\"0\": 0.739, \"1\": 0.808}, {\"0\": 0.364, \"1\": 0.472}, {\"0\": 0.535, \"1\": 0.808}, {\"0\": 0.769, \"1\": 0.69}, {\"0\": 0.557, \"1\": 0.599}, {\"0\": 0.619, \"1\": 0.413}, {\"0\": 0.429, \"1\": 0.893}, {\"0\": 0.4, \"1\": 0.667}, {\"0\": 0.747, \"1\": 0.667}, {\"0\": 0.629, \"1\": 0.618}, {\"0\": 0.231, \"1\": 0.698}, {\"0\": 0.529, \"1\": 0.81}, {\"0\": 0.436, \"1\": 0.625}, {\"0\": 0.675, \"1\": 0.646}, {\"0\": 0.519, \"1\": 0.654}, {\"0\": 0.423, \"1\": 0.646}, {\"0\": 0.763, \"1\": 0.333}, {\"0\": 0.764, \"1\": 0.558}, {\"0\": 0.767, \"1\": 0.289}, {\"0\": 0.385, \"1\": 0.228}, {\"0\": 0.605, \"1\": 0.653}, {\"0\": 0.693, \"1\": 0.637}, {\"0\": 0.755, \"1\": 0.88}, {\"0\": 0.677, \"1\": 0.649}, {\"0\": 0.296, \"1\": 0.592}, {\"0\": 0.794, \"1\": 0.802}, {\"0\": 0.444, \"1\": 0.758}, {\"0\": 0.491, \"1\": 0.76}, {\"0\": 0.725, \"1\": 0.706}, {\"0\": 0.599, \"1\": 0.725}, {\"0\": 0.526, \"1\": 0.741}, {\"0\": 0.152, \"1\": 0.738}, {\"0\": 0.741, \"1\": 0.76}, {\"0\": 0.514, \"1\": 0.612}, {\"0\": 0.642, \"1\": 0.686}, {\"0\": 0.719, \"1\": 0.896}, {\"0\": 0.697, \"1\": 0.449}, {\"0\": 0.531, \"1\": 0.804}, {\"0\": 0.209, \"1\": 0.481}, {\"0\": 0.54, \"1\": 0.675}, {\"0\": 0.719, \"1\": 0.575}, {\"0\": 0.605, \"1\": 0.736}, {\"0\": 0.602, \"1\": 0.683}, {\"0\": 0.719, \"1\": 0.555}, {\"0\": 0.608, \"1\": 0.486}, {\"0\": 0.587, \"1\": 0.857}, {\"0\": 0.859, \"1\": 0.633}, {\"0\": 0.495, \"1\": 0.869}, {\"0\": 0.729, \"1\": 0.568}, {\"0\": 0.569, \"1\": 0.807}, {\"0\": 0.554, \"1\": 0.673}, {\"0\": 0.776, \"1\": 0.651}, {\"0\": 0.825, \"1\": 0.564}, {\"0\": 0.206, \"1\": 0.527}, {\"0\": 0.522, \"1\": 0.726}, {\"0\": 0.653, \"1\": 0.691}, {\"0\": 0.597, \"1\": 0.487}, {\"0\": 0.442, \"1\": 0.405}, {\"0\": 0.583, \"1\": 0.331}, {\"0\": 0.661, \"1\": 0.774}, {\"0\": 0.776, \"1\": 0.564}, {\"0\": 0.68, \"1\": 0.288}, {\"0\": 0.519, \"1\": 0.502}, {\"0\": 0.604, \"1\": 0.35}, {\"0\": 0.92, \"1\": 0.356}, {\"0\": 0.513, \"1\": 0.291}, {\"0\": 0.413, \"1\": 0.494}, {\"0\": 0.662, \"1\": 0.729}, {\"0\": 0.698, \"1\": 0.226}, {\"0\": 0.66, \"1\": 0.716}, {\"0\": 0.42, \"1\": 0.614}, {\"0\": 0.631, \"1\": 0.907}, {\"0\": 0.711, \"1\": 0.747}, {\"0\": 0.336, \"1\": 0.586}, {\"0\": 0.623, \"1\": 0.771}, {\"0\": 0.333, \"1\": 0.479}, {\"0\": 0.64, \"1\": 0.58}, {\"0\": 0.55, \"1\": 0.757}, {\"0\": 0.635, \"1\": 0.567}, {\"0\": 0.599, \"1\": 0.587}, {\"0\": 0.329, \"1\": 0.796}, {\"0\": 0.538, \"1\": 0.683}, {\"0\": 0.478, \"1\": 0.722}, {\"0\": 0.828, \"1\": 0.785}, {\"0\": 0.495, \"1\": 0.76}, {\"0\": 0.633, \"1\": 0.584}, {\"0\": 0.274, \"1\": 0.789}, {\"0\": 0.786, \"1\": 0.371}, {\"0\": 0.635, \"1\": 0.786}, {\"0\": 0.779, \"1\": 0.799}, {\"0\": 0.76, \"1\": 0.912}, {\"0\": 0.428, \"1\": 0.727}, {\"0\": 0.507, \"1\": 0.833}, {\"0\": 0.766, \"1\": 0.63}, {\"0\": 0.533, \"1\": 0.735}, {\"0\": 0.633, \"1\": 0.57}, {\"0\": 0.659, \"1\": 0.499}, {\"0\": 0.487, \"1\": 0.528}, {\"0\": 0.466, \"1\": 0.294}, {\"0\": 0.476, \"1\": 0.289}, {\"0\": 0.873, \"1\": 0.564}, {\"0\": 0.679, \"1\": 0.714}, {\"0\": 0.68, \"1\": 0.838}, {\"0\": 0.46, \"1\": 0.807}, {\"0\": 0.66, \"1\": 0.589}, {\"0\": 0.211, \"1\": 0.651}, {\"0\": 0.403, \"1\": 0.597}, {\"0\": 0.684, \"1\": 0.631}, {\"0\": 0.534, \"1\": 0.54}, {\"0\": 0.436, \"1\": 0.684}, {\"0\": 0.226, \"1\": 0.715}, {\"0\": 0.5, \"1\": 0.716}, {\"0\": 0.543, \"1\": 0.735}, {\"0\": 0.947, \"1\": 0.728}, {\"0\": 0.614, \"1\": 0.586}, {\"0\": 0.173, \"1\": 0.745}, {\"0\": 0.335, \"1\": 0.429}, {\"0\": 0.732, \"1\": 0.498}, {\"0\": 0.528, \"1\": 0.688}, {\"0\": 0.591, \"1\": 0.506}, {\"0\": 0.598, \"1\": 0.912}, {\"0\": 0.889, \"1\": 0.476}, {\"0\": 0.593, \"1\": 0.749}, {\"0\": 0.635, \"1\": 0.738}, {\"0\": 0.841, \"1\": 0.727}, {\"0\": 0.967, \"1\": 0.609}, {\"0\": 0.498, \"1\": 0.688}, {\"0\": 0.615, \"1\": 0.557}, {\"0\": 0.539, \"1\": 0.694}, {\"0\": 0.738, \"1\": 0.536}, {\"0\": 0.706, \"1\": 0.246}, {\"0\": 0.468, \"1\": 0.549}, {\"0\": 0.409, \"1\": 0.769}, {\"0\": 0.608, \"1\": 0.644}, {\"0\": 0.584, \"1\": 0.598}, {\"0\": 0.566, \"1\": 0.191}, {\"0\": 0.42, \"1\": 0.82}, {\"0\": 0.648, \"1\": 0.747}, {\"0\": 0.78, \"1\": 0.619}, {\"0\": 0.414, \"1\": 0.772}, {\"0\": 0.657, \"1\": 0.646}, {\"0\": 0.291, \"1\": 0.373}, {\"0\": 0.368, \"1\": 0.689}, {\"0\": 0.677, \"1\": 0.646}, {\"0\": 0.644, \"1\": 0.654}, {\"0\": 0.472, \"1\": 0.599}, {\"0\": 0.73, \"1\": 0.749}, {\"0\": 0.534, \"1\": 0.597}, {\"0\": 0.89, \"1\": 0.723}, {\"0\": 0.585, \"1\": 0.607}, {\"0\": 0.623, \"1\": 0.829}, {\"0\": 0.853, \"1\": 0.798}, {\"0\": 0.741, \"1\": 0.736}, {\"0\": 0.528, \"1\": 0.575}, {\"0\": 0.718, \"1\": 0.5}, {\"0\": 0.49, \"1\": 0.597}, {\"0\": 0.627, \"1\": 0.834}, {\"0\": 0.892, \"1\": 0.697}, {\"0\": 0.555, \"1\": 0.666}, {\"0\": 0.626, \"1\": 0.803}, {\"0\": 0.712, \"1\": 0.494}, {\"0\": 0.256, \"1\": 0.683}, {\"0\": 0.85, \"1\": 0.582}, {\"0\": 0.603, \"1\": 0.771}, {\"0\": 0.386, \"1\": 0.735}, {\"0\": 0.499, \"1\": 0.698}, {\"0\": 0.631, \"1\": 0.703}, {\"0\": 0.458, \"1\": 0.586}, {\"0\": 0.616, \"1\": 0.521}, {\"0\": 0.54, \"1\": 0.305}, {\"0\": 0.583, \"1\": 0.9}, {\"0\": 0.652, \"1\": 0.864}, {\"0\": 0.498, \"1\": 0.931}, {\"0\": 0.295, \"1\": 0.652}, {\"0\": 0.633, \"1\": 0.286}, {\"0\": 0.393, \"1\": 0.683}, {\"0\": 0.562, \"1\": 0.559}, {\"0\": 0.457, \"1\": 0.285}, {\"0\": 0.545, \"1\": 0.715}, {\"0\": 0.476, \"1\": 0.781}, {\"0\": 0.579, \"1\": 0.716}, {\"0\": 0.417, \"1\": 0.526}, {\"0\": 0.801, \"1\": 0.763}, {\"0\": 0.829, \"1\": 0.494}, {\"0\": 0.164, \"1\": 0.607}, {\"0\": 0.285, \"1\": 0.354}, {\"0\": 0.396, \"1\": 0.786}, {\"0\": 0.475, \"1\": 0.275}, {\"0\": 0.553, \"1\": 0.576}, {\"0\": 0.428, \"1\": 0.678}, {\"0\": 0.615, \"1\": 0.628}, {\"0\": 0.741, \"1\": 0.603}, {\"0\": 0.532, \"1\": 0.826}, {\"0\": 0.482, \"1\": 0.529}, {\"0\": 0.399, \"1\": 0.739}, {\"0\": 0.583, \"1\": 0.792}, {\"0\": 0.295, \"1\": 0.266}, {\"0\": 0.72, \"1\": 0.878}, {\"0\": 0.816, \"1\": 0.597}, {\"0\": 0.725, \"1\": 0.578}, {\"0\": 0.719, \"1\": 0.754}, {\"0\": 0.524, \"1\": 0.413}, {\"0\": 0.664, \"1\": 0.514}, {\"0\": 0.571, \"1\": 0.6}, {\"0\": 0.69, \"1\": 0.805}, {\"0\": 0.689, \"1\": 0.61}, {\"0\": 0.862, \"1\": 0.901}, {\"0\": 0.388, \"1\": 0.735}, {\"0\": 0.592, \"1\": 0.66}, {\"0\": 0.479, \"1\": 0.862}, {\"0\": 0.476, \"1\": 0.507}, {\"0\": 0.516, \"1\": 0.665}, {\"0\": 0.507, \"1\": 0.644}, {\"0\": 0.428, \"1\": 0.79}, {\"0\": 0.7, \"1\": 0.726}, {\"0\": 0.401, \"1\": 0.614}, {\"0\": 0.867, \"1\": 0.446}, {\"0\": 0.428, \"1\": 0.596}, {\"0\": 0.814, \"1\": 0.89}, {\"0\": 0.489, \"1\": 0.614}, {\"0\": 0.66, \"1\": 0.784}, {\"0\": 0.717, \"1\": 0.735}, {\"0\": 0.746, \"1\": 0.571}, {\"0\": 0.323, \"1\": 0.729}, {\"0\": 0.518, \"1\": 0.512}, {\"0\": 0.549, \"1\": 0.704}, {\"0\": 0.692, \"1\": 0.571}, {\"0\": 0.606, \"1\": 0.573}, {\"0\": 0.851, \"1\": 0.732}, {\"0\": 0.721, \"1\": 0.799}, {\"0\": 0.598, \"1\": 0.385}, {\"0\": 0.543, \"1\": 0.685}, {\"0\": 0.784, \"1\": 0.34}, {\"0\": 0.635, \"1\": 0.76}, {\"0\": 0.658, \"1\": 0.599}, {\"0\": 0.505, \"1\": 0.286}, {\"0\": 0.713, \"1\": 0.819}, {\"0\": 0.691, \"1\": 0.346}, {\"0\": 0.745, \"1\": 0.811}, {\"0\": 0.817, \"1\": 0.771}, {\"0\": 0.832, \"1\": 0.777}, {\"0\": 0.574, \"1\": 0.825}, {\"0\": 0.726, \"1\": 0.654}, {\"0\": 0.585, \"1\": 0.767}, {\"0\": 0.551, \"1\": 0.679}, {\"0\": 0.688, \"1\": 0.316}, {\"0\": 0.894, \"1\": 0.749}, {\"0\": 0.589, \"1\": 0.706}, {\"0\": 0.529, \"1\": 0.709}, {\"0\": 0.24, \"1\": 0.835}, {\"0\": 0.655, \"1\": 0.734}, {\"0\": 0.876, \"1\": 0.502}, {\"0\": 0.87, \"1\": 0.683}, {\"0\": 0.624, \"1\": 0.651}, {\"0\": 0.439, \"1\": 0.438}, {\"0\": 0.514, \"1\": 0.695}, {\"0\": 0.497, \"1\": 0.786}, {\"0\": 0.485, \"1\": 0.815}, {\"0\": 0.664, \"1\": 0.632}, {\"0\": 0.635, \"1\": 0.895}, {\"0\": 0.743, \"1\": 0.851}, {\"0\": 0.581, \"1\": 0.606}, {\"0\": 0.712, \"1\": 0.859}, {\"0\": 0.408, \"1\": 0.86}, {\"0\": 0.674, \"1\": 0.512}, {\"0\": 0.505, \"1\": 0.77}, {\"0\": 0.764, \"1\": 0.76}, {\"0\": 0.646, \"1\": 0.85}, {\"0\": 0.525, \"1\": 0.789}, {\"0\": 0.69, \"1\": 0.769}, {\"0\": 0.622, \"1\": 0.448}, {\"0\": 0.576, \"1\": 0.959}, {\"0\": 0.959, \"1\": 0.764}, {\"0\": 0.446, \"1\": 0.595}, {\"0\": 0.575, \"1\": 0.742}, {\"0\": 0.67, \"1\": 0.736}, {\"0\": 0.45, \"1\": 0.709}, {\"0\": 0.42, \"1\": 0.513}, {\"0\": 0.489, \"1\": 0.829}, {\"0\": 0.421, \"1\": 0.751}, {\"0\": 0.458, \"1\": 0.665}, {\"0\": 0.222, \"1\": 0.723}, {\"0\": 0.434, \"1\": 0.63}, {\"0\": 0.694, \"1\": 0.568}, {\"0\": 0.458, \"1\": 0.614}, {\"0\": 0.517, \"1\": 0.661}, {\"0\": 0.586, \"1\": 0.7}, {\"0\": 0.638, \"1\": 0.443}, {\"0\": 0.35, \"1\": 0.916}, {\"0\": 0.236, \"1\": 0.508}, {\"0\": 0.696, \"1\": 0.567}, {\"0\": 0.899, \"1\": 0.759}, {\"0\": 0.76, \"1\": 0.743}, {\"0\": 0.762, \"1\": 0.648}, {\"0\": 0.763, \"1\": 0.797}, {\"0\": 0.657, \"1\": 0.876}, {\"0\": 0.792, \"1\": 0.883}, {\"0\": 0.585, \"1\": 0.34}, {\"0\": 0.52, \"1\": 0.771}, {\"0\": 0.237, \"1\": 0.721}, {\"0\": 0.889, \"1\": 0.725}, {\"0\": 0.622, \"1\": 0.79}, {\"0\": 0.723, \"1\": 0.688}, {\"0\": 0.743, \"1\": 0.658}, {\"0\": 0.591, \"1\": 0.399}, {\"0\": 0.641, \"1\": 0.309}, {\"0\": 0.523, \"1\": 0.432}, {\"0\": 0.575, \"1\": 0.743}, {\"0\": 0.47, \"1\": 0.605}, {\"0\": 0.64, \"1\": 0.671}, {\"0\": 0.528, \"1\": 0.597}, {\"0\": 0.375, \"1\": 0.888}, {\"0\": 0.716, \"1\": 0.962}, {\"0\": 0.475, \"1\": 0.787}, {\"0\": 0.466, \"1\": 0.824}, {\"0\": 0.54, \"1\": 0.67}, {\"0\": 0.714, \"1\": 0.661}, {\"0\": 0.593, \"1\": 0.595}, {\"0\": 0.615, \"1\": 0.476}, {\"0\": 0.389, \"1\": 0.251}, {\"0\": 0.659, \"1\": 0.293}, {\"0\": 0.546, \"1\": 0.853}, {\"0\": 0.715, \"1\": 0.879}, {\"0\": 0.524, \"1\": 0.767}, {\"0\": 0.649, \"1\": 0.535}, {\"0\": 0.605, \"1\": 0.876}, {\"0\": 0.833, \"1\": 0.545}, {\"0\": 0.488, \"1\": 0.796}, {\"0\": 0.176, \"1\": 0.896}, {\"0\": 0.583, \"1\": 0.817}, {\"0\": 0.566, \"1\": 0.695}, {\"0\": 0.37, \"1\": 0.748}, {\"0\": 0.421, \"1\": 0.79}, {\"0\": 0.704, \"1\": 0.728}, {\"0\": 0.568, \"1\": 0.726}, {\"0\": 0.472, \"1\": 0.566}, {\"0\": 0.448, \"1\": 0.316}, {\"0\": 0.696, \"1\": 0.839}, {\"0\": 0.296, \"1\": 0.831}, {\"0\": 0.574, \"1\": 0.427}, {\"0\": 0.549, \"1\": 0.379}, {\"0\": 0.63, \"1\": 0.43}, {\"0\": 0.445, \"1\": 0.622}, {\"0\": 0.48, \"1\": 0.561}, {\"0\": 0.443, \"1\": 0.438}, {\"0\": 0.633, \"1\": 0.763}, {\"0\": 0.553, \"1\": 0.633}, {\"0\": 0.732, \"1\": 0.342}, {\"0\": 0.853, \"1\": 0.542}, {\"0\": 0.652, \"1\": 0.634}, {\"0\": 0.563, \"1\": 0.821}, {\"0\": 0.768, \"1\": 0.44}, {\"0\": 0.586, \"1\": 0.821}, {\"0\": 0.711, \"1\": 0.789}, {\"0\": 0.662, \"1\": 0.551}, {\"0\": 0.862, \"1\": 0.44}, {\"0\": 0.383, \"1\": 0.747}, {\"0\": 0.67, \"1\": 0.687}, {\"0\": 0.511, \"1\": 0.853}, {\"0\": 0.653, \"1\": 0.52}, {\"0\": 0.54, \"1\": 0.847}, {\"0\": 0.561, \"1\": 0.697}, {\"0\": 0.498, \"1\": 0.482}, {\"0\": 0.379, \"1\": 0.677}, {\"0\": 0.778, \"1\": 0.237}, {\"0\": 0.768, \"1\": 0.818}, {\"0\": 0.59, \"1\": 0.646}, {\"0\": 0.653, \"1\": 0.671}, {\"0\": 0.277, \"1\": 0.378}, {\"0\": 0.678, \"1\": 0.636}, {\"0\": 0.494, \"1\": 0.578}, {\"0\": 0.617, \"1\": 0.752}, {\"0\": 0.619, \"1\": 0.769}, {\"0\": 0.754, \"1\": 0.516}, {\"0\": 0.895, \"1\": 0.81}, {\"0\": 0.638, \"1\": 0.621}, {\"0\": 0.257, \"1\": 0.635}, {\"0\": 0.691, \"1\": 0.5}, {\"0\": 0.685, \"1\": 0.428}, {\"0\": 0.458, \"1\": 0.491}, {\"0\": 0.596, \"1\": 0.797}, {\"0\": 0.421, \"1\": 0.482}, {\"0\": 0.748, \"1\": 0.673}, {\"0\": 0.447, \"1\": 0.663}, {\"0\": 0.717, \"1\": 0.612}, {\"0\": 0.757, \"1\": 0.329}, {\"0\": 0.503, \"1\": 0.882}, {\"0\": 0.516, \"1\": 0.756}, {\"0\": 0.575, \"1\": 0.421}, {\"0\": 0.645, \"1\": 0.606}, {\"0\": 0.443, \"1\": 0.763}, {\"0\": 0.534, \"1\": 0.587}, {\"0\": 0.17, \"1\": 0.594}, {\"0\": 0.613, \"1\": 0.676}, {\"0\": 0.529, \"1\": 0.785}, {\"0\": 0.747, \"1\": 0.855}, {\"0\": 0.703, \"1\": 0.7}, {\"0\": 0.58, \"1\": 0.583}, {\"0\": 0.475, \"1\": 0.667}, {\"0\": 0.686, \"1\": 0.479}, {\"0\": 0.455, \"1\": 0.697}, {\"0\": 0.156, \"1\": 0.624}, {\"0\": 0.726, \"1\": 0.702}, {\"0\": 0.591, \"1\": 0.51}, {\"0\": 0.589, \"1\": 0.746}, {\"0\": 0.618, \"1\": 0.941}, {\"0\": 0.704, \"1\": 0.468}, {\"0\": 0.688, \"1\": 0.63}, {\"0\": 0.637, \"1\": 0.637}, {\"0\": 0.718, \"1\": 0.756}, {\"0\": 0.637, \"1\": 0.491}, {\"0\": 0.4, \"1\": 0.664}, {\"0\": 0.854, \"1\": 0.67}, {\"0\": 0.926, \"1\": 0.886}, {\"0\": 0.276, \"1\": 0.815}, {\"0\": 0.618, \"1\": 0.667}, {\"0\": 0.599, \"1\": 0.714}, {\"0\": 0.652, \"1\": 0.77}, {\"0\": 0.672, \"1\": 0.544}, {\"0\": 0.636, \"1\": 0.915}, {\"0\": 0.446, \"1\": 0.61}, {\"0\": 0.757, \"1\": 0.695}, {\"0\": 0.386, \"1\": 0.557}, {\"0\": 0.466, \"1\": 0.756}, {\"0\": 0.456, \"1\": 0.739}, {\"0\": 0.726, \"1\": 0.836}, {\"0\": 0.467, \"1\": 0.574}, {\"0\": 0.638, \"1\": 0.845}, {\"0\": 0.566, \"1\": 0.561}, {\"0\": 0.873, \"1\": 0.604}, {\"0\": 0.468, \"1\": 0.704}, {\"0\": 0.403, \"1\": 0.398}, {\"0\": 0.39, \"1\": 0.148}, {\"0\": 0.47, \"1\": 0.7}, {\"0\": 0.485, \"1\": 0.832}, {\"0\": 0.261, \"1\": 0.632}, {\"0\": 0.55, \"1\": 0.823}, {\"0\": 0.609, \"1\": 0.507}, {\"0\": 0.715, \"1\": 0.328}, {\"0\": 0.474, \"1\": 0.787}, {\"0\": 0.406, \"1\": 0.409}, {\"0\": 0.618, \"1\": 0.808}, {\"0\": 0.385, \"1\": 0.865}, {\"0\": 0.539, \"1\": 0.523}, {\"0\": 0.191, \"1\": 0.731}, {\"0\": 0.446, \"1\": 0.865}, {\"0\": 0.622, \"1\": 0.313}, {\"0\": 0.277, \"1\": 0.697}, {\"0\": 0.66, \"1\": 0.556}, {\"0\": 0.6, \"1\": 0.507}, {\"0\": 0.559, \"1\": 0.736}, {\"0\": 0.646, \"1\": 0.484}, {\"0\": 0.551, \"1\": 0.752}, {\"0\": 0.466, \"1\": 0.657}, {\"0\": 0.618, \"1\": 0.457}, {\"0\": 0.511, \"1\": 0.462}, {\"0\": 0.353, \"1\": 0.49}, {\"0\": 0.836, \"1\": 0.807}, {\"0\": 0.894, \"1\": 0.635}, {\"0\": 0.738, \"1\": 0.565}, {\"0\": 0.723, \"1\": 0.614}, {\"0\": 0.694, \"1\": 0.624}, {\"0\": 0.608, \"1\": 0.806}, {\"0\": 0.55, \"1\": 0.529}, {\"0\": 0.984, \"1\": 0.793}, {\"0\": 0.659, \"1\": 0.162}, {\"0\": 0.534, \"1\": 0.517}, {\"0\": 0.616, \"1\": 0.788}, {\"0\": 0.425, \"1\": 0.603}, {\"0\": 0.546, \"1\": 0.85}, {\"0\": 0.481, \"1\": 0.422}, {\"0\": 0.696, \"1\": 0.473}, {\"0\": 0.743, \"1\": 0.539}, {\"0\": 0.718, \"1\": 0.622}, {\"0\": 0.581, \"1\": 0.784}, {\"0\": 0.63, \"1\": 0.569}, {\"0\": 0.81, \"1\": 0.744}, {\"0\": 0.7, \"1\": 0.857}, {\"0\": 0.6, \"1\": 0.867}, {\"0\": 0.526, \"1\": 0.733}, {\"0\": 0.668, \"1\": 0.606}, {\"0\": 0.627, \"1\": 0.674}, {\"0\": 0.66, \"1\": 0.804}, {\"0\": 0.561, \"1\": 0.791}, {\"0\": 0.547, \"1\": 0.869}, {\"0\": 0.623, \"1\": 0.516}, {\"0\": 0.52, \"1\": 0.53}, {\"0\": 0.753, \"1\": 0.647}, {\"0\": 0.442, \"1\": 0.867}, {\"0\": 0.701, \"1\": 0.696}, {\"0\": 0.701, \"1\": 0.858}, {\"0\": 0.865, \"1\": 0.348}, {\"0\": 0.486, \"1\": 0.837}, {\"0\": 0.604, \"1\": 0.704}, {\"0\": 0.591, \"1\": 0.881}, {\"0\": 0.761, \"1\": 0.525}, {\"0\": 0.593, \"1\": 0.793}, {\"0\": 0.561, \"1\": 0.418}, {\"0\": 0.746, \"1\": 0.468}, {\"0\": 0.51, \"1\": 0.646}, {\"0\": 0.339, \"1\": 0.722}, {\"0\": 0.173, \"1\": 0.401}, {\"0\": 0.44, \"1\": 0.739}, {\"0\": 0.544, \"1\": 0.791}, {\"0\": 0.623, \"1\": 0.667}, {\"0\": 0.685, \"1\": 0.648}, {\"0\": 0.779, \"1\": 0.366}, {\"0\": 0.581, \"1\": 0.648}, {\"0\": 0.442, \"1\": 0.703}, {\"0\": 0.531, \"1\": 0.844}, {\"0\": 0.548, \"1\": 0.833}, {\"0\": 0.616, \"1\": 0.748}, {\"0\": 0.768, \"1\": 0.805}, {\"0\": 0.546, \"1\": 0.824}, {\"0\": 0.467, \"1\": 0.751}, {\"0\": 0.412, \"1\": 0.641}, {\"0\": 0.595, \"1\": 0.748}, {\"0\": 0.428, \"1\": 0.415}, {\"0\": 0.862, \"1\": 0.628}, {\"0\": 0.47, \"1\": 0.791}, {\"0\": 0.681, \"1\": 0.794}, {\"0\": 0.632, \"1\": 0.654}, {\"0\": 0.823, \"1\": 0.702}, {\"0\": 0.66, \"1\": 0.841}, {\"0\": 0.704, \"1\": 0.586}, {\"0\": 0.803, \"1\": 0.718}, {\"0\": 0.673, \"1\": 0.611}, {\"0\": 0.479, \"1\": 0.762}, {\"0\": 0.437, \"1\": 0.765}, {\"0\": 0.809, \"1\": 0.525}, {\"0\": 0.624, \"1\": 0.71}, {\"0\": 0.651, \"1\": 0.388}, {\"0\": 0.591, \"1\": 0.808}, {\"0\": 0.162, \"1\": 0.787}, {\"0\": 0.625, \"1\": 0.586}, {\"0\": 0.644, \"1\": 0.448}, {\"0\": 0.301, \"1\": 0.283}, {\"0\": 0.758, \"1\": 0.563}, {\"0\": 0.481, \"1\": 0.735}, {\"0\": 0.792, \"1\": 0.585}, {\"0\": 0.427, \"1\": 0.535}, {\"0\": 0.63, \"1\": 0.823}, {\"0\": 0.484, \"1\": 0.444}, {\"0\": 0.713, \"1\": 0.312}, {\"0\": 0.319, \"1\": 0.749}, {\"0\": 0.709, \"1\": 0.719}, {\"0\": 0.385, \"1\": 0.89}, {\"0\": 0.51, \"1\": 0.695}, {\"0\": 0.581, \"1\": 0.603}, {\"0\": 0.439, \"1\": 0.556}, {\"0\": 0.398, \"1\": 0.805}, {\"0\": 0.595, \"1\": 0.777}, {\"0\": 0.68, \"1\": 0.569}, {\"0\": 0.657, \"1\": 0.754}, {\"0\": 0.672, \"1\": 0.602}, {\"0\": 0.635, \"1\": 0.227}, {\"0\": 0.95, \"1\": 0.647}, {\"0\": 0.724, \"1\": 0.313}, {\"0\": 0.628, \"1\": 0.6}, {\"0\": 0.596, \"1\": 0.706}, {\"0\": 0.671, \"1\": 0.727}, {\"0\": 0.401, \"1\": 0.814}, {\"0\": 0.682, \"1\": 0.747}, {\"0\": 0.616, \"1\": 0.794}, {\"0\": 0.396, \"1\": 0.376}, {\"0\": 0.375, \"1\": 0.634}, {\"0\": 0.622, \"1\": 0.744}, {\"0\": 0.587, \"1\": 0.655}, {\"0\": 0.673, \"1\": 0.768}, {\"0\": 0.841, \"1\": 0.872}, {\"0\": 0.499, \"1\": 0.701}, {\"0\": 0.406, \"1\": 0.645}, {\"0\": 0.833, \"1\": 0.819}, {\"0\": 0.715, \"1\": 0.64}, {\"0\": 0.682, \"1\": 0.839}, {\"0\": 0.685, \"1\": 0.844}, {\"0\": 0.667, \"1\": 0.845}, {\"0\": 0.326, \"1\": 0.663}, {\"0\": 0.696, \"1\": 0.444}, {\"0\": 0.331, \"1\": 0.805}, {\"0\": 0.589, \"1\": 0.645}, {\"0\": 0.724, \"1\": 0.542}, {\"0\": 0.749, \"1\": 0.746}, {\"0\": 0.624, \"1\": 0.417}, {\"0\": 0.535, \"1\": 0.93}, {\"0\": 0.55, \"1\": 0.707}, {\"0\": 0.465, \"1\": 0.628}, {\"0\": 0.73, \"1\": 0.403}, {\"0\": 0.657, \"1\": 0.802}, {\"0\": 0.555, \"1\": 0.683}, {\"0\": 0.732, \"1\": 0.514}, {\"0\": 0.685, \"1\": 0.847}, {\"0\": 0.703, \"1\": 0.876}, {\"0\": 0.415, \"1\": 0.816}, {\"0\": 0.637, \"1\": 0.794}, {\"0\": 0.499, \"1\": 0.638}, {\"0\": 0.755, \"1\": 0.907}, {\"0\": 0.631, \"1\": 0.74}, {\"0\": 0.41, \"1\": 0.457}, {\"0\": 0.736, \"1\": 0.583}, {\"0\": 0.621, \"1\": 0.327}, {\"0\": 0.568, \"1\": 0.683}, {\"0\": 0.562, \"1\": 0.723}, {\"0\": 0.721, \"1\": 0.669}, {\"0\": 0.775, \"1\": 0.853}, {\"0\": 0.72, \"1\": 0.795}, {\"0\": 0.559, \"1\": 0.632}, {\"0\": 0.743, \"1\": 0.841}, {\"0\": 0.709, \"1\": 0.856}, {\"0\": 0.548, \"1\": 0.722}, {\"0\": 0.747, \"1\": 0.799}, {\"0\": 0.683, \"1\": 0.616}, {\"0\": 0.679, \"1\": 0.804}, {\"0\": 0.639, \"1\": 0.805}, {\"0\": 0.726, \"1\": 0.589}, {\"0\": 0.547, \"1\": 0.36}, {\"0\": 0.685, \"1\": 0.428}, {\"0\": 0.904, \"1\": 0.701}, {\"0\": 0.801, \"1\": 0.579}, {\"0\": 0.441, \"1\": 0.554}, {\"0\": 0.615, \"1\": 0.515}, {\"0\": 0.409, \"1\": 0.6}, {\"0\": 0.766, \"1\": 0.731}, {\"0\": 0.701, \"1\": 0.654}, {\"0\": 0.832, \"1\": 0.745}, {\"0\": 0.757, \"1\": 0.748}, {\"0\": 0.751, \"1\": 0.656}, {\"0\": 0.844, \"1\": 0.636}, {\"0\": 0.263, \"1\": 0.874}, {\"0\": 0.422, \"1\": 0.469}, {\"0\": 0.549, \"1\": 0.908}, {\"0\": 0.737, \"1\": 0.698}, {\"0\": 0.798, \"1\": 0.725}, {\"0\": 0.602, \"1\": 0.767}, {\"0\": 0.841, \"1\": 0.836}, {\"0\": 0.483, \"1\": 0.793}, {\"0\": 0.654, \"1\": 0.236}, {\"0\": 0.446, \"1\": 0.389}, {\"0\": 0.941, \"1\": 0.515}, {\"0\": 0.64, \"1\": 0.614}, {\"0\": 0.634, \"1\": 0.723}, {\"0\": 0.68, \"1\": 0.532}, {\"0\": 0.594, \"1\": 0.846}, {\"0\": 0.447, \"1\": 0.589}, {\"0\": 0.622, \"1\": 0.646}, {\"0\": 0.505, \"1\": 0.81}, {\"0\": 0.439, \"1\": 0.702}, {\"0\": 0.34, \"1\": 0.605}, {\"0\": 0.712, \"1\": 0.45}, {\"0\": 0.546, \"1\": 0.803}, {\"0\": 0.637, \"1\": 0.897}, {\"0\": 0.616, \"1\": 0.599}, {\"0\": 0.612, \"1\": 0.523}, {\"0\": 0.592, \"1\": 0.824}, {\"0\": 0.732, \"1\": 0.36}, {\"0\": 0.565, \"1\": 0.542}, {\"0\": 0.607, \"1\": 0.615}, {\"0\": 0.773, \"1\": 0.76}, {\"0\": 0.258, \"1\": 0.374}, {\"0\": 0.407, \"1\": 0.393}, {\"0\": 0.484, \"1\": 0.753}, {\"0\": 0.676, \"1\": 0.533}, {\"0\": 0.584, \"1\": 0.684}, {\"0\": 0.685, \"1\": 0.937}, {\"0\": 0.729, \"1\": 0.753}, {\"0\": 0.658, \"1\": 0.478}, {\"0\": 0.412, \"1\": 0.709}, {\"0\": 0.75, \"1\": 0.65}, {\"0\": 0.294, \"1\": 0.609}, {\"0\": 0.454, \"1\": 0.615}, {\"0\": 0.745, \"1\": 0.788}, {\"0\": 0.445, \"1\": 0.514}, {\"0\": 0.715, \"1\": 0.374}, {\"0\": 0.388, \"1\": 0.571}, {\"0\": 0.53, \"1\": 0.464}, {\"0\": 0.562, \"1\": 0.722}, {\"0\": 0.585, \"1\": 0.858}, {\"0\": 0.393, \"1\": 0.401}, {\"0\": 0.521, \"1\": 0.713}, {\"0\": 0.677, \"1\": 0.609}, {\"0\": 0.771, \"1\": 0.7}, {\"0\": 0.616, \"1\": 0.754}, {\"0\": 0.81, \"1\": 0.771}, {\"0\": 0.512, \"1\": 0.7}, {\"0\": 0.755, \"1\": 0.897}, {\"0\": 0.556, \"1\": 0.875}, {\"0\": 0.523, \"1\": 0.739}, {\"0\": 0.303, \"1\": 0.65}, {\"0\": 0.857, \"1\": 0.334}, {\"0\": 0.557, \"1\": 0.601}, {\"0\": 0.656, \"1\": 0.759}, {\"0\": 0.361, \"1\": 0.801}, {\"0\": 0.569, \"1\": 0.715}, {\"0\": 0.415, \"1\": 0.731}, {\"0\": 0.72, \"1\": 0.406}, {\"0\": 0.501, \"1\": 0.474}, {\"0\": 0.647, \"1\": 0.48}, {\"0\": 0.492, \"1\": 0.468}, {\"0\": null, \"1\": 0.812}, {\"0\": null, \"1\": 0.794}, {\"0\": null, \"1\": 0.359}, {\"0\": null, \"1\": 0.469}, {\"0\": null, \"1\": 0.697}, {\"0\": null, \"1\": 0.742}, {\"0\": null, \"1\": 0.698}, {\"0\": null, \"1\": 0.749}, {\"0\": null, \"1\": 0.484}, {\"0\": null, \"1\": 0.857}, {\"0\": null, \"1\": 0.67}, {\"0\": null, \"1\": 0.607}, {\"0\": null, \"1\": 0.234}, {\"0\": null, \"1\": 0.624}, {\"0\": null, \"1\": 0.603}, {\"0\": null, \"1\": 0.603}, {\"0\": null, \"1\": 0.75}, {\"0\": null, \"1\": 0.682}, {\"0\": null, \"1\": 0.677}, {\"0\": null, \"1\": 0.485}, {\"0\": null, \"1\": 0.564}, {\"0\": null, \"1\": 0.757}, {\"0\": null, \"1\": 0.662}, {\"0\": null, \"1\": 0.832}, {\"0\": null, \"1\": 0.691}, {\"0\": null, \"1\": 0.54}, {\"0\": null, \"1\": 0.694}, {\"0\": null, \"1\": 0.417}, {\"0\": null, \"1\": 0.633}, {\"0\": null, \"1\": 0.803}, {\"0\": null, \"1\": 0.575}, {\"0\": null, \"1\": 0.672}, {\"0\": null, \"1\": 0.889}, {\"0\": null, \"1\": 0.855}, {\"0\": null, \"1\": 0.849}, {\"0\": null, \"1\": 0.758}, {\"0\": null, \"1\": 0.462}, {\"0\": null, \"1\": 0.596}, {\"0\": null, \"1\": 0.603}], \"data-871d06e82555bd1fd3e3fd9b182c0c96\": [{\"0\": 144.784, \"1\": 119.941}, {\"0\": 114.001, \"1\": 148.075}, {\"0\": 135.956, \"1\": 125.008}, {\"0\": 129.501, \"1\": 110.016}, {\"0\": 148.895, \"1\": 145.995}, {\"0\": 139.024, \"1\": 131.588}, {\"0\": 116.666, \"1\": 114.407}, {\"0\": 136.006, \"1\": 104.03}, {\"0\": 97.043, \"1\": 144.053}, {\"0\": 169.896, \"1\": 100.036}, {\"0\": 167.097, \"1\": 152.009}, {\"0\": 126.704, \"1\": 140.002}, {\"0\": 92.002, \"1\": 114.217}, {\"0\": 111.963, \"1\": 82.553}, {\"0\": 91.878, \"1\": 113.036}, {\"0\": 119.891, \"1\": 74.04}, {\"0\": 169.842, \"1\": 195.058}, {\"0\": 128.125, \"1\": 100.002}, {\"0\": 138.013, \"1\": 127.95}, {\"0\": 102.033, \"1\": 89.821}, {\"0\": 169.801, \"1\": 99.995}, {\"0\": 94.291, \"1\": 106.963}, {\"0\": 148.077, \"1\": 158.188}, {\"0\": 94.933, \"1\": 91.045}, {\"0\": 139.969, \"1\": 186.287}, {\"0\": 91.012, \"1\": 75.286}, {\"0\": 101.821, \"1\": 140.157}, {\"0\": 119.439, \"1\": 147.73}, {\"0\": 96.031, \"1\": 98.742}, {\"0\": 120.037, \"1\": 126.394}, {\"0\": 145.861, \"1\": 123.994}, {\"0\": 129.983, \"1\": 127.023}, {\"0\": 160.052, \"1\": 90.772}, {\"0\": 170.124, \"1\": 81.0}, {\"0\": 147.957, \"1\": 80.015}, {\"0\": 92.033, \"1\": 123.974}, {\"0\": 121.797, \"1\": 127.142}, {\"0\": 94.999, \"1\": 140.029}, {\"0\": 159.618, \"1\": 69.77}, {\"0\": 138.004, \"1\": 140.086}, {\"0\": 115.4, \"1\": 97.512}, {\"0\": 174.039, \"1\": 115.564}, {\"0\": 127.273, \"1\": 168.039}, {\"0\": 105.013, \"1\": 123.98}, {\"0\": 122.667, \"1\": 154.008}, {\"0\": 149.843, \"1\": 128.049}, {\"0\": 176.127, \"1\": 124.982}, {\"0\": 88.957, \"1\": 107.83}, {\"0\": 149.996, \"1\": 83.072}, {\"0\": 75.019, \"1\": 166.012}, {\"0\": 168.883, \"1\": 112.007}, {\"0\": 104.311, \"1\": 112.923}, {\"0\": 79.092, \"1\": 128.128}, {\"0\": 93.894, \"1\": 182.13}, {\"0\": 179.91, \"1\": 119.951}, {\"0\": 154.901, \"1\": 129.03}, {\"0\": 139.705, \"1\": 150.162}, {\"0\": 180.044, \"1\": 114.812}, {\"0\": 125.014, \"1\": 125.927}, {\"0\": 129.966, \"1\": 134.034}, {\"0\": 96.09, \"1\": 130.032}, {\"0\": 47.859, \"1\": 119.35}, {\"0\": 135.019, \"1\": 112.575}, {\"0\": 155.356, \"1\": 139.608}, {\"0\": 88.997, \"1\": 94.506}, {\"0\": 92.515, \"1\": 90.053}, {\"0\": 125.965, \"1\": 126.018}, {\"0\": 122.037, \"1\": 146.032}, {\"0\": 114.492, \"1\": 144.812}, {\"0\": 136.093, \"1\": 130.041}, {\"0\": 104.202, \"1\": 101.029}, {\"0\": 123.036, \"1\": 71.302}, {\"0\": 94.006, \"1\": 153.027}, {\"0\": 125.976, \"1\": 169.988}, {\"0\": 165.29, \"1\": 94.988}, {\"0\": 125.989, \"1\": 117.123}, {\"0\": 100.338, \"1\": 119.954}, {\"0\": 98.981, \"1\": 127.857}, {\"0\": 102.984, \"1\": 115.004}, {\"0\": 115.219, \"1\": 103.966}, {\"0\": 97.96, \"1\": 129.986}, {\"0\": 175.483, \"1\": 123.289}, {\"0\": 141.951, \"1\": 137.644}, {\"0\": 92.112, \"1\": 131.04}, {\"0\": 102.315, \"1\": 115.121}, {\"0\": 100.449, \"1\": 129.982}, {\"0\": 127.921, \"1\": 175.953}, {\"0\": 134.056, \"1\": 94.967}, {\"0\": 90.066, \"1\": 176.057}, {\"0\": 88.009, \"1\": 121.99}, {\"0\": 117.715, \"1\": 154.908}, {\"0\": 102.186, \"1\": 89.876}, {\"0\": 100.968, \"1\": 115.02}, {\"0\": 127.999, \"1\": 122.262}, {\"0\": 149.935, \"1\": 121.976}, {\"0\": 123.966, \"1\": 119.969}, {\"0\": 108.196, \"1\": 140.026}, {\"0\": 92.028, \"1\": 110.953}, {\"0\": 85.814, \"1\": 190.05}, {\"0\": 87.337, \"1\": 129.948}, {\"0\": 73.946, \"1\": 120.752}, {\"0\": 147.978, \"1\": 179.063}, {\"0\": 90.02, \"1\": 119.998}, {\"0\": 123.659, \"1\": 96.973}, {\"0\": 106.974, \"1\": 109.013}, {\"0\": 120.06, \"1\": 82.66}, {\"0\": 95.024, \"1\": 118.293}, {\"0\": 119.923, \"1\": 137.281}, {\"0\": 146.128, \"1\": 165.201}, {\"0\": 174.974, \"1\": 116.956}, {\"0\": 78.158, \"1\": 92.227}, {\"0\": 126.877, \"1\": 129.989}, {\"0\": 108.054, \"1\": 110.091}, {\"0\": 115.003, \"1\": 102.099}, {\"0\": 104.315, \"1\": 117.005}, {\"0\": 119.945, \"1\": 99.995}, {\"0\": 132.973, \"1\": 111.01}, {\"0\": 136.086, \"1\": 134.009}, {\"0\": 121.05, \"1\": 125.009}, {\"0\": 137.849, \"1\": 95.108}, {\"0\": 137.174, \"1\": 120.248}, {\"0\": 112.956, \"1\": 122.007}, {\"0\": 126.112, \"1\": 145.306}, {\"0\": 122.977, \"1\": 143.598}, {\"0\": 77.996, \"1\": 134.962}, {\"0\": 124.829, \"1\": 140.804}, {\"0\": 77.351, \"1\": 124.007}, {\"0\": 199.988, \"1\": 174.144}, {\"0\": 90.012, \"1\": 140.124}, {\"0\": 127.991, \"1\": 111.984}, {\"0\": 125.967, \"1\": 120.995}, {\"0\": 75.09, \"1\": 126.18}, {\"0\": 122.02, \"1\": 170.02}, {\"0\": 167.969, \"1\": 75.461}, {\"0\": 80.013, \"1\": 107.434}, {\"0\": 117.008, \"1\": 111.98}, {\"0\": 130.002, \"1\": 120.0}, {\"0\": 95.981, \"1\": 150.067}, {\"0\": 122.08, \"1\": 94.242}, {\"0\": 159.119, \"1\": 109.99}, {\"0\": 120.079, \"1\": 129.882}, {\"0\": 95.977, \"1\": 122.035}, {\"0\": 67.584, \"1\": 120.722}, {\"0\": 128.019, \"1\": 130.04}, {\"0\": 132.965, \"1\": 108.049}, {\"0\": 85.018, \"1\": 94.46}, {\"0\": 65.003, \"1\": 131.952}, {\"0\": 142.029, \"1\": 143.299}, {\"0\": 139.012, \"1\": 128.024}, {\"0\": 98.051, \"1\": 89.394}, {\"0\": 124.035, \"1\": 151.632}, {\"0\": 133.943, \"1\": 142.053}, {\"0\": 127.991, \"1\": 114.64}, {\"0\": 117.046, \"1\": 166.3}, {\"0\": 129.834, \"1\": 150.052}, {\"0\": 97.942, \"1\": 140.832}, {\"0\": 86.412, \"1\": 146.747}, {\"0\": 106.083, \"1\": 108.915}, {\"0\": 93.033, \"1\": 106.714}, {\"0\": 168.019, \"1\": 127.971}, {\"0\": 88.541, \"1\": 124.036}, {\"0\": 99.954, \"1\": 130.031}, {\"0\": 184.007, \"1\": 144.933}, {\"0\": 95.878, \"1\": 111.959}, {\"0\": 99.889, \"1\": 136.067}, {\"0\": 145.108, \"1\": 129.959}, {\"0\": 109.552, \"1\": 138.923}, {\"0\": 159.913, \"1\": 114.902}, {\"0\": 81.021, \"1\": 139.99}, {\"0\": 116.461, \"1\": 129.008}, {\"0\": 132.346, \"1\": 103.848}, {\"0\": 76.952, \"1\": 140.031}, {\"0\": 148.073, \"1\": 126.465}, {\"0\": 90.024, \"1\": 147.028}, {\"0\": 124.022, \"1\": 122.494}, {\"0\": 161.25, \"1\": 109.988}, {\"0\": 110.038, \"1\": 120.599}, {\"0\": 111.955, \"1\": 140.054}, {\"0\": 138.858, \"1\": 141.655}, {\"0\": 99.991, \"1\": 119.978}, {\"0\": 98.096, \"1\": 127.16}, {\"0\": 92.006, \"1\": 103.693}, {\"0\": 100.042, \"1\": 108.009}, {\"0\": 159.907, \"1\": 109.982}, {\"0\": 115.51, \"1\": 127.193}, {\"0\": 166.009, \"1\": 186.328}, {\"0\": 105.854, \"1\": 122.872}, {\"0\": 113.457, \"1\": 81.108}, {\"0\": 145.947, \"1\": 145.556}, {\"0\": 123.091, \"1\": 92.054}, {\"0\": 129.75, \"1\": 113.074}, {\"0\": 74.04, \"1\": 121.986}, {\"0\": 168.807, \"1\": 124.051}, {\"0\": 89.279, \"1\": 140.03}, {\"0\": 150.11, \"1\": 88.237}, {\"0\": 202.013, \"1\": 108.993}, {\"0\": 91.048, \"1\": 107.007}, {\"0\": 153.947, \"1\": 103.081}, {\"0\": 83.982, \"1\": 140.063}, {\"0\": 129.255, \"1\": 95.487}, {\"0\": 83.886, \"1\": 137.131}, {\"0\": 129.023, \"1\": 100.01}, {\"0\": 129.991, \"1\": 81.998}, {\"0\": 77.926, \"1\": 117.99}, {\"0\": 99.131, \"1\": 127.99}, {\"0\": 110.522, \"1\": 82.925}, {\"0\": 123.981, \"1\": 111.015}, {\"0\": 136.08, \"1\": 109.278}, {\"0\": 128.076, \"1\": 108.212}, {\"0\": 130.057, \"1\": 99.346}, {\"0\": 128.065, \"1\": 159.089}, {\"0\": 146.919, \"1\": 98.82}, {\"0\": 145.085, \"1\": 95.987}, {\"0\": 119.941, \"1\": 167.909}, {\"0\": 117.0, \"1\": 114.019}, {\"0\": 81.945, \"1\": 138.0}, {\"0\": 109.95, \"1\": 109.67}, {\"0\": 128.015, \"1\": 121.427}, {\"0\": 139.998, \"1\": 121.977}, {\"0\": 144.127, \"1\": 80.047}, {\"0\": 84.992, \"1\": 82.118}, {\"0\": 149.859, \"1\": 136.021}, {\"0\": 114.484, \"1\": 108.99}, {\"0\": 137.972, \"1\": 114.969}, {\"0\": 168.152, \"1\": 179.236}, {\"0\": 99.873, \"1\": 127.86}, {\"0\": 99.983, \"1\": 100.215}, {\"0\": 98.002, \"1\": 106.034}, {\"0\": 139.401, \"1\": 172.03}, {\"0\": 135.805, \"1\": 127.813}, {\"0\": 155.667, \"1\": 115.942}, {\"0\": 61.758, \"1\": 140.732}, {\"0\": 98.963, \"1\": 108.438}, {\"0\": 95.966, \"1\": 153.547}, {\"0\": 140.034, \"1\": 83.013}, {\"0\": 97.954, \"1\": 146.024}, {\"0\": 186.346, \"1\": 110.761}, {\"0\": 119.993, \"1\": 97.195}, {\"0\": 109.781, \"1\": 120.045}, {\"0\": 149.958, \"1\": 108.398}, {\"0\": 122.65, \"1\": 162.003}, {\"0\": 101.78, \"1\": 126.012}, {\"0\": 83.948, \"1\": 91.714}, {\"0\": 125.004, \"1\": 120.038}, {\"0\": 197.989, \"1\": 99.999}, {\"0\": 94.029, \"1\": 121.999}, {\"0\": 107.984, \"1\": 155.742}, {\"0\": 114.995, \"1\": 124.954}, {\"0\": 154.023, \"1\": 108.567}, {\"0\": 100.004, \"1\": 86.468}, {\"0\": 94.99, \"1\": 128.181}, {\"0\": 99.717, \"1\": 112.357}, {\"0\": 111.405, \"1\": 108.03}, {\"0\": 94.88, \"1\": 113.997}, {\"0\": 161.977, \"1\": 85.023}, {\"0\": 127.938, \"1\": 95.012}, {\"0\": 107.506, \"1\": 132.048}, {\"0\": 92.984, \"1\": 81.971}, {\"0\": 78.063, \"1\": 159.882}, {\"0\": 188.256, \"1\": 129.991}, {\"0\": 104.98, \"1\": 112.378}, {\"0\": 79.871, \"1\": 97.995}, {\"0\": 85.482, \"1\": 123.006}, {\"0\": 142.027, \"1\": 145.843}, {\"0\": 129.958, \"1\": 123.658}, {\"0\": 159.927, \"1\": 79.638}, {\"0\": 101.384, \"1\": 94.922}, {\"0\": 100.304, \"1\": 113.507}, {\"0\": 152.117, \"1\": 109.977}, {\"0\": 174.063, \"1\": 136.894}, {\"0\": 95.245, \"1\": 105.009}, {\"0\": 126.013, \"1\": 115.032}, {\"0\": 99.581, \"1\": 139.725}, {\"0\": 171.908, \"1\": 124.677}, {\"0\": 140.019, \"1\": 111.057}, {\"0\": 125.923, \"1\": 120.011}, {\"0\": 77.857, \"1\": 116.388}, {\"0\": 73.033, \"1\": 141.345}, {\"0\": 108.962, \"1\": 174.004}, {\"0\": 103.006, \"1\": 108.166}, {\"0\": 101.999, \"1\": 150.072}, {\"0\": 169.923, \"1\": 105.909}, {\"0\": 155.932, \"1\": 102.428}, {\"0\": 116.308, \"1\": 126.705}, {\"0\": 123.052, \"1\": 136.996}, {\"0\": 98.939, \"1\": 144.154}, {\"0\": 124.045, \"1\": 119.999}, {\"0\": 124.037, \"1\": 150.13}, {\"0\": 93.003, \"1\": 101.534}, {\"0\": 126.033, \"1\": 148.04}, {\"0\": 131.206, \"1\": 71.607}, {\"0\": 141.916, \"1\": 86.018}, {\"0\": 110.958, \"1\": 100.894}, {\"0\": 101.958, \"1\": 119.999}, {\"0\": 153.919, \"1\": 85.338}, {\"0\": 124.964, \"1\": 135.95}, {\"0\": 135.866, \"1\": 129.004}, {\"0\": 90.16, \"1\": 137.932}, {\"0\": 111.643, \"1\": 97.51}, {\"0\": 127.964, \"1\": 130.141}, {\"0\": 130.069, \"1\": 100.344}, {\"0\": 156.004, \"1\": 99.989}, {\"0\": 82.353, \"1\": 132.002}, {\"0\": 133.981, \"1\": 125.004}, {\"0\": 102.035, \"1\": 122.906}, {\"0\": 101.959, \"1\": 170.002}, {\"0\": 149.998, \"1\": 124.012}, {\"0\": 129.957, \"1\": 114.959}, {\"0\": 121.054, \"1\": 164.025}, {\"0\": 127.977, \"1\": 111.044}, {\"0\": 110.023, \"1\": 123.094}, {\"0\": 104.827, \"1\": 127.356}, {\"0\": 79.554, \"1\": 126.012}, {\"0\": 130.048, \"1\": 94.498}, {\"0\": 109.774, \"1\": 131.69}, {\"0\": 127.993, \"1\": 93.641}, {\"0\": 117.819, \"1\": 151.817}, {\"0\": 119.002, \"1\": 118.994}, {\"0\": 175.914, \"1\": 116.081}, {\"0\": 115.005, \"1\": 159.939}, {\"0\": 93.957, \"1\": 138.034}, {\"0\": 107.999, \"1\": 100.089}, {\"0\": 82.142, \"1\": 123.985}, {\"0\": 80.035, \"1\": 141.209}, {\"0\": 126.021, \"1\": 169.904}, {\"0\": 138.009, \"1\": 130.268}, {\"0\": 117.021, \"1\": 219.331}, {\"0\": 120.025, \"1\": 137.595}, {\"0\": 96.029, \"1\": 131.952}, {\"0\": 94.013, \"1\": 126.019}, {\"0\": 121.989, \"1\": 96.99}, {\"0\": 97.985, \"1\": 153.547}, {\"0\": 77.487, \"1\": 144.084}, {\"0\": 145.107, \"1\": 120.012}, {\"0\": 84.858, \"1\": 79.337}, {\"0\": 110.041, \"1\": 106.077}, {\"0\": 77.521, \"1\": 96.056}, {\"0\": 149.915, \"1\": 174.062}, {\"0\": 92.742, \"1\": 121.993}, {\"0\": 130.018, \"1\": 128.289}, {\"0\": 69.009, \"1\": 122.025}, {\"0\": 82.53, \"1\": 91.558}, {\"0\": 102.003, \"1\": 141.226}, {\"0\": 135.944, \"1\": 166.649}, {\"0\": 64.97, \"1\": 112.008}, {\"0\": 131.935, \"1\": 128.014}, {\"0\": 89.956, \"1\": 124.996}, {\"0\": 104.851, \"1\": 114.243}, {\"0\": 116.631, \"1\": 147.65}, {\"0\": 109.961, \"1\": 116.629}, {\"0\": 181.893, \"1\": 158.859}, {\"0\": 97.98, \"1\": 149.948}, {\"0\": 114.926, \"1\": 121.999}, {\"0\": 160.023, \"1\": 149.988}, {\"0\": 119.993, \"1\": 80.126}, {\"0\": 115.697, \"1\": 99.223}, {\"0\": 89.99, \"1\": 105.266}, {\"0\": 77.013, \"1\": 122.415}, {\"0\": 130.052, \"1\": 120.975}, {\"0\": 94.364, \"1\": 142.008}, {\"0\": 153.901, \"1\": 127.029}, {\"0\": 115.266, \"1\": 125.988}, {\"0\": 126.681, \"1\": 122.199}, {\"0\": 96.308, \"1\": 96.988}, {\"0\": 113.306, \"1\": 140.04}, {\"0\": 63.938, \"1\": 139.639}, {\"0\": 122.041, \"1\": 135.894}, {\"0\": 104.563, \"1\": 120.761}, {\"0\": 110.926, \"1\": 140.103}, {\"0\": 166.904, \"1\": 100.344}, {\"0\": 136.227, \"1\": 95.237}, {\"0\": 171.945, \"1\": 179.955}, {\"0\": 88.931, \"1\": 173.937}, {\"0\": 120.006, \"1\": 126.477}, {\"0\": 120.026, \"1\": 93.287}, {\"0\": 94.954, \"1\": 132.475}, {\"0\": 100.01, \"1\": 128.044}, {\"0\": 146.661, \"1\": 103.521}, {\"0\": 160.114, \"1\": 92.187}, {\"0\": 189.932, \"1\": 116.392}, {\"0\": 88.007, \"1\": 117.996}, {\"0\": 102.974, \"1\": 160.083}, {\"0\": 109.689, \"1\": 102.128}, {\"0\": 122.988, \"1\": 118.032}, {\"0\": 119.95, \"1\": 135.959}, {\"0\": 89.98, \"1\": 121.099}, {\"0\": 77.987, \"1\": 167.922}, {\"0\": 107.327, \"1\": 139.937}, {\"0\": 151.545, \"1\": 159.967}, {\"0\": 93.022, \"1\": 124.045}, {\"0\": 91.0, \"1\": 111.038}, {\"0\": 128.027, \"1\": 125.012}, {\"0\": 92.909, \"1\": 124.654}, {\"0\": 149.931, \"1\": 140.084}, {\"0\": 129.004, \"1\": 175.88}, {\"0\": 155.855, \"1\": 187.302}, {\"0\": 65.23, \"1\": 85.041}, {\"0\": 170.137, \"1\": 195.978}, {\"0\": 102.026, \"1\": 134.02}, {\"0\": 128.268, \"1\": 120.074}, {\"0\": 159.841, \"1\": 125.008}, {\"0\": 110.026, \"1\": 130.684}, {\"0\": 115.642, \"1\": 117.501}, {\"0\": 87.791, \"1\": 99.938}, {\"0\": 123.988, \"1\": 140.037}, {\"0\": 128.004, \"1\": 102.831}, {\"0\": 76.403, \"1\": 131.619}, {\"0\": 106.002, \"1\": 141.221}, {\"0\": 143.919, \"1\": 177.125}, {\"0\": 118.04, \"1\": 161.933}, {\"0\": 117.299, \"1\": 98.027}, {\"0\": 95.078, \"1\": 102.652}, {\"0\": 145.88, \"1\": 125.025}, {\"0\": 128.032, \"1\": 121.93}, {\"0\": 126.026, \"1\": 131.942}, {\"0\": 132.876, \"1\": 124.311}, {\"0\": 75.019, \"1\": 124.989}, {\"0\": 100.831, \"1\": 112.0}, {\"0\": 144.994, \"1\": 100.997}, {\"0\": 149.989, \"1\": 160.043}, {\"0\": 149.923, \"1\": 110.615}, {\"0\": 134.027, \"1\": 116.986}, {\"0\": 113.962, \"1\": 108.991}, {\"0\": 122.997, \"1\": 140.943}, {\"0\": 150.005, \"1\": 129.948}, {\"0\": 133.881, \"1\": 103.459}, {\"0\": 100.0, \"1\": 87.99}, {\"0\": 95.028, \"1\": 96.115}, {\"0\": 128.062, \"1\": 112.137}, {\"0\": 118.709, \"1\": 101.698}, {\"0\": 99.964, \"1\": 133.992}, {\"0\": 117.283, \"1\": 126.047}, {\"0\": 96.0, \"1\": 83.204}, {\"0\": 74.827, \"1\": 137.895}, {\"0\": 140.037, \"1\": 93.024}, {\"0\": 149.962, \"1\": 149.97}, {\"0\": 115.985, \"1\": 91.963}, {\"0\": 134.066, \"1\": 108.496}, {\"0\": 169.983, \"1\": 83.021}, {\"0\": 82.757, \"1\": 112.98}, {\"0\": 110.008, \"1\": 79.952}, {\"0\": 88.025, \"1\": 132.002}, {\"0\": 103.637, \"1\": 120.961}, {\"0\": 76.493, \"1\": 136.958}, {\"0\": 120.002, \"1\": 79.952}, {\"0\": 98.606, \"1\": 90.493}, {\"0\": 172.069, \"1\": 128.001}, {\"0\": 138.309, \"1\": 99.989}, {\"0\": 172.008, \"1\": 111.958}, {\"0\": 159.813, \"1\": 119.998}, {\"0\": 152.011, \"1\": 98.002}, {\"0\": 93.31, \"1\": 90.999}, {\"0\": 140.029, \"1\": 89.988}, {\"0\": 100.024, \"1\": 169.792}, {\"0\": 89.983, \"1\": 120.368}, {\"0\": 153.113, \"1\": 128.995}, {\"0\": 174.324, \"1\": 112.011}, {\"0\": 117.119, \"1\": 134.957}, {\"0\": 102.0, \"1\": 125.998}, {\"0\": 81.34, \"1\": 113.907}, {\"0\": 142.046, \"1\": 159.912}, {\"0\": 100.071, \"1\": 119.674}, {\"0\": 112.042, \"1\": 112.065}, {\"0\": 131.644, \"1\": 121.971}, {\"0\": 92.097, \"1\": 141.91}, {\"0\": 65.125, \"1\": 114.865}, {\"0\": 126.989, \"1\": 68.456}, {\"0\": 104.881, \"1\": 134.981}, {\"0\": 160.125, \"1\": 131.75}, {\"0\": 147.976, \"1\": 128.015}, {\"0\": 95.438, \"1\": 113.263}, {\"0\": 120.963, \"1\": 140.008}, {\"0\": 74.933, \"1\": 149.51}, {\"0\": 107.946, \"1\": 172.176}, {\"0\": 125.962, \"1\": 120.462}, {\"0\": 174.012, \"1\": 123.97}, {\"0\": 119.972, \"1\": 100.004}, {\"0\": 128.014, \"1\": 189.935}, {\"0\": 169.986, \"1\": 80.04}, {\"0\": 151.928, \"1\": 130.025}, {\"0\": 104.012, \"1\": 128.004}, {\"0\": 117.539, \"1\": 125.987}, {\"0\": 108.038, \"1\": 130.073}, {\"0\": 125.988, \"1\": 136.05}, {\"0\": 107.991, \"1\": 122.967}, {\"0\": 126.027, \"1\": 81.997}, {\"0\": 92.934, \"1\": 115.732}, {\"0\": 154.906, \"1\": 123.883}, {\"0\": 104.889, \"1\": 107.699}, {\"0\": 125.035, \"1\": 86.969}, {\"0\": 140.715, \"1\": 135.223}, {\"0\": 85.899, \"1\": 100.006}, {\"0\": 86.981, \"1\": 139.876}, {\"0\": 88.965, \"1\": 132.036}, {\"0\": 78.311, \"1\": 97.524}, {\"0\": 115.48, \"1\": 176.972}, {\"0\": 88.015, \"1\": 166.999}, {\"0\": 105.932, \"1\": 99.988}, {\"0\": 122.948, \"1\": 130.031}, {\"0\": 103.119, \"1\": 157.993}, {\"0\": 120.329, \"1\": 77.92}, {\"0\": 131.02, \"1\": 127.994}, {\"0\": 110.007, \"1\": 150.068}, {\"0\": 116.038, \"1\": 151.362}, {\"0\": 113.961, \"1\": 91.98}, {\"0\": 126.057, \"1\": 77.516}, {\"0\": 123.208, \"1\": 120.986}, {\"0\": 122.999, \"1\": 139.939}, {\"0\": 139.815, \"1\": 100.039}, {\"0\": 169.889, \"1\": 112.007}, {\"0\": 130.05, \"1\": 124.015}, {\"0\": 106.259, \"1\": 73.061}, {\"0\": 183.76, \"1\": 96.99}, {\"0\": 124.976, \"1\": 139.87}, {\"0\": 119.983, \"1\": 160.188}, {\"0\": 162.435, \"1\": 91.007}, {\"0\": 91.961, \"1\": 126.024}, {\"0\": 127.515, \"1\": 105.029}, {\"0\": 127.93, \"1\": 145.014}, {\"0\": 168.001, \"1\": 98.946}, {\"0\": 186.241, \"1\": 110.815}, {\"0\": 100.221, \"1\": 141.84}, {\"0\": 129.623, \"1\": 123.033}, {\"0\": 130.813, \"1\": 136.213}, {\"0\": 108.347, \"1\": 189.992}, {\"0\": 128.014, \"1\": 115.021}, {\"0\": 115.996, \"1\": 87.569}, {\"0\": 86.26, \"1\": 103.127}, {\"0\": 139.97, \"1\": 110.983}, {\"0\": 177.916, \"1\": 99.935}, {\"0\": 131.988, \"1\": 109.953}, {\"0\": 156.066, \"1\": 142.8}, {\"0\": 124.105, \"1\": 129.424}, {\"0\": 203.822, \"1\": 91.96}, {\"0\": 187.961, \"1\": 135.006}, {\"0\": 112.053, \"1\": 74.749}, {\"0\": 199.519, \"1\": 155.974}, {\"0\": 177.833, \"1\": 130.028}, {\"0\": 124.303, \"1\": 157.013}, {\"0\": 156.295, \"1\": 149.902}, {\"0\": 97.957, \"1\": 113.564}, {\"0\": 133.758, \"1\": 117.973}, {\"0\": 139.87, \"1\": 123.017}, {\"0\": 175.822, \"1\": 93.193}, {\"0\": 128.004, \"1\": 144.952}, {\"0\": 135.045, \"1\": 182.969}, {\"0\": 119.952, \"1\": 120.001}, {\"0\": 138.944, \"1\": 93.603}, {\"0\": 125.923, \"1\": 95.019}, {\"0\": 98.007, \"1\": 150.009}, {\"0\": 95.002, \"1\": 139.94}, {\"0\": 127.075, \"1\": 125.325}, {\"0\": 166.004, \"1\": 119.964}, {\"0\": 122.003, \"1\": 140.058}, {\"0\": 152.015, \"1\": 108.943}, {\"0\": 92.963, \"1\": 104.996}, {\"0\": 81.856, \"1\": 122.065}, {\"0\": 168.527, \"1\": 130.035}, {\"0\": 97.9, \"1\": 98.004}, {\"0\": 96.493, \"1\": 187.027}, {\"0\": 99.977, \"1\": 100.018}, {\"0\": 103.963, \"1\": 95.07}, {\"0\": 124.943, \"1\": 92.584}, {\"0\": 141.607, \"1\": 140.203}, {\"0\": 142.952, \"1\": 180.559}, {\"0\": 116.049, \"1\": 87.675}, {\"0\": 95.058, \"1\": 106.059}, {\"0\": 140.123, \"1\": 97.835}, {\"0\": 150.181, \"1\": 118.449}, {\"0\": 95.952, \"1\": 146.15}, {\"0\": 97.967, \"1\": 107.363}, {\"0\": 110.091, \"1\": 141.996}, {\"0\": 128.025, \"1\": 103.709}, {\"0\": 119.567, \"1\": 109.998}, {\"0\": 92.021, \"1\": 129.857}, {\"0\": 170.032, \"1\": 160.14}, {\"0\": 128.011, \"1\": 130.007}, {\"0\": 151.914, \"1\": 121.99}, {\"0\": 135.877, \"1\": 117.514}, {\"0\": 97.042, \"1\": 120.004}, {\"0\": 109.989, \"1\": 188.046}, {\"0\": 79.277, \"1\": 85.951}, {\"0\": 138.976, \"1\": 118.662}, {\"0\": 90.002, \"1\": 100.032}, {\"0\": 88.997, \"1\": 100.518}, {\"0\": 91.964, \"1\": 124.922}, {\"0\": 134.369, \"1\": 131.99}, {\"0\": 179.984, \"1\": 92.003}, {\"0\": 188.058, \"1\": 90.986}, {\"0\": 128.806, \"1\": 66.711}, {\"0\": 72.695, \"1\": 152.839}, {\"0\": 131.659, \"1\": 143.948}, {\"0\": 148.121, \"1\": 81.678}, {\"0\": 125.94, \"1\": 119.975}, {\"0\": 90.89, \"1\": 141.378}, {\"0\": 95.003, \"1\": 148.436}, {\"0\": 166.151, \"1\": 112.014}, {\"0\": 170.139, \"1\": 152.219}, {\"0\": 159.973, \"1\": 96.671}, {\"0\": 84.009, \"1\": 112.7}, {\"0\": 150.038, \"1\": 140.067}, {\"0\": 114.679, \"1\": 143.944}, {\"0\": 127.942, \"1\": 153.977}, {\"0\": 81.966, \"1\": 123.997}, {\"0\": 127.894, \"1\": 122.79}, {\"0\": 150.001, \"1\": 105.045}, {\"0\": 135.819, \"1\": 171.893}, {\"0\": 109.878, \"1\": 191.404}, {\"0\": 118.176, \"1\": 110.037}, {\"0\": 132.023, \"1\": 147.072}, {\"0\": 90.007, \"1\": 126.995}, {\"0\": 125.989, \"1\": 121.374}, {\"0\": 103.087, \"1\": 130.009}, {\"0\": 127.997, \"1\": 110.325}, {\"0\": 106.035, \"1\": 121.922}, {\"0\": 134.944, \"1\": 129.729}, {\"0\": 80.018, \"1\": 140.974}, {\"0\": 121.387, \"1\": 126.99}, {\"0\": 130.008, \"1\": 80.032}, {\"0\": 121.986, \"1\": 108.006}, {\"0\": 96.394, \"1\": 170.678}, {\"0\": 156.107, \"1\": 122.663}, {\"0\": 96.822, \"1\": 77.505}, {\"0\": 110.018, \"1\": 110.325}, {\"0\": 99.05, \"1\": 142.008}, {\"0\": 161.864, \"1\": 101.797}, {\"0\": 115.076, \"1\": 99.996}, {\"0\": 78.829, \"1\": 105.993}, {\"0\": 83.625, \"1\": 80.021}, {\"0\": 161.776, \"1\": 111.027}, {\"0\": 88.357, \"1\": 104.63}, {\"0\": 98.434, \"1\": 184.012}, {\"0\": 130.006, \"1\": 120.286}, {\"0\": 112.377, \"1\": 128.339}, {\"0\": 90.046, \"1\": 119.996}, {\"0\": 176.014, \"1\": 122.002}, {\"0\": 125.951, \"1\": 100.262}, {\"0\": 129.992, \"1\": 127.681}, {\"0\": 168.032, \"1\": 131.023}, {\"0\": 100.03, \"1\": 81.004}, {\"0\": 149.964, \"1\": 124.111}, {\"0\": 104.029, \"1\": 104.981}, {\"0\": 145.889, \"1\": 124.971}, {\"0\": 130.015, \"1\": 105.997}, {\"0\": 99.99, \"1\": 170.179}, {\"0\": 121.019, \"1\": 120.832}, {\"0\": 90.999, \"1\": 122.362}, {\"0\": 130.057, \"1\": 129.952}, {\"0\": 129.491, \"1\": 160.026}, {\"0\": 102.358, \"1\": 107.826}, {\"0\": 81.049, \"1\": 124.998}, {\"0\": 94.954, \"1\": 136.024}, {\"0\": 144.073, \"1\": 167.879}, {\"0\": 171.927, \"1\": 152.826}, {\"0\": 147.124, \"1\": 123.991}, {\"0\": 100.048, \"1\": 121.241}, {\"0\": 127.978, \"1\": 99.96}, {\"0\": 103.999, \"1\": 107.234}, {\"0\": 99.002, \"1\": 95.971}, {\"0\": 142.726, \"1\": 123.966}, {\"0\": 139.922, \"1\": 123.966}, {\"0\": 150.062, \"1\": 129.921}, {\"0\": 128.054, \"1\": 87.93}, {\"0\": 118.539, \"1\": 120.973}, {\"0\": 110.046, \"1\": 124.297}, {\"0\": 95.031, \"1\": 119.987}, {\"0\": 127.397, \"1\": 135.989}, {\"0\": 135.925, \"1\": 125.371}, {\"0\": 157.938, \"1\": 122.994}, {\"0\": 90.011, \"1\": 90.852}, {\"0\": 95.037, \"1\": 128.041}, {\"0\": 101.972, \"1\": 98.977}, {\"0\": 124.994, \"1\": 81.179}, {\"0\": 116.956, \"1\": 120.022}, {\"0\": 121.37, \"1\": 89.094}, {\"0\": 160.064, \"1\": 122.01}, {\"0\": 175.95, \"1\": 90.336}, {\"0\": 144.014, \"1\": 128.011}, {\"0\": 140.911, \"1\": 106.446}, {\"0\": 130.14, \"1\": 94.569}, {\"0\": 125.011, \"1\": 132.063}, {\"0\": 121.03, \"1\": 127.975}, {\"0\": 95.746, \"1\": 95.04}, {\"0\": 150.082, \"1\": 140.042}, {\"0\": 112.02, \"1\": 133.948}, {\"0\": 120.032, \"1\": 124.036}, {\"0\": 80.996, \"1\": 119.964}, {\"0\": 184.059, \"1\": 105.018}, {\"0\": 80.126, \"1\": 108.066}, {\"0\": 82.897, \"1\": 102.532}, {\"0\": 130.12, \"1\": 125.999}, {\"0\": 145.06, \"1\": 126.013}, {\"0\": 88.933, \"1\": 131.946}, {\"0\": 125.006, \"1\": 142.065}, {\"0\": 124.043, \"1\": 127.014}, {\"0\": 126.064, \"1\": 99.999}, {\"0\": 110.832, \"1\": 132.073}, {\"0\": 112.017, \"1\": 122.416}, {\"0\": 85.03, \"1\": 148.23}, {\"0\": 110.991, \"1\": 194.055}, {\"0\": 99.035, \"1\": 129.963}, {\"0\": 97.8, \"1\": 85.023}, {\"0\": 144.986, \"1\": 123.013}, {\"0\": 115.0, \"1\": 120.007}, {\"0\": 110.54, \"1\": 116.846}, {\"0\": 177.912, \"1\": 133.064}, {\"0\": 96.999, \"1\": 142.584}, {\"0\": 176.048, \"1\": 144.001}, {\"0\": 134.469, \"1\": 168.045}, {\"0\": 80.171, \"1\": 105.128}, {\"0\": 74.923, \"1\": 93.618}, {\"0\": 113.946, \"1\": 135.406}, {\"0\": 127.925, \"1\": 114.696}, {\"0\": 96.016, \"1\": 86.733}, {\"0\": 92.96, \"1\": 93.019}, {\"0\": 140.013, \"1\": 94.495}, {\"0\": 125.927, \"1\": 124.944}, {\"0\": 113.106, \"1\": 64.992}, {\"0\": 108.009, \"1\": 122.316}, {\"0\": 87.024, \"1\": 130.047}, {\"0\": 127.987, \"1\": 95.009}, {\"0\": 100.031, \"1\": 127.82}, {\"0\": 125.024, \"1\": 122.026}, {\"0\": 99.018, \"1\": 99.994}, {\"0\": 80.117, \"1\": 140.992}, {\"0\": 86.948, \"1\": 105.624}, {\"0\": 190.013, \"1\": 128.604}, {\"0\": 110.082, \"1\": 174.589}, {\"0\": 99.968, \"1\": 85.125}, {\"0\": 167.992, \"1\": 116.021}, {\"0\": 140.048, \"1\": 119.296}, {\"0\": 138.021, \"1\": 118.001}, {\"0\": 139.522, \"1\": 96.616}, {\"0\": 93.984, \"1\": 100.04}, {\"0\": 59.385, \"1\": 116.049}, {\"0\": 161.905, \"1\": 145.05}, {\"0\": 162.892, \"1\": 122.3}, {\"0\": 98.003, \"1\": 173.954}, {\"0\": 151.456, \"1\": 118.008}, {\"0\": 102.029, \"1\": 127.081}, {\"0\": 127.992, \"1\": 126.012}, {\"0\": 104.986, \"1\": 124.046}, {\"0\": 74.784, \"1\": 127.989}, {\"0\": 91.7, \"1\": 80.205}, {\"0\": 168.385, \"1\": 92.215}, {\"0\": 100.034, \"1\": 105.967}, {\"0\": 98.205, \"1\": 94.017}, {\"0\": 112.94, \"1\": 200.035}, {\"0\": 94.231, \"1\": 142.019}, {\"0\": 131.049, \"1\": 104.322}, {\"0\": 106.015, \"1\": 80.004}, {\"0\": 90.005, \"1\": 115.076}, {\"0\": 90.008, \"1\": 136.029}, {\"0\": 136.869, \"1\": 119.72}, {\"0\": 85.998, \"1\": 143.98}, {\"0\": 125.043, \"1\": 105.004}, {\"0\": 119.985, \"1\": 85.045}, {\"0\": 94.296, \"1\": 105.976}, {\"0\": 72.28, \"1\": 144.005}, {\"0\": 92.973, \"1\": 133.295}, {\"0\": 110.821, \"1\": 75.548}, {\"0\": 128.059, \"1\": 149.812}, {\"0\": 143.514, \"1\": 80.694}, {\"0\": 77.092, \"1\": 107.651}, {\"0\": 87.539, \"1\": 165.998}, {\"0\": 114.059, \"1\": 125.993}, {\"0\": 163.938, \"1\": 107.998}, {\"0\": 141.967, \"1\": 140.061}, {\"0\": 97.97, \"1\": 130.034}, {\"0\": 97.721, \"1\": 124.882}, {\"0\": 122.984, \"1\": 132.033}, {\"0\": 107.024, \"1\": 124.0}, {\"0\": 134.006, \"1\": 114.28}, {\"0\": 93.973, \"1\": 127.984}, {\"0\": 176.003, \"1\": 116.556}, {\"0\": 117.264, \"1\": 125.02}, {\"0\": 80.329, \"1\": 153.009}, {\"0\": 121.996, \"1\": 113.12}, {\"0\": 150.011, \"1\": 124.997}, {\"0\": 139.91, \"1\": 89.245}, {\"0\": 147.309, \"1\": 123.999}, {\"0\": 128.083, \"1\": 129.87}, {\"0\": 156.177, \"1\": 143.018}, {\"0\": 122.012, \"1\": 92.333}, {\"0\": 85.929, \"1\": 165.844}, {\"0\": 165.103, \"1\": 98.496}, {\"0\": 79.513, \"1\": 76.27}, {\"0\": null, \"1\": 139.997}, {\"0\": null, \"1\": 116.047}, {\"0\": null, \"1\": 152.096}, {\"0\": null, \"1\": 173.964}, {\"0\": null, \"1\": 119.977}, {\"0\": null, \"1\": 146.923}, {\"0\": null, \"1\": 92.001}, {\"0\": null, \"1\": 111.906}, {\"0\": null, \"1\": 76.467}, {\"0\": null, \"1\": 148.009}, {\"0\": null, \"1\": 124.029}, {\"0\": null, \"1\": 77.021}, {\"0\": null, \"1\": 81.805}, {\"0\": null, \"1\": 78.008}, {\"0\": null, \"1\": 123.922}, {\"0\": null, \"1\": 122.978}, {\"0\": null, \"1\": 127.026}, {\"0\": null, \"1\": 124.996}, {\"0\": null, \"1\": 110.639}, {\"0\": null, \"1\": 131.951}, {\"0\": null, \"1\": 104.764}, {\"0\": null, \"1\": 102.96}, {\"0\": null, \"1\": 125.011}, {\"0\": null, \"1\": 117.741}, {\"0\": null, \"1\": 118.006}, {\"0\": null, \"1\": 92.995}, {\"0\": null, \"1\": 109.014}, {\"0\": null, \"1\": 109.721}, {\"0\": null, \"1\": 114.845}, {\"0\": null, \"1\": 122.992}, {\"0\": null, \"1\": 107.039}, {\"0\": null, \"1\": 87.996}, {\"0\": null, \"1\": 119.999}, {\"0\": null, \"1\": 114.999}, {\"0\": null, \"1\": 99.876}, {\"0\": null, \"1\": 112.575}, {\"0\": null, \"1\": 209.686}, {\"0\": null, \"1\": 139.965}, {\"0\": null, \"1\": 125.462}], \"data-b63207ba6e5359e75461234396b8b8d8\": [{\"0\": 0.561, \"1\": 0.819}, {\"0\": 0.968, \"1\": 0.873}, {\"0\": 0.758, \"1\": 0.852}, {\"0\": 0.701, \"1\": 0.684}, {\"0\": 0.497, \"1\": 0.504}, {\"0\": 0.98, \"1\": 0.691}, {\"0\": 0.0562, \"1\": 0.755}, {\"0\": 0.878, \"1\": 0.982}, {\"0\": 0.803, \"1\": 0.705}, {\"0\": 0.96, \"1\": 0.612}, {\"0\": 0.309, \"1\": 0.571}, {\"0\": 0.326, \"1\": 0.948}, {\"0\": 0.762, \"1\": 0.776}, {\"0\": 0.459, \"1\": 0.657}, {\"0\": 0.524, \"1\": 0.875}, {\"0\": 0.623, \"1\": 0.489}, {\"0\": 0.57, \"1\": 0.794}, {\"0\": 0.756, \"1\": 0.453}, {\"0\": 0.58, \"1\": 0.512}, {\"0\": 0.756, \"1\": 0.585}, {\"0\": 0.869, \"1\": 0.941}, {\"0\": 0.989, \"1\": 0.77}, {\"0\": 0.602, \"1\": 0.766}, {\"0\": 0.592, \"1\": 0.714}, {\"0\": 0.921, \"1\": 0.961}, {\"0\": 0.669, \"1\": 0.461}, {\"0\": 0.718, \"1\": 0.724}, {\"0\": 0.705, \"1\": 0.539}, {\"0\": 0.909, \"1\": 0.215}, {\"0\": 0.656, \"1\": 0.931}, {\"0\": 0.664, \"1\": 0.462}, {\"0\": 0.974, \"1\": 0.696}, {\"0\": 0.78, \"1\": 0.807}, {\"0\": 0.774, \"1\": 0.603}, {\"0\": 0.907, \"1\": 0.217}, {\"0\": 0.829, \"1\": 0.671}, {\"0\": 0.725, \"1\": 0.79}, {\"0\": 0.83, \"1\": 0.86}, {\"0\": 0.156, \"1\": 0.488}, {\"0\": 0.761, \"1\": 0.545}, {\"0\": 0.456, \"1\": 0.46}, {\"0\": 0.802, \"1\": 0.78}, {\"0\": 0.815, \"1\": 0.635}, {\"0\": 0.531, \"1\": 0.648}, {\"0\": 0.3, \"1\": 0.824}, {\"0\": 0.891, \"1\": 0.719}, {\"0\": 0.657, \"1\": 0.862}, {\"0\": 0.579, \"1\": 0.904}, {\"0\": 0.596, \"1\": 0.94}, {\"0\": 0.447, \"1\": 0.819}, {\"0\": 0.781, \"1\": 0.817}, {\"0\": 0.257, \"1\": 0.77}, {\"0\": 0.87, \"1\": 0.179}, {\"0\": 0.89, \"1\": 0.764}, {\"0\": 0.691, \"1\": 0.461}, {\"0\": 0.683, \"1\": 0.822}, {\"0\": 0.724, \"1\": 0.847}, {\"0\": 0.687, \"1\": 0.41}, {\"0\": 0.727, \"1\": 0.905}, {\"0\": 0.648, \"1\": 0.947}, {\"0\": 0.77, \"1\": 0.851}, {\"0\": 0.0302, \"1\": 0.503}, {\"0\": 0.985, \"1\": 0.645}, {\"0\": 0.988, \"1\": 0.74}, {\"0\": 0.685, \"1\": 0.638}, {\"0\": 0.744, \"1\": 0.436}, {\"0\": 0.811, \"1\": 0.664}, {\"0\": 0.828, \"1\": 0.719}, {\"0\": 0.961, \"1\": 0.628}, {\"0\": 0.498, \"1\": 0.823}, {\"0\": 0.856, \"1\": 0.781}, {\"0\": 0.929, \"1\": 0.29}, {\"0\": 0.702, \"1\": 0.784}, {\"0\": 0.799, \"1\": 0.801}, {\"0\": 0.843, \"1\": 0.844}, {\"0\": 0.884, \"1\": 0.697}, {\"0\": 0.798, \"1\": 0.704}, {\"0\": 0.364, \"1\": 0.862}, {\"0\": 0.895, \"1\": 0.708}, {\"0\": 0.944, \"1\": 0.499}, {\"0\": 0.0416, \"1\": 0.672}, {\"0\": 0.705, \"1\": 0.554}, {\"0\": 0.678, \"1\": 0.43}, {\"0\": 0.563, \"1\": 0.647}, {\"0\": 0.624, \"1\": 0.478}, {\"0\": 0.227, \"1\": 0.509}, {\"0\": 0.838, \"1\": 0.538}, {\"0\": 0.896, \"1\": 0.528}, {\"0\": 0.592, \"1\": 0.878}, {\"0\": 0.947, \"1\": 0.916}, {\"0\": 0.114, \"1\": 0.855}, {\"0\": 0.107, \"1\": 0.593}, {\"0\": 0.631, \"1\": 0.737}, {\"0\": 0.796, \"1\": 0.533}, {\"0\": 0.949, \"1\": 0.555}, {\"0\": 0.636, \"1\": 0.378}, {\"0\": 0.337, \"1\": 0.901}, {\"0\": 0.864, \"1\": 0.581}, {\"0\": 0.7, \"1\": 0.717}, {\"0\": 0.791, \"1\": 0.793}, {\"0\": 0.769, \"1\": 0.547}, {\"0\": 0.836, \"1\": 0.576}, {\"0\": 0.553, \"1\": 0.892}, {\"0\": 0.829, \"1\": 0.794}, {\"0\": 0.158, \"1\": 0.629}, {\"0\": 0.671, \"1\": 0.789}, {\"0\": 0.811, \"1\": 0.74}, {\"0\": 0.445, \"1\": 0.942}, {\"0\": 0.404, \"1\": 0.886}, {\"0\": 0.92, \"1\": 0.875}, {\"0\": 0.578, \"1\": 0.498}, {\"0\": 0.838, \"1\": 0.697}, {\"0\": 0.437, \"1\": 0.406}, {\"0\": 0.983, \"1\": 0.688}, {\"0\": 0.891, \"1\": 0.872}, {\"0\": 0.943, \"1\": 0.343}, {\"0\": 0.928, \"1\": 0.76}, {\"0\": 0.487, \"1\": 0.581}, {\"0\": 0.888, \"1\": 0.918}, {\"0\": 0.692, \"1\": 0.449}, {\"0\": 0.0626, \"1\": 0.808}, {\"0\": 0.667, \"1\": 0.669}, {\"0\": 0.862, \"1\": 0.858}, {\"0\": 0.823, \"1\": 0.61}, {\"0\": 0.478, \"1\": 0.623}, {\"0\": 0.692, \"1\": 0.416}, {\"0\": 0.769, \"1\": 0.64}, {\"0\": 0.955, \"1\": 0.851}, {\"0\": 0.661, \"1\": 0.617}, {\"0\": 0.737, \"1\": 0.625}, {\"0\": 0.642, \"1\": 0.68}, {\"0\": 0.707, \"1\": 0.776}, {\"0\": 0.657, \"1\": 0.699}, {\"0\": 0.841, \"1\": 0.526}, {\"0\": 0.554, \"1\": 0.484}, {\"0\": 0.697, \"1\": 0.884}, {\"0\": 0.955, \"1\": 0.712}, {\"0\": 0.561, \"1\": 0.936}, {\"0\": 0.484, \"1\": 0.634}, {\"0\": 0.844, \"1\": 0.857}, {\"0\": 0.741, \"1\": 0.773}, {\"0\": 0.652, \"1\": 0.846}, {\"0\": 0.0347, \"1\": 0.871}, {\"0\": 0.86, \"1\": 0.834}, {\"0\": 0.933, \"1\": 0.737}, {\"0\": 0.575, \"1\": 0.68}, {\"0\": 0.124, \"1\": 0.647}, {\"0\": 0.748, \"1\": 0.957}, {\"0\": 0.544, \"1\": 0.901}, {\"0\": 0.507, \"1\": 0.594}, {\"0\": 0.941, \"1\": 0.394}, {\"0\": 0.684, \"1\": 0.833}, {\"0\": 0.796, \"1\": 0.844}, {\"0\": 0.654, \"1\": 0.872}, {\"0\": 0.274, \"1\": 0.901}, {\"0\": 0.439, \"1\": 0.666}, {\"0\": 0.507, \"1\": 0.795}, {\"0\": 0.882, \"1\": 0.261}, {\"0\": 0.867, \"1\": 0.957}, {\"0\": 0.672, \"1\": 0.838}, {\"0\": 0.716, \"1\": 0.616}, {\"0\": 0.722, \"1\": 0.74}, {\"0\": 0.328, \"1\": 0.658}, {\"0\": 0.656, \"1\": 0.949}, {\"0\": 0.913, \"1\": 0.854}, {\"0\": 0.947, \"1\": 0.859}, {\"0\": 0.917, \"1\": 0.868}, {\"0\": 0.575, \"1\": 0.876}, {\"0\": 0.677, \"1\": 0.784}, {\"0\": 0.993, \"1\": 0.86}, {\"0\": 0.677, \"1\": 0.692}, {\"0\": 0.0757, \"1\": 0.619}, {\"0\": 0.433, \"1\": 0.573}, {\"0\": 0.71, \"1\": 0.402}, {\"0\": 0.964, \"1\": 0.761}, {\"0\": 0.925, \"1\": 0.682}, {\"0\": 0.857, \"1\": 0.703}, {\"0\": 0.977, \"1\": 0.604}, {\"0\": 0.867, \"1\": 0.657}, {\"0\": 0.303, \"1\": 0.781}, {\"0\": 0.212, \"1\": 0.235}, {\"0\": 0.918, \"1\": 0.663}, {\"0\": 0.602, \"1\": 0.98}, {\"0\": 0.533, \"1\": 0.759}, {\"0\": 0.12, \"1\": 0.742}, {\"0\": 0.848, \"1\": 0.506}, {\"0\": 0.135, \"1\": 0.514}, {\"0\": 0.943, \"1\": 0.786}, {\"0\": 0.962, \"1\": 0.523}, {\"0\": 0.591, \"1\": 0.912}, {\"0\": 0.126, \"1\": 0.566}, {\"0\": 0.489, \"1\": 0.537}, {\"0\": 0.141, \"1\": 0.785}, {\"0\": 0.707, \"1\": 0.621}, {\"0\": 0.649, \"1\": 0.642}, {\"0\": 0.883, \"1\": 0.839}, {\"0\": 0.823, \"1\": 0.626}, {\"0\": 0.508, \"1\": 0.21}, {\"0\": 0.618, \"1\": 0.637}, {\"0\": 0.553, \"1\": 0.833}, {\"0\": 0.622, \"1\": 0.724}, {\"0\": 0.852, \"1\": 0.555}, {\"0\": 0.829, \"1\": 0.62}, {\"0\": 0.549, \"1\": 0.915}, {\"0\": 0.927, \"1\": 0.894}, {\"0\": 0.582, \"1\": 0.725}, {\"0\": 0.801, \"1\": 0.553}, {\"0\": 0.271, \"1\": 0.457}, {\"0\": 0.881, \"1\": 0.813}, {\"0\": 0.748, \"1\": 0.317}, {\"0\": 0.393, \"1\": 0.818}, {\"0\": 0.814, \"1\": 0.422}, {\"0\": 0.983, \"1\": 0.849}, {\"0\": 0.866, \"1\": 0.599}, {\"0\": 0.583, \"1\": 0.909}, {\"0\": 0.586, \"1\": 0.421}, {\"0\": 0.657, \"1\": 0.608}, {\"0\": 0.936, \"1\": 0.716}, {\"0\": 0.818, \"1\": 0.922}, {\"0\": 0.689, \"1\": 0.843}, {\"0\": 0.786, \"1\": 0.715}, {\"0\": 0.992, \"1\": 0.792}, {\"0\": 0.816, \"1\": 0.758}, {\"0\": 0.54, \"1\": 0.657}, {\"0\": 0.705, \"1\": 0.979}, {\"0\": 0.105, \"1\": 0.616}, {\"0\": 0.532, \"1\": 0.722}, {\"0\": 0.69, \"1\": 0.734}, {\"0\": 0.469, \"1\": 0.582}, {\"0\": 0.485, \"1\": 0.778}, {\"0\": 0.79, \"1\": 0.63}, {\"0\": 0.14, \"1\": 0.83}, {\"0\": 0.738, \"1\": 0.757}, {\"0\": 0.523, \"1\": 0.292}, {\"0\": 0.844, \"1\": 0.524}, {\"0\": 0.602, \"1\": 0.647}, {\"0\": 0.582, \"1\": 0.721}, {\"0\": 0.891, \"1\": 0.755}, {\"0\": 0.757, \"1\": 0.435}, {\"0\": 0.956, \"1\": 0.65}, {\"0\": 0.676, \"1\": 0.751}, {\"0\": 0.219, \"1\": 0.882}, {\"0\": 0.842, \"1\": 0.641}, {\"0\": 0.944, \"1\": 0.95}, {\"0\": 0.894, \"1\": 0.798}, {\"0\": 0.669, \"1\": 0.481}, {\"0\": 0.443, \"1\": 0.682}, {\"0\": 0.896, \"1\": 0.811}, {\"0\": 0.695, \"1\": 0.579}, {\"0\": 0.625, \"1\": 0.338}, {\"0\": 0.98, \"1\": 0.71}, {\"0\": 0.398, \"1\": 0.401}, {\"0\": 0.355, \"1\": 0.857}, {\"0\": 0.148, \"1\": 0.798}, {\"0\": 0.824, \"1\": 0.515}, {\"0\": 0.838, \"1\": 0.464}, {\"0\": 0.217, \"1\": 0.892}, {\"0\": 0.908, \"1\": 0.888}, {\"0\": 0.82, \"1\": 0.827}, {\"0\": 0.681, \"1\": 0.929}, {\"0\": 0.512, \"1\": 0.552}, {\"0\": 0.436, \"1\": 0.415}, {\"0\": 0.593, \"1\": 0.744}, {\"0\": 0.976, \"1\": 0.704}, {\"0\": 0.94, \"1\": 0.805}, {\"0\": 0.96, \"1\": 0.656}, {\"0\": 0.627, \"1\": 0.512}, {\"0\": 0.576, \"1\": 0.64}, {\"0\": 0.527, \"1\": 0.795}, {\"0\": 0.821, \"1\": 0.568}, {\"0\": 0.946, \"1\": 0.712}, {\"0\": 0.642, \"1\": 0.623}, {\"0\": 0.652, \"1\": 0.78}, {\"0\": 0.878, \"1\": 0.913}, {\"0\": 0.989, \"1\": 0.366}, {\"0\": 0.912, \"1\": 0.706}, {\"0\": 0.672, \"1\": 0.216}, {\"0\": 0.747, \"1\": 0.602}, {\"0\": 0.187, \"1\": 0.561}, {\"0\": 0.564, \"1\": 0.575}, {\"0\": 0.559, \"1\": 0.978}, {\"0\": 0.778, \"1\": 0.751}, {\"0\": 0.739, \"1\": 0.571}, {\"0\": 0.307, \"1\": 0.878}, {\"0\": 0.786, \"1\": 0.387}, {\"0\": 0.0918, \"1\": 0.348}, {\"0\": 0.746, \"1\": 0.43}, {\"0\": 0.5, \"1\": 0.71}, {\"0\": 0.711, \"1\": 0.633}, {\"0\": 0.81, \"1\": 0.442}, {\"0\": 0.429, \"1\": 0.676}, {\"0\": 0.32, \"1\": 0.75}, {\"0\": 0.89, \"1\": 0.811}, {\"0\": 0.626, \"1\": 0.687}, {\"0\": 0.677, \"1\": 0.791}, {\"0\": 0.351, \"1\": 0.744}, {\"0\": 0.542, \"1\": 0.4}, {\"0\": 0.562, \"1\": 0.763}, {\"0\": 0.146, \"1\": 0.364}, {\"0\": 0.815, \"1\": 0.63}, {\"0\": 0.786, \"1\": 0.513}, {\"0\": 0.81, \"1\": 0.848}, {\"0\": 0.781, \"1\": 0.676}, {\"0\": 0.693, \"1\": 0.497}, {\"0\": 0.921, \"1\": 0.907}, {\"0\": 0.487, \"1\": 0.89}, {\"0\": 0.997, \"1\": 0.639}, {\"0\": 0.431, \"1\": 0.52}, {\"0\": 0.173, \"1\": 0.858}, {\"0\": 0.652, \"1\": 0.803}, {\"0\": 0.844, \"1\": 0.801}, {\"0\": 0.794, \"1\": 0.839}, {\"0\": 0.118, \"1\": 0.527}, {\"0\": 0.666, \"1\": 0.564}, {\"0\": 0.816, \"1\": 0.942}, {\"0\": 0.898, \"1\": 0.836}, {\"0\": 0.411, \"1\": 0.946}, {\"0\": 0.806, \"1\": 0.468}, {\"0\": 0.687, \"1\": 0.789}, {\"0\": 0.534, \"1\": 0.902}, {\"0\": 0.944, \"1\": 0.423}, {\"0\": 0.587, \"1\": 0.454}, {\"0\": 0.691, \"1\": 0.944}, {\"0\": 0.566, \"1\": 0.645}, {\"0\": 0.705, \"1\": 0.694}, {\"0\": 0.678, \"1\": 0.684}, {\"0\": 0.721, \"1\": 0.808}, {\"0\": 0.714, \"1\": 0.635}, {\"0\": 0.869, \"1\": 0.416}, {\"0\": 0.608, \"1\": 0.688}, {\"0\": 0.51, \"1\": 0.489}, {\"0\": 0.769, \"1\": 0.292}, {\"0\": 0.612, \"1\": 0.912}, {\"0\": 0.732, \"1\": 0.462}, {\"0\": 0.835, \"1\": 0.748}, {\"0\": 0.892, \"1\": 0.802}, {\"0\": 0.658, \"1\": 0.577}, {\"0\": 0.739, \"1\": 0.412}, {\"0\": 0.16, \"1\": 0.481}, {\"0\": 0.873, \"1\": 0.548}, {\"0\": 0.301, \"1\": 0.732}, {\"0\": 0.502, \"1\": 0.989}, {\"0\": 0.971, \"1\": 0.824}, {\"0\": 0.931, \"1\": 0.539}, {\"0\": 0.0814, \"1\": 0.792}, {\"0\": 0.604, \"1\": 0.525}, {\"0\": 0.911, \"1\": 0.459}, {\"0\": 0.679, \"1\": 0.586}, {\"0\": 0.53, \"1\": 0.783}, {\"0\": 0.886, \"1\": 0.387}, {\"0\": 0.312, \"1\": 0.668}, {\"0\": 0.901, \"1\": 0.624}, {\"0\": 0.582, \"1\": 0.857}, {\"0\": 0.706, \"1\": 0.821}, {\"0\": 0.985, \"1\": 0.409}, {\"0\": 0.177, \"1\": 0.636}, {\"0\": 0.795, \"1\": 0.818}, {\"0\": 0.657, \"1\": 0.659}, {\"0\": 0.841, \"1\": 0.712}, {\"0\": 0.593, \"1\": 0.733}, {\"0\": 0.686, \"1\": 0.598}, {\"0\": 0.545, \"1\": 0.768}, {\"0\": 0.242, \"1\": 0.549}, {\"0\": 0.886, \"1\": 0.77}, {\"0\": 0.348, \"1\": 0.58}, {\"0\": 0.0437, \"1\": 0.797}, {\"0\": 0.0802, \"1\": 0.494}, {\"0\": 0.962, \"1\": 0.637}, {\"0\": 0.0161, \"1\": 0.882}, {\"0\": 0.99, \"1\": 0.513}, {\"0\": 0.985, \"1\": 0.687}, {\"0\": 0.921, \"1\": 0.664}, {\"0\": 0.815, \"1\": 0.662}, {\"0\": 0.957, \"1\": 0.677}, {\"0\": 0.991, \"1\": 0.531}, {\"0\": 0.896, \"1\": 0.84}, {\"0\": 0.666, \"1\": 0.645}, {\"0\": 0.347, \"1\": 0.571}, {\"0\": 0.981, \"1\": 0.325}, {\"0\": 0.713, \"1\": 0.506}, {\"0\": 0.626, \"1\": 0.764}, {\"0\": 0.588, \"1\": 0.359}, {\"0\": 0.277, \"1\": 0.887}, {\"0\": 0.569, \"1\": 0.508}, {\"0\": 0.536, \"1\": 0.675}, {\"0\": 0.819, \"1\": 0.333}, {\"0\": 0.776, \"1\": 0.805}, {\"0\": 0.0748, \"1\": 0.248}, {\"0\": 0.99, \"1\": 0.621}, {\"0\": 0.853, \"1\": 0.798}, {\"0\": 0.745, \"1\": 0.616}, {\"0\": 0.784, \"1\": 0.587}, {\"0\": 0.675, \"1\": 0.835}, {\"0\": 0.75, \"1\": 0.761}, {\"0\": 0.0628, \"1\": 0.325}, {\"0\": 0.725, \"1\": 0.703}, {\"0\": 0.0935, \"1\": 0.633}, {\"0\": 0.863, \"1\": 0.732}, {\"0\": 0.864, \"1\": 0.781}, {\"0\": 0.879, \"1\": 0.444}, {\"0\": 0.862, \"1\": 0.483}, {\"0\": 0.972, \"1\": 0.592}, {\"0\": 0.0588, \"1\": 0.679}, {\"0\": 0.0753, \"1\": 0.513}, {\"0\": 0.933, \"1\": 0.544}, {\"0\": 0.851, \"1\": 0.837}, {\"0\": 0.642, \"1\": 0.794}, {\"0\": 0.638, \"1\": 0.571}, {\"0\": 0.525, \"1\": 0.868}, {\"0\": 0.9, \"1\": 0.792}, {\"0\": 0.522, \"1\": 0.578}, {\"0\": 0.655, \"1\": 0.495}, {\"0\": 0.392, \"1\": 0.712}, {\"0\": 0.706, \"1\": 0.374}, {\"0\": 0.873, \"1\": 0.716}, {\"0\": 0.677, \"1\": 0.507}, {\"0\": 0.915, \"1\": 0.774}, {\"0\": 0.479, \"1\": 0.482}, {\"0\": 0.729, \"1\": 0.528}, {\"0\": 0.804, \"1\": 0.912}, {\"0\": 0.723, \"1\": 0.703}, {\"0\": 0.831, \"1\": 0.427}, {\"0\": 0.423, \"1\": 0.77}, {\"0\": 0.928, \"1\": 0.583}, {\"0\": 0.942, \"1\": 0.869}, {\"0\": 0.412, \"1\": 0.308}, {\"0\": 0.521, \"1\": 0.583}, {\"0\": 0.0709, \"1\": 0.373}, {\"0\": 0.802, \"1\": 0.465}, {\"0\": 0.576, \"1\": 0.782}, {\"0\": 0.772, \"1\": 0.734}, {\"0\": 0.0978, \"1\": 0.752}, {\"0\": 0.826, \"1\": 0.588}, {\"0\": 0.727, \"1\": 0.404}, {\"0\": 0.926, \"1\": 0.491}, {\"0\": 0.743, \"1\": 0.77}, {\"0\": 0.606, \"1\": 0.159}, {\"0\": 0.56, \"1\": 0.946}, {\"0\": 0.621, \"1\": 0.609}, {\"0\": 0.847, \"1\": 0.902}, {\"0\": 0.864, \"1\": 0.433}, {\"0\": 0.637, \"1\": 0.469}, {\"0\": 0.875, \"1\": 0.698}, {\"0\": 0.808, \"1\": 0.952}, {\"0\": 0.608, \"1\": 0.433}, {\"0\": 0.131, \"1\": 0.894}, {\"0\": 0.679, \"1\": 0.799}, {\"0\": 0.453, \"1\": 0.532}, {\"0\": 0.748, \"1\": 0.872}, {\"0\": 0.408, \"1\": 0.654}, {\"0\": 0.927, \"1\": 0.68}, {\"0\": 0.721, \"1\": 0.72}, {\"0\": 0.979, \"1\": 0.905}, {\"0\": 0.824, \"1\": 0.89}, {\"0\": 0.743, \"1\": 0.528}, {\"0\": 0.568, \"1\": 0.929}, {\"0\": 0.409, \"1\": 0.73}, {\"0\": 0.0156, \"1\": 0.572}, {\"0\": 0.783, \"1\": 0.702}, {\"0\": 0.542, \"1\": 0.825}, {\"0\": 0.524, \"1\": 0.783}, {\"0\": 0.733, \"1\": 0.514}, {\"0\": 0.946, \"1\": 0.66}, {\"0\": 0.883, \"1\": 0.949}, {\"0\": 0.634, \"1\": 0.813}, {\"0\": 0.216, \"1\": 0.459}, {\"0\": 0.951, \"1\": 0.593}, {\"0\": 0.948, \"1\": 0.74}, {\"0\": 0.894, \"1\": 0.883}, {\"0\": 0.544, \"1\": 0.961}, {\"0\": 0.0867, \"1\": 0.96}, {\"0\": 0.627, \"1\": 0.922}, {\"0\": 0.163, \"1\": 0.602}, {\"0\": 0.32, \"1\": 0.628}, {\"0\": 0.867, \"1\": 0.814}, {\"0\": 0.989, \"1\": 0.743}, {\"0\": 0.595, \"1\": 0.853}, {\"0\": 0.913, \"1\": 0.8}, {\"0\": 0.52, \"1\": 0.503}, {\"0\": 0.804, \"1\": 0.773}, {\"0\": 0.862, \"1\": 0.915}, {\"0\": 0.866, \"1\": 0.77}, {\"0\": 0.874, \"1\": 0.528}, {\"0\": 0.955, \"1\": 0.796}, {\"0\": 0.877, \"1\": 0.638}, {\"0\": 0.684, \"1\": 0.718}, {\"0\": 0.575, \"1\": 0.515}, {\"0\": 0.92, \"1\": 0.465}, {\"0\": 0.952, \"1\": 0.609}, {\"0\": 0.994, \"1\": 0.655}, {\"0\": 0.023, \"1\": 0.852}, {\"0\": 0.429, \"1\": 0.773}, {\"0\": 0.866, \"1\": 0.603}, {\"0\": 0.809, \"1\": 0.416}, {\"0\": 0.688, \"1\": 0.685}, {\"0\": 0.604, \"1\": 0.88}, {\"0\": 0.719, \"1\": 0.374}, {\"0\": 0.507, \"1\": 0.832}, {\"0\": 0.609, \"1\": 0.595}, {\"0\": 0.846, \"1\": 0.935}, {\"0\": 0.144, \"1\": 0.727}, {\"0\": 0.903, \"1\": 0.897}, {\"0\": 0.916, \"1\": 0.474}, {\"0\": 0.919, \"1\": 0.672}, {\"0\": 0.896, \"1\": 0.658}, {\"0\": 0.803, \"1\": 0.924}, {\"0\": 0.473, \"1\": 0.895}, {\"0\": 0.785, \"1\": 0.427}, {\"0\": 0.802, \"1\": 0.54}, {\"0\": 0.881, \"1\": 0.907}, {\"0\": 0.902, \"1\": 0.615}, {\"0\": 0.248, \"1\": 0.607}, {\"0\": 0.849, \"1\": 0.415}, {\"0\": 0.97, \"1\": 0.665}, {\"0\": 0.889, \"1\": 0.537}, {\"0\": 0.794, \"1\": 0.324}, {\"0\": 0.705, \"1\": 0.59}, {\"0\": 0.986, \"1\": 0.329}, {\"0\": 0.511, \"1\": 0.894}, {\"0\": 0.535, \"1\": 0.894}, {\"0\": 0.745, \"1\": 0.776}, {\"0\": 0.978, \"1\": 0.031}, {\"0\": 0.271, \"1\": 0.986}, {\"0\": 0.332, \"1\": 0.854}, {\"0\": 0.863, \"1\": 0.793}, {\"0\": 0.747, \"1\": 0.768}, {\"0\": 0.923, \"1\": 0.739}, {\"0\": 0.721, \"1\": 0.827}, {\"0\": 0.925, \"1\": 0.658}, {\"0\": 0.638, \"1\": 0.532}, {\"0\": 0.96, \"1\": 0.687}, {\"0\": 0.663, \"1\": 0.549}, {\"0\": 0.0678, \"1\": 0.722}, {\"0\": 0.664, \"1\": 0.812}, {\"0\": 0.9, \"1\": 0.603}, {\"0\": 0.692, \"1\": 0.77}, {\"0\": 0.604, \"1\": 0.529}, {\"0\": 0.786, \"1\": 0.722}, {\"0\": 0.811, \"1\": 0.466}, {\"0\": 0.868, \"1\": 0.68}, {\"0\": 0.714, \"1\": 0.265}, {\"0\": 0.527, \"1\": 0.767}, {\"0\": 0.72, \"1\": 0.699}, {\"0\": 0.794, \"1\": 0.909}, {\"0\": 0.815, \"1\": 0.714}, {\"0\": 0.87, \"1\": 0.376}, {\"0\": 0.71, \"1\": 0.778}, {\"0\": 0.284, \"1\": 0.766}, {\"0\": 0.646, \"1\": 0.86}, {\"0\": 0.809, \"1\": 0.575}, {\"0\": 0.472, \"1\": 0.792}, {\"0\": 0.86, \"1\": 0.772}, {\"0\": 0.563, \"1\": 0.95}, {\"0\": 0.941, \"1\": 0.7}, {\"0\": 0.926, \"1\": 0.254}, {\"0\": 0.642, \"1\": 0.491}, {\"0\": 0.387, \"1\": 0.617}, {\"0\": 0.136, \"1\": 0.944}, {\"0\": 0.737, \"1\": 0.534}, {\"0\": 0.94, \"1\": 0.641}, {\"0\": 0.503, \"1\": 0.865}, {\"0\": 0.889, \"1\": 0.582}, {\"0\": 0.89, \"1\": 0.253}, {\"0\": 0.817, \"1\": 0.649}, {\"0\": 0.684, \"1\": 0.74}, {\"0\": 0.793, \"1\": 0.685}, {\"0\": 0.425, \"1\": 0.831}, {\"0\": 0.931, \"1\": 0.733}, {\"0\": 0.825, \"1\": 0.87}, {\"0\": 0.897, \"1\": 0.913}, {\"0\": 0.56, \"1\": 0.501}, {\"0\": 0.575, \"1\": 0.708}, {\"0\": 0.771, \"1\": 0.916}, {\"0\": 0.693, \"1\": 0.658}, {\"0\": 0.886, \"1\": 0.718}, {\"0\": 0.809, \"1\": 0.433}, {\"0\": 0.801, \"1\": 0.793}, {\"0\": 0.892, \"1\": 0.669}, {\"0\": 0.383, \"1\": 0.643}, {\"0\": 0.261, \"1\": 0.717}, {\"0\": 0.548, \"1\": 0.856}, {\"0\": 0.331, \"1\": 0.492}, {\"0\": 0.841, \"1\": 0.895}, {\"0\": 0.649, \"1\": 0.564}, {\"0\": 0.82, \"1\": 0.797}, {\"0\": 0.872, \"1\": 0.978}, {\"0\": 0.555, \"1\": 0.547}, {\"0\": 0.961, \"1\": 0.971}, {\"0\": 0.708, \"1\": 0.621}, {\"0\": 0.958, \"1\": 0.28}, {\"0\": 0.0619, \"1\": 0.972}, {\"0\": 0.413, \"1\": 0.526}, {\"0\": 0.788, \"1\": 0.58}, {\"0\": 0.975, \"1\": 0.848}, {\"0\": 0.784, \"1\": 0.547}, {\"0\": 0.659, \"1\": 0.407}, {\"0\": 0.526, \"1\": 0.85}, {\"0\": 0.784, \"1\": 0.412}, {\"0\": 0.771, \"1\": 0.688}, {\"0\": 0.877, \"1\": 0.684}, {\"0\": 0.91, \"1\": 0.586}, {\"0\": 0.742, \"1\": 0.634}, {\"0\": 0.772, \"1\": 0.572}, {\"0\": 0.564, \"1\": 0.597}, {\"0\": 0.83, \"1\": 0.954}, {\"0\": 0.975, \"1\": 0.623}, {\"0\": 0.514, \"1\": 0.681}, {\"0\": 0.479, \"1\": 0.568}, {\"0\": 0.156, \"1\": 0.779}, {\"0\": 0.845, \"1\": 0.801}, {\"0\": 0.518, \"1\": 0.734}, {\"0\": 0.815, \"1\": 0.504}, {\"0\": 0.255, \"1\": 0.733}, {\"0\": 0.962, \"1\": 0.806}, {\"0\": 0.793, \"1\": 0.921}, {\"0\": 0.676, \"1\": 0.9}, {\"0\": 0.86, \"1\": 0.883}, {\"0\": 0.802, \"1\": 0.891}, {\"0\": 0.628, \"1\": 0.8}, {\"0\": 0.673, \"1\": 0.959}, {\"0\": 0.645, \"1\": 0.688}, {\"0\": 0.577, \"1\": 0.41}, {\"0\": 0.957, \"1\": 0.81}, {\"0\": 0.426, \"1\": 0.806}, {\"0\": 0.842, \"1\": 0.733}, {\"0\": 0.762, \"1\": 0.331}, {\"0\": 0.927, \"1\": 0.81}, {\"0\": 0.716, \"1\": 0.641}, {\"0\": 0.15, \"1\": 0.62}, {\"0\": 0.861, \"1\": 0.91}, {\"0\": 0.815, \"1\": 0.811}, {\"0\": 0.0291, \"1\": 0.412}, {\"0\": 0.81, \"1\": 0.907}, {\"0\": 0.664, \"1\": 0.855}, {\"0\": 0.29, \"1\": 0.66}, {\"0\": 0.866, \"1\": 0.443}, {\"0\": 0.747, \"1\": 0.94}, {\"0\": 0.963, \"1\": 0.434}, {\"0\": 0.802, \"1\": 0.582}, {\"0\": 0.955, \"1\": 0.596}, {\"0\": 0.837, \"1\": 0.591}, {\"0\": 0.954, \"1\": 0.857}, {\"0\": 0.827, \"1\": 0.968}, {\"0\": 0.396, \"1\": 0.764}, {\"0\": 0.54, \"1\": 0.635}, {\"0\": 0.412, \"1\": 0.762}, {\"0\": 0.904, \"1\": 0.872}, {\"0\": 0.694, \"1\": 0.586}, {\"0\": 0.661, \"1\": 0.643}, {\"0\": 0.727, \"1\": 0.548}, {\"0\": 0.7, \"1\": 0.83}, {\"0\": 0.858, \"1\": 0.705}, {\"0\": 0.91, \"1\": 0.51}, {\"0\": 0.821, \"1\": 0.801}, {\"0\": 0.93, \"1\": 0.858}, {\"0\": 0.593, \"1\": 0.555}, {\"0\": 0.772, \"1\": 0.704}, {\"0\": 0.813, \"1\": 0.853}, {\"0\": 0.639, \"1\": 0.541}, {\"0\": 0.873, \"1\": 0.914}, {\"0\": 0.936, \"1\": 0.836}, {\"0\": 0.434, \"1\": 0.671}, {\"0\": 0.989, \"1\": 0.888}, {\"0\": 0.518, \"1\": 0.48}, {\"0\": 0.872, \"1\": 0.728}, {\"0\": 0.648, \"1\": 0.455}, {\"0\": 0.0575, \"1\": 0.877}, {\"0\": 0.439, \"1\": 0.978}, {\"0\": 0.427, \"1\": 0.79}, {\"0\": 0.651, \"1\": 0.924}, {\"0\": 0.809, \"1\": 0.854}, {\"0\": 0.745, \"1\": 0.682}, {\"0\": 0.923, \"1\": 0.73}, {\"0\": 0.974, \"1\": 0.483}, {\"0\": 0.249, \"1\": 0.709}, {\"0\": 0.953, \"1\": 0.824}, {\"0\": 0.701, \"1\": 0.742}, {\"0\": 0.427, \"1\": 0.586}, {\"0\": 0.645, \"1\": 0.708}, {\"0\": 0.772, \"1\": 0.632}, {\"0\": 0.607, \"1\": 0.713}, {\"0\": 0.868, \"1\": 0.606}, {\"0\": 0.369, \"1\": 0.543}, {\"0\": 0.935, \"1\": 0.82}, {\"0\": 0.94, \"1\": 0.897}, {\"0\": 0.832, \"1\": 0.616}, {\"0\": 0.658, \"1\": 0.798}, {\"0\": 0.738, \"1\": 0.918}, {\"0\": 0.449, \"1\": 0.843}, {\"0\": 0.579, \"1\": 0.654}, {\"0\": 0.69, \"1\": 0.698}, {\"0\": 0.885, \"1\": 0.669}, {\"0\": 0.723, \"1\": 0.765}, {\"0\": 0.883, \"1\": 0.547}, {\"0\": 0.802, \"1\": 0.578}, {\"0\": 0.882, \"1\": 0.519}, {\"0\": 0.67, \"1\": 0.729}, {\"0\": 0.697, \"1\": 0.874}, {\"0\": 0.532, \"1\": 0.823}, {\"0\": 0.592, \"1\": 0.605}, {\"0\": 0.687, \"1\": 0.943}, {\"0\": 0.657, \"1\": 0.56}, {\"0\": 0.978, \"1\": 0.879}, {\"0\": 0.595, \"1\": 0.938}, {\"0\": 0.513, \"1\": 0.362}, {\"0\": 0.731, \"1\": 0.972}, {\"0\": 0.517, \"1\": 0.788}, {\"0\": 0.822, \"1\": 0.767}, {\"0\": 0.87, \"1\": 0.749}, {\"0\": 0.403, \"1\": 0.876}, {\"0\": 0.645, \"1\": 0.413}, {\"0\": 0.771, \"1\": 0.75}, {\"0\": 0.457, \"1\": 0.641}, {\"0\": 0.772, \"1\": 0.822}, {\"0\": 0.725, \"1\": 0.733}, {\"0\": 0.489, \"1\": 0.787}, {\"0\": 0.32, \"1\": 0.72}, {\"0\": 0.896, \"1\": 0.492}, {\"0\": 0.336, \"1\": 0.823}, {\"0\": 0.371, \"1\": 0.513}, {\"0\": 0.614, \"1\": 0.923}, {\"0\": 0.862, \"1\": 0.661}, {\"0\": 0.771, \"1\": 0.891}, {\"0\": 0.552, \"1\": 0.603}, {\"0\": 0.747, \"1\": 0.607}, {\"0\": 0.564, \"1\": 0.706}, {\"0\": 0.915, \"1\": 0.884}, {\"0\": 0.256, \"1\": 0.902}, {\"0\": 0.533, \"1\": 0.857}, {\"0\": 0.624, \"1\": 0.848}, {\"0\": 0.725, \"1\": 0.596}, {\"0\": 0.858, \"1\": 0.501}, {\"0\": 0.288, \"1\": 0.616}, {\"0\": 0.503, \"1\": 0.837}, {\"0\": 0.159, \"1\": 0.793}, {\"0\": 0.906, \"1\": 0.74}, {\"0\": 0.237, \"1\": 0.755}, {\"0\": 0.841, \"1\": 0.782}, {\"0\": 0.76, \"1\": 0.427}, {\"0\": 0.659, \"1\": 0.661}, {\"0\": 0.946, \"1\": 0.677}, {\"0\": 0.841, \"1\": 0.863}, {\"0\": 0.752, \"1\": 0.883}, {\"0\": 0.629, \"1\": 0.774}, {\"0\": 0.771, \"1\": 0.799}, {\"0\": 0.889, \"1\": 0.781}, {\"0\": 0.801, \"1\": 0.79}, {\"0\": 0.905, \"1\": 0.853}, {\"0\": 0.106, \"1\": 0.838}, {\"0\": 0.609, \"1\": 0.748}, {\"0\": 0.508, \"1\": 0.213}, {\"0\": 0.716, \"1\": 0.486}, {\"0\": 0.783, \"1\": 0.67}, {\"0\": 0.838, \"1\": 0.9}, {\"0\": 0.696, \"1\": 0.6}, {\"0\": 0.871, \"1\": 0.669}, {\"0\": 0.78, \"1\": 0.448}, {\"0\": 0.142, \"1\": 0.664}, {\"0\": 0.521, \"1\": 0.814}, {\"0\": 0.921, \"1\": 0.864}, {\"0\": 0.223, \"1\": 0.453}, {\"0\": 0.964, \"1\": 0.937}, {\"0\": 0.0354, \"1\": 0.719}, {\"0\": 0.445, \"1\": 0.772}, {\"0\": 0.659, \"1\": 0.605}, {\"0\": 0.614, \"1\": 0.789}, {\"0\": 0.505, \"1\": 0.661}, {\"0\": 0.805, \"1\": 0.611}, {\"0\": 0.778, \"1\": 0.798}, {\"0\": 0.812, \"1\": 0.675}, {\"0\": 0.709, \"1\": 0.877}, {\"0\": 0.885, \"1\": 0.963}, {\"0\": 0.587, \"1\": 0.756}, {\"0\": 0.789, \"1\": 0.642}, {\"0\": 0.802, \"1\": 0.532}, {\"0\": 0.783, \"1\": 0.848}, {\"0\": 0.879, \"1\": 0.985}, {\"0\": 0.8, \"1\": 0.265}, {\"0\": 0.992, \"1\": 0.873}, {\"0\": 0.615, \"1\": 0.772}, {\"0\": 0.236, \"1\": 0.718}, {\"0\": 0.896, \"1\": 0.518}, {\"0\": 0.661, \"1\": 0.638}, {\"0\": 0.643, \"1\": 0.74}, {\"0\": 0.775, \"1\": 0.837}, {\"0\": 0.87, \"1\": 0.964}, {\"0\": 0.797, \"1\": 0.58}, {\"0\": null, \"1\": 0.937}, {\"0\": null, \"1\": 0.811}, {\"0\": null, \"1\": 0.662}, {\"0\": null, \"1\": 0.821}, {\"0\": null, \"1\": 0.905}, {\"0\": null, \"1\": 0.559}, {\"0\": null, \"1\": 0.804}, {\"0\": null, \"1\": 0.857}, {\"0\": null, \"1\": 0.492}, {\"0\": null, \"1\": 0.981}, {\"0\": null, \"1\": 0.848}, {\"0\": null, \"1\": 0.488}, {\"0\": null, \"1\": 0.747}, {\"0\": null, \"1\": 0.724}, {\"0\": null, \"1\": 0.955}, {\"0\": null, \"1\": 0.921}, {\"0\": null, \"1\": 0.835}, {\"0\": null, \"1\": 0.953}, {\"0\": null, \"1\": 0.64}, {\"0\": null, \"1\": 0.793}, {\"0\": null, \"1\": 0.737}, {\"0\": null, \"1\": 0.578}, {\"0\": null, \"1\": 0.603}, {\"0\": null, \"1\": 0.496}, {\"0\": null, \"1\": 0.71}, {\"0\": null, \"1\": 0.945}, {\"0\": null, \"1\": 0.789}, {\"0\": null, \"1\": 0.944}, {\"0\": null, \"1\": 0.492}, {\"0\": null, \"1\": 0.769}, {\"0\": null, \"1\": 0.723}, {\"0\": null, \"1\": 0.527}, {\"0\": null, \"1\": 0.597}, {\"0\": null, \"1\": 0.605}, {\"0\": null, \"1\": 0.408}, {\"0\": null, \"1\": 0.629}, {\"0\": null, \"1\": 0.449}, {\"0\": null, \"1\": 0.628}, {\"0\": null, \"1\": 0.64}], \"data-54e1b0c37e6a24880ae6994cca0f699a\": [{\"0\": 0.298, \"1\": 0.552}, {\"0\": 0.245, \"1\": 0.808}, {\"0\": 0.541, \"1\": 0.622}, {\"0\": 0.299, \"1\": 0.598}, {\"0\": 0.41, \"1\": 0.647}, {\"0\": 0.0658, \"1\": 0.718}, {\"0\": 0.186, \"1\": 0.736}, {\"0\": 0.19, \"1\": 0.721}, {\"0\": 0.343, \"1\": 0.403}, {\"0\": 0.199, \"1\": 0.319}, {\"0\": 0.225, \"1\": 0.442}, {\"0\": 0.136, \"1\": 0.669}, {\"0\": 0.673, \"1\": 0.416}, {\"0\": 0.372, \"1\": 0.794}, {\"0\": 0.519, \"1\": 0.358}, {\"0\": 0.963, \"1\": 0.261}, {\"0\": 0.813, \"1\": 0.706}, {\"0\": 0.0605, \"1\": 0.346}, {\"0\": 0.492, \"1\": 0.277}, {\"0\": 0.913, \"1\": 0.894}, {\"0\": 0.766, \"1\": 0.381}, {\"0\": 0.153, \"1\": 0.973}, {\"0\": 0.305, \"1\": 0.547}, {\"0\": 0.587, \"1\": 0.96}, {\"0\": 0.232, \"1\": 0.199}, {\"0\": 0.646, \"1\": 0.72}, {\"0\": 0.521, \"1\": 0.412}, {\"0\": 0.776, \"1\": 0.761}, {\"0\": 0.679, \"1\": 0.255}, {\"0\": 0.555, \"1\": 0.604}, {\"0\": 0.478, \"1\": 0.341}, {\"0\": 0.245, \"1\": 0.131}, {\"0\": 0.466, \"1\": 0.0459}, {\"0\": 0.462, \"1\": 0.508}, {\"0\": 0.603, \"1\": 0.194}, {\"0\": 0.82, \"1\": 0.482}, {\"0\": 0.964, \"1\": 0.674}, {\"0\": 0.595, \"1\": 0.883}, {\"0\": 0.0623, \"1\": 0.708}, {\"0\": 0.334, \"1\": 0.438}, {\"0\": 0.181, \"1\": 0.492}, {\"0\": 0.743, \"1\": 0.334}, {\"0\": 0.96, \"1\": 0.592}, {\"0\": 0.758, \"1\": 0.646}, {\"0\": 0.221, \"1\": 0.921}, {\"0\": 0.321, \"1\": 0.471}, {\"0\": 0.313, \"1\": 0.819}, {\"0\": 0.731, \"1\": 0.921}, {\"0\": 0.419, \"1\": 0.72}, {\"0\": 0.162, \"1\": 0.624}, {\"0\": 0.929, \"1\": 0.658}, {\"0\": 0.171, \"1\": 0.738}, {\"0\": 0.334, \"1\": 0.197}, {\"0\": 0.331, \"1\": 0.551}, {\"0\": 0.746, \"1\": 0.091}, {\"0\": 0.307, \"1\": 0.739}, {\"0\": 0.974, \"1\": 0.682}, {\"0\": 0.565, \"1\": 0.16}, {\"0\": 0.68, \"1\": 0.824}, {\"0\": 0.744, \"1\": 0.452}, {\"0\": 0.529, \"1\": 0.825}, {\"0\": 0.155, \"1\": 0.94}, {\"0\": 0.0637, \"1\": 0.464}, {\"0\": 0.191, \"1\": 0.645}, {\"0\": 0.683, \"1\": 0.594}, {\"0\": 0.528, \"1\": 0.22}, {\"0\": 0.564, \"1\": 0.584}, {\"0\": 0.102, \"1\": 0.369}, {\"0\": 0.531, \"1\": 0.304}, {\"0\": 0.52, \"1\": 0.288}, {\"0\": 0.732, \"1\": 0.49}, {\"0\": 0.706, \"1\": 0.0576}, {\"0\": 0.731, \"1\": 0.189}, {\"0\": 0.675, \"1\": 0.634}, {\"0\": 0.716, \"1\": 0.54}, {\"0\": 0.364, \"1\": 0.668}, {\"0\": 0.268, \"1\": 0.444}, {\"0\": 0.0384, \"1\": 0.176}, {\"0\": 0.558, \"1\": 0.445}, {\"0\": 0.681, \"1\": 0.124}, {\"0\": 0.0746, \"1\": 0.925}, {\"0\": 0.564, \"1\": 0.786}, {\"0\": 0.381, \"1\": 0.294}, {\"0\": 0.887, \"1\": 0.159}, {\"0\": 0.4, \"1\": 0.771}, {\"0\": 0.15, \"1\": 0.254}, {\"0\": 0.236, \"1\": 0.791}, {\"0\": 0.809, \"1\": 0.961}, {\"0\": 0.643, \"1\": 0.603}, {\"0\": 0.762, \"1\": 0.69}, {\"0\": 0.0581, \"1\": 0.166}, {\"0\": 0.159, \"1\": 0.88}, {\"0\": 0.501, \"1\": 0.821}, {\"0\": 0.452, \"1\": 0.609}, {\"0\": 0.257, \"1\": 0.542}, {\"0\": 0.233, \"1\": 0.642}, {\"0\": 0.354, \"1\": 0.824}, {\"0\": 0.746, \"1\": 0.372}, {\"0\": 0.159, \"1\": 0.601}, {\"0\": 0.404, \"1\": 0.46}, {\"0\": 0.648, \"1\": 0.817}, {\"0\": 0.314, \"1\": 0.223}, {\"0\": 0.57, \"1\": 0.935}, {\"0\": 0.629, \"1\": 0.464}, {\"0\": 0.19, \"1\": 0.176}, {\"0\": 0.608, \"1\": 0.338}, {\"0\": 0.967, \"1\": 0.214}, {\"0\": 0.375, \"1\": 0.302}, {\"0\": 0.0703, \"1\": 0.499}, {\"0\": 0.22, \"1\": 0.796}, {\"0\": 0.851, \"1\": 0.931}, {\"0\": 0.454, \"1\": 0.761}, {\"0\": 0.455, \"1\": 0.478}, {\"0\": 0.231, \"1\": 0.927}, {\"0\": 0.813, \"1\": 0.68}, {\"0\": 0.292, \"1\": 0.218}, {\"0\": 0.14, \"1\": 0.616}, {\"0\": 0.595, \"1\": 0.463}, {\"0\": 0.84, \"1\": 0.191}, {\"0\": 0.266, \"1\": 0.444}, {\"0\": 0.0484, \"1\": 0.816}, {\"0\": 0.609, \"1\": 0.683}, {\"0\": 0.487, \"1\": 0.451}, {\"0\": 0.441, \"1\": 0.861}, {\"0\": 0.354, \"1\": 0.564}, {\"0\": 0.275, \"1\": 0.118}, {\"0\": 0.454, \"1\": 0.0451}, {\"0\": 0.114, \"1\": 0.852}, {\"0\": 0.509, \"1\": 0.508}, {\"0\": 0.356, \"1\": 0.295}, {\"0\": 0.157, \"1\": 0.0793}, {\"0\": 0.539, \"1\": 0.667}, {\"0\": 0.694, \"1\": 0.544}, {\"0\": 0.25, \"1\": 0.726}, {\"0\": 0.431, \"1\": 0.729}, {\"0\": 0.875, \"1\": 0.455}, {\"0\": 0.362, \"1\": 0.458}, {\"0\": 0.395, \"1\": 0.792}, {\"0\": 0.308, \"1\": 0.674}, {\"0\": 0.935, \"1\": 0.457}, {\"0\": 0.845, \"1\": 0.418}, {\"0\": 0.933, \"1\": 0.695}, {\"0\": 0.0378, \"1\": 0.12}, {\"0\": 0.263, \"1\": 0.574}, {\"0\": 0.413, \"1\": 0.675}, {\"0\": 0.516, \"1\": 0.076}, {\"0\": 0.205, \"1\": 0.204}, {\"0\": 0.234, \"1\": 0.561}, {\"0\": 0.307, \"1\": 0.768}, {\"0\": 0.51, \"1\": 0.489}, {\"0\": 0.338, \"1\": 0.0733}, {\"0\": 0.293, \"1\": 0.593}, {\"0\": 0.209, \"1\": 0.107}, {\"0\": 0.856, \"1\": 0.592}, {\"0\": 0.381, \"1\": 0.511}, {\"0\": 0.336, \"1\": 0.704}, {\"0\": 0.652, \"1\": 0.853}, {\"0\": 0.462, \"1\": 0.0795}, {\"0\": 0.376, \"1\": 0.93}, {\"0\": 0.459, \"1\": 0.584}, {\"0\": 0.637, \"1\": 0.614}, {\"0\": 0.483, \"1\": 0.77}, {\"0\": 0.198, \"1\": 0.276}, {\"0\": 0.371, \"1\": 0.372}, {\"0\": 0.129, \"1\": 0.55}, {\"0\": 0.68, \"1\": 0.285}, {\"0\": 0.247, \"1\": 0.787}, {\"0\": 0.493, \"1\": 0.697}, {\"0\": 0.458, \"1\": 0.13}, {\"0\": 0.372, \"1\": 0.973}, {\"0\": 0.919, \"1\": 0.593}, {\"0\": 0.235, \"1\": 0.606}, {\"0\": 0.446, \"1\": 0.626}, {\"0\": 0.332, \"1\": 0.069}, {\"0\": 0.111, \"1\": 0.722}, {\"0\": 0.102, \"1\": 0.562}, {\"0\": 0.602, \"1\": 0.558}, {\"0\": 0.431, \"1\": 0.502}, {\"0\": 0.918, \"1\": 0.956}, {\"0\": 0.157, \"1\": 0.891}, {\"0\": 0.312, \"1\": 0.59}, {\"0\": 0.275, \"1\": 0.457}, {\"0\": 0.378, \"1\": 0.805}, {\"0\": 0.506, \"1\": 0.763}, {\"0\": 0.473, \"1\": 0.039}, {\"0\": 0.961, \"1\": 0.367}, {\"0\": 0.23, \"1\": 0.428}, {\"0\": 0.598, \"1\": 0.136}, {\"0\": 0.451, \"1\": 0.366}, {\"0\": 0.49, \"1\": 0.582}, {\"0\": 0.114, \"1\": 0.429}, {\"0\": 0.261, \"1\": 0.357}, {\"0\": 0.16, \"1\": 0.421}, {\"0\": 0.779, \"1\": 0.398}, {\"0\": 0.289, \"1\": 0.711}, {\"0\": 0.769, \"1\": 0.199}, {\"0\": 0.645, \"1\": 0.104}, {\"0\": 0.407, \"1\": 0.411}, {\"0\": 0.352, \"1\": 0.245}, {\"0\": 0.323, \"1\": 0.607}, {\"0\": 0.293, \"1\": 0.698}, {\"0\": 0.689, \"1\": 0.535}, {\"0\": 0.837, \"1\": 0.904}, {\"0\": 0.255, \"1\": 0.747}, {\"0\": 0.183, \"1\": 0.49}, {\"0\": 0.13, \"1\": 0.174}, {\"0\": 0.818, \"1\": 0.228}, {\"0\": 0.49, \"1\": 0.645}, {\"0\": 0.16, \"1\": 0.855}, {\"0\": 0.321, \"1\": 0.949}, {\"0\": 0.31, \"1\": 0.745}, {\"0\": 0.47, \"1\": 0.587}, {\"0\": 0.422, \"1\": 0.863}, {\"0\": 0.875, \"1\": 0.381}, {\"0\": 0.965, \"1\": 0.416}, {\"0\": 0.489, \"1\": 0.63}, {\"0\": 0.78, \"1\": 0.211}, {\"0\": 0.211, \"1\": 0.896}, {\"0\": 0.53, \"1\": 0.368}, {\"0\": 0.42, \"1\": 0.681}, {\"0\": 0.479, \"1\": 0.785}, {\"0\": 0.339, \"1\": 0.323}, {\"0\": 0.497, \"1\": 0.752}, {\"0\": 0.19, \"1\": 0.2}, {\"0\": 0.785, \"1\": 0.313}, {\"0\": 0.108, \"1\": 0.972}, {\"0\": 0.497, \"1\": 0.56}, {\"0\": 0.327, \"1\": 0.853}, {\"0\": 0.322, \"1\": 0.564}, {\"0\": 0.402, \"1\": 0.173}, {\"0\": 0.493, \"1\": 0.357}, {\"0\": 0.172, \"1\": 0.822}, {\"0\": 0.561, \"1\": 0.635}, {\"0\": 0.453, \"1\": 0.111}, {\"0\": 0.343, \"1\": 0.44}, {\"0\": 0.932, \"1\": 0.196}, {\"0\": 0.289, \"1\": 0.854}, {\"0\": 0.947, \"1\": 0.573}, {\"0\": 0.615, \"1\": 0.255}, {\"0\": 0.718, \"1\": 0.744}, {\"0\": 0.558, \"1\": 0.482}, {\"0\": 0.198, \"1\": 0.512}, {\"0\": 0.797, \"1\": 0.901}, {\"0\": 0.328, \"1\": 0.298}, {\"0\": 0.656, \"1\": 0.383}, {\"0\": 0.5, \"1\": 0.306}, {\"0\": 0.729, \"1\": 0.672}, {\"0\": 0.558, \"1\": 0.955}, {\"0\": 0.586, \"1\": 0.726}, {\"0\": 0.718, \"1\": 0.23}, {\"0\": 0.11, \"1\": 0.93}, {\"0\": 0.717, \"1\": 0.414}, {\"0\": 0.441, \"1\": 0.66}, {\"0\": 0.128, \"1\": 0.469}, {\"0\": 0.692, \"1\": 0.712}, {\"0\": 0.392, \"1\": 0.689}, {\"0\": 0.125, \"1\": 0.523}, {\"0\": 0.81, \"1\": 0.264}, {\"0\": 0.516, \"1\": 0.463}, {\"0\": 0.57, \"1\": 0.566}, {\"0\": 0.284, \"1\": 0.74}, {\"0\": 0.261, \"1\": 0.614}, {\"0\": 0.361, \"1\": 0.488}, {\"0\": 0.471, \"1\": 0.185}, {\"0\": 0.293, \"1\": 0.836}, {\"0\": 0.311, \"1\": 0.671}, {\"0\": 0.535, \"1\": 0.48}, {\"0\": 0.416, \"1\": 0.398}, {\"0\": 0.205, \"1\": 0.795}, {\"0\": 0.561, \"1\": 0.915}, {\"0\": 0.501, \"1\": 0.299}, {\"0\": 0.703, \"1\": 0.369}, {\"0\": 0.717, \"1\": 0.884}, {\"0\": 0.196, \"1\": 0.517}, {\"0\": 0.201, \"1\": 0.561}, {\"0\": 0.146, \"1\": 0.318}, {\"0\": 0.382, \"1\": 0.176}, {\"0\": 0.702, \"1\": 0.57}, {\"0\": 0.171, \"1\": 0.904}, {\"0\": 0.443, \"1\": 0.182}, {\"0\": 0.441, \"1\": 0.496}, {\"0\": 0.242, \"1\": 0.919}, {\"0\": 0.596, \"1\": 0.104}, {\"0\": 0.0621, \"1\": 0.855}, {\"0\": 0.556, \"1\": 0.677}, {\"0\": 0.119, \"1\": 0.393}, {\"0\": 0.508, \"1\": 0.138}, {\"0\": 0.654, \"1\": 0.767}, {\"0\": 0.476, \"1\": 0.635}, {\"0\": 0.483, \"1\": 0.599}, {\"0\": 0.237, \"1\": 0.601}, {\"0\": 0.71, \"1\": 0.551}, {\"0\": 0.725, \"1\": 0.372}, {\"0\": 0.652, \"1\": 0.683}, {\"0\": 0.265, \"1\": 0.938}, {\"0\": 0.504, \"1\": 0.606}, {\"0\": 0.128, \"1\": 0.326}, {\"0\": 0.2, \"1\": 0.661}, {\"0\": 0.0728, \"1\": 0.766}, {\"0\": 0.144, \"1\": 0.559}, {\"0\": 0.182, \"1\": 0.0373}, {\"0\": 0.492, \"1\": 0.302}, {\"0\": 0.72, \"1\": 0.314}, {\"0\": 0.513, \"1\": 0.472}, {\"0\": 0.372, \"1\": 0.354}, {\"0\": 0.777, \"1\": 0.503}, {\"0\": 0.0533, \"1\": 0.197}, {\"0\": 0.546, \"1\": 0.717}, {\"0\": 0.625, \"1\": 0.477}, {\"0\": 0.167, \"1\": 0.79}, {\"0\": 0.825, \"1\": 0.491}, {\"0\": 0.471, \"1\": 0.813}, {\"0\": 0.0706, \"1\": 0.23}, {\"0\": 0.254, \"1\": 0.401}, {\"0\": 0.523, \"1\": 0.584}, {\"0\": 0.331, \"1\": 0.54}, {\"0\": 0.234, \"1\": 0.769}, {\"0\": 0.769, \"1\": 0.191}, {\"0\": 0.828, \"1\": 0.833}, {\"0\": 0.0473, \"1\": 0.52}, {\"0\": 0.728, \"1\": 0.649}, {\"0\": 0.85, \"1\": 0.39}, {\"0\": 0.63, \"1\": 0.294}, {\"0\": 0.513, \"1\": 0.371}, {\"0\": 0.309, \"1\": 0.411}, {\"0\": 0.738, \"1\": 0.84}, {\"0\": 0.463, \"1\": 0.711}, {\"0\": 0.847, \"1\": 0.752}, {\"0\": 0.56, \"1\": 0.218}, {\"0\": 0.579, \"1\": 0.18}, {\"0\": 0.125, \"1\": 0.424}, {\"0\": 0.75, \"1\": 0.111}, {\"0\": 0.173, \"1\": 0.496}, {\"0\": 0.628, \"1\": 0.289}, {\"0\": 0.879, \"1\": 0.239}, {\"0\": 0.867, \"1\": 0.824}, {\"0\": 0.254, \"1\": 0.385}, {\"0\": 0.403, \"1\": 0.505}, {\"0\": 0.199, \"1\": 0.304}, {\"0\": 0.571, \"1\": 0.72}, {\"0\": 0.932, \"1\": 0.529}, {\"0\": 0.676, \"1\": 0.78}, {\"0\": 0.442, \"1\": 0.731}, {\"0\": 0.516, \"1\": 0.0759}, {\"0\": 0.235, \"1\": 0.424}, {\"0\": 0.225, \"1\": 0.43}, {\"0\": 0.55, \"1\": 0.616}, {\"0\": 0.728, \"1\": 0.889}, {\"0\": 0.204, \"1\": 0.696}, {\"0\": 0.397, \"1\": 0.365}, {\"0\": 0.506, \"1\": 0.958}, {\"0\": 0.84, \"1\": 0.628}, {\"0\": 0.34, \"1\": 0.826}, {\"0\": 0.962, \"1\": 0.396}, {\"0\": 0.267, \"1\": 0.869}, {\"0\": 0.569, \"1\": 0.682}, {\"0\": 0.791, \"1\": 0.845}, {\"0\": 0.286, \"1\": 0.842}, {\"0\": 0.533, \"1\": 0.948}, {\"0\": 0.595, \"1\": 0.404}, {\"0\": 0.599, \"1\": 0.424}, {\"0\": 0.733, \"1\": 0.627}, {\"0\": 0.176, \"1\": 0.131}, {\"0\": 0.608, \"1\": 0.687}, {\"0\": 0.81, \"1\": 0.144}, {\"0\": 0.0906, \"1\": 0.474}, {\"0\": 0.0513, \"1\": 0.521}, {\"0\": 0.507, \"1\": 0.688}, {\"0\": 0.155, \"1\": 0.657}, {\"0\": 0.462, \"1\": 0.0373}, {\"0\": 0.16, \"1\": 0.867}, {\"0\": 0.426, \"1\": 0.231}, {\"0\": 0.826, \"1\": 0.452}, {\"0\": 0.154, \"1\": 0.726}, {\"0\": 0.194, \"1\": 0.657}, {\"0\": 0.353, \"1\": 0.745}, {\"0\": 0.474, \"1\": 0.138}, {\"0\": 0.241, \"1\": 0.467}, {\"0\": 0.129, \"1\": 0.271}, {\"0\": 0.368, \"1\": 0.354}, {\"0\": 0.871, \"1\": 0.315}, {\"0\": 0.384, \"1\": 0.588}, {\"0\": 0.286, \"1\": 0.464}, {\"0\": 0.751, \"1\": 0.958}, {\"0\": 0.184, \"1\": 0.618}, {\"0\": 0.45, \"1\": 0.964}, {\"0\": 0.43, \"1\": 0.654}, {\"0\": 0.134, \"1\": 0.281}, {\"0\": 0.169, \"1\": 0.578}, {\"0\": 0.916, \"1\": 0.72}, {\"0\": 0.585, \"1\": 0.373}, {\"0\": 0.382, \"1\": 0.757}, {\"0\": 0.604, \"1\": 0.568}, {\"0\": 0.38, \"1\": 0.244}, {\"0\": 0.289, \"1\": 0.46}, {\"0\": 0.52, \"1\": 0.675}, {\"0\": 0.289, \"1\": 0.398}, {\"0\": 0.335, \"1\": 0.136}, {\"0\": 0.735, \"1\": 0.684}, {\"0\": 0.327, \"1\": 0.361}, {\"0\": 0.472, \"1\": 0.221}, {\"0\": 0.566, \"1\": 0.361}, {\"0\": 0.229, \"1\": 0.214}, {\"0\": 0.0877, \"1\": 0.568}, {\"0\": 0.23, \"1\": 0.331}, {\"0\": 0.692, \"1\": 0.615}, {\"0\": 0.755, \"1\": 0.685}, {\"0\": 0.41, \"1\": 0.419}, {\"0\": 0.0963, \"1\": 0.587}, {\"0\": 0.882, \"1\": 0.696}, {\"0\": 0.494, \"1\": 0.566}, {\"0\": 0.557, \"1\": 0.797}, {\"0\": 0.296, \"1\": 0.947}, {\"0\": 0.0744, \"1\": 0.339}, {\"0\": 0.47, \"1\": 0.758}, {\"0\": 0.731, \"1\": 0.685}, {\"0\": 0.637, \"1\": 0.327}, {\"0\": 0.0622, \"1\": 0.847}, {\"0\": 0.313, \"1\": 0.161}, {\"0\": 0.868, \"1\": 0.612}, {\"0\": 0.117, \"1\": 0.618}, {\"0\": 0.489, \"1\": 0.961}, {\"0\": 0.45, \"1\": 0.668}, {\"0\": 0.57, \"1\": 0.48}, {\"0\": 0.186, \"1\": 0.656}, {\"0\": 0.308, \"1\": 0.0933}, {\"0\": 0.471, \"1\": 0.557}, {\"0\": 0.066, \"1\": 0.356}, {\"0\": 0.52, \"1\": 0.462}, {\"0\": 0.462, \"1\": 0.529}, {\"0\": 0.714, \"1\": 0.0537}, {\"0\": 0.172, \"1\": 0.576}, {\"0\": 0.701, \"1\": 0.683}, {\"0\": 0.423, \"1\": 0.134}, {\"0\": 0.493, \"1\": 0.475}, {\"0\": 0.574, \"1\": 0.324}, {\"0\": 0.637, \"1\": 0.329}, {\"0\": 0.853, \"1\": 0.389}, {\"0\": 0.647, \"1\": 0.659}, {\"0\": 0.81, \"1\": 0.269}, {\"0\": 0.876, \"1\": 0.217}, {\"0\": 0.331, \"1\": 0.321}, {\"0\": 0.929, \"1\": 0.713}, {\"0\": 0.448, \"1\": 0.628}, {\"0\": 0.825, \"1\": 0.217}, {\"0\": 0.0999, \"1\": 0.57}, {\"0\": 0.794, \"1\": 0.0397}, {\"0\": 0.142, \"1\": 0.499}, {\"0\": 0.809, \"1\": 0.524}, {\"0\": 0.655, \"1\": 0.624}, {\"0\": 0.562, \"1\": 0.288}, {\"0\": 0.673, \"1\": 0.222}, {\"0\": 0.156, \"1\": 0.742}, {\"0\": 0.767, \"1\": 0.308}, {\"0\": 0.692, \"1\": 0.895}, {\"0\": 0.731, \"1\": 0.229}, {\"0\": 0.181, \"1\": 0.302}, {\"0\": 0.0855, \"1\": 0.332}, {\"0\": 0.687, \"1\": 0.471}, {\"0\": 0.246, \"1\": 0.959}, {\"0\": 0.225, \"1\": 0.611}, {\"0\": 0.507, \"1\": 0.719}, {\"0\": 0.865, \"1\": 0.238}, {\"0\": 0.893, \"1\": 0.778}, {\"0\": 0.445, \"1\": 0.524}, {\"0\": 0.19, \"1\": 0.345}, {\"0\": 0.617, \"1\": 0.325}, {\"0\": 0.627, \"1\": 0.0576}, {\"0\": 0.709, \"1\": 0.516}, {\"0\": 0.381, \"1\": 0.355}, {\"0\": 0.0718, \"1\": 0.539}, {\"0\": 0.513, \"1\": 0.364}, {\"0\": 0.19, \"1\": 0.899}, {\"0\": 0.331, \"1\": 0.373}, {\"0\": 0.518, \"1\": 0.403}, {\"0\": 0.615, \"1\": 0.678}, {\"0\": 0.209, \"1\": 0.426}, {\"0\": 0.208, \"1\": 0.0951}, {\"0\": 0.395, \"1\": 0.325}, {\"0\": 0.437, \"1\": 0.347}, {\"0\": 0.435, \"1\": 0.539}, {\"0\": 0.11, \"1\": 0.14}, {\"0\": 0.532, \"1\": 0.301}, {\"0\": 0.102, \"1\": 0.806}, {\"0\": 0.763, \"1\": 0.433}, {\"0\": 0.222, \"1\": 0.343}, {\"0\": 0.372, \"1\": 0.621}, {\"0\": 0.729, \"1\": 0.338}, {\"0\": 0.641, \"1\": 0.522}, {\"0\": 0.0703, \"1\": 0.436}, {\"0\": 0.0588, \"1\": 0.517}, {\"0\": 0.874, \"1\": 0.484}, {\"0\": 0.496, \"1\": 0.094}, {\"0\": 0.464, \"1\": 0.216}, {\"0\": 0.561, \"1\": 0.695}, {\"0\": 0.486, \"1\": 0.645}, {\"0\": 0.589, \"1\": 0.573}, {\"0\": 0.566, \"1\": 0.317}, {\"0\": 0.47, \"1\": 0.0681}, {\"0\": 0.596, \"1\": 0.964}, {\"0\": 0.154, \"1\": 0.527}, {\"0\": 0.818, \"1\": 0.294}, {\"0\": 0.903, \"1\": 0.823}, {\"0\": 0.149, \"1\": 0.813}, {\"0\": 0.727, \"1\": 0.36}, {\"0\": 0.254, \"1\": 0.897}, {\"0\": 0.36, \"1\": 0.678}, {\"0\": 0.352, \"1\": 0.189}, {\"0\": 0.334, \"1\": 0.113}, {\"0\": 0.428, \"1\": 0.397}, {\"0\": 0.853, \"1\": 0.425}, {\"0\": 0.153, \"1\": 0.259}, {\"0\": 0.353, \"1\": 0.662}, {\"0\": 0.0399, \"1\": 0.357}, {\"0\": 0.531, \"1\": 0.43}, {\"0\": 0.577, \"1\": 0.226}, {\"0\": 0.292, \"1\": 0.346}, {\"0\": 0.15, \"1\": 0.297}, {\"0\": 0.972, \"1\": 0.893}, {\"0\": 0.298, \"1\": 0.408}, {\"0\": 0.634, \"1\": 0.689}, {\"0\": 0.102, \"1\": 0.0638}, {\"0\": 0.396, \"1\": 0.64}, {\"0\": 0.255, \"1\": 0.583}, {\"0\": 0.316, \"1\": 0.393}, {\"0\": 0.185, \"1\": 0.207}, {\"0\": 0.96, \"1\": 0.17}, {\"0\": 0.814, \"1\": 0.709}, {\"0\": 0.142, \"1\": 0.867}, {\"0\": 0.404, \"1\": 0.354}, {\"0\": 0.949, \"1\": 0.458}, {\"0\": 0.237, \"1\": 0.619}, {\"0\": 0.257, \"1\": 0.496}, {\"0\": 0.545, \"1\": 0.861}, {\"0\": 0.884, \"1\": 0.595}, {\"0\": 0.758, \"1\": 0.186}, {\"0\": 0.28, \"1\": 0.483}, {\"0\": 0.849, \"1\": 0.508}, {\"0\": 0.601, \"1\": 0.0978}, {\"0\": 0.608, \"1\": 0.601}, {\"0\": 0.589, \"1\": 0.282}, {\"0\": 0.662, \"1\": 0.564}, {\"0\": 0.393, \"1\": 0.531}, {\"0\": 0.919, \"1\": 0.844}, {\"0\": 0.245, \"1\": 0.375}, {\"0\": 0.308, \"1\": 0.107}, {\"0\": 0.531, \"1\": 0.797}, {\"0\": 0.413, \"1\": 0.675}, {\"0\": 0.401, \"1\": 0.782}, {\"0\": 0.271, \"1\": 0.284}, {\"0\": 0.466, \"1\": 0.318}, {\"0\": 0.441, \"1\": 0.436}, {\"0\": 0.462, \"1\": 0.456}, {\"0\": 0.331, \"1\": 0.887}, {\"0\": 0.949, \"1\": 0.0778}, {\"0\": 0.139, \"1\": 0.331}, {\"0\": 0.47, \"1\": 0.13}, {\"0\": 0.227, \"1\": 0.398}, {\"0\": 0.188, \"1\": 0.963}, {\"0\": 0.643, \"1\": 0.668}, {\"0\": 0.539, \"1\": 0.554}, {\"0\": 0.186, \"1\": 0.249}, {\"0\": 0.304, \"1\": 0.328}, {\"0\": 0.772, \"1\": 0.503}, {\"0\": 0.881, \"1\": 0.709}, {\"0\": 0.86, \"1\": 0.56}, {\"0\": 0.283, \"1\": 0.251}, {\"0\": 0.527, \"1\": 0.761}, {\"0\": 0.108, \"1\": 0.657}, {\"0\": 0.512, \"1\": 0.82}, {\"0\": 0.488, \"1\": 0.567}, {\"0\": 0.485, \"1\": 0.966}, {\"0\": 0.212, \"1\": 0.62}, {\"0\": 0.581, \"1\": 0.244}, {\"0\": 0.691, \"1\": 0.58}, {\"0\": 0.362, \"1\": 0.272}, {\"0\": 0.787, \"1\": 0.188}, {\"0\": 0.258, \"1\": 0.696}, {\"0\": 0.241, \"1\": 0.537}, {\"0\": 0.895, \"1\": 0.593}, {\"0\": 0.879, \"1\": 0.261}, {\"0\": 0.421, \"1\": 0.407}, {\"0\": 0.585, \"1\": 0.865}, {\"0\": 0.482, \"1\": 0.36}, {\"0\": 0.619, \"1\": 0.769}, {\"0\": 0.521, \"1\": 0.711}, {\"0\": 0.509, \"1\": 0.127}, {\"0\": 0.954, \"1\": 0.814}, {\"0\": 0.767, \"1\": 0.408}, {\"0\": 0.273, \"1\": 0.466}, {\"0\": 0.0886, \"1\": 0.405}, {\"0\": 0.125, \"1\": 0.374}, {\"0\": 0.821, \"1\": 0.733}, {\"0\": 0.247, \"1\": 0.895}, {\"0\": 0.734, \"1\": 0.851}, {\"0\": 0.924, \"1\": 0.352}, {\"0\": 0.551, \"1\": 0.559}, {\"0\": 0.554, \"1\": 0.815}, {\"0\": 0.608, \"1\": 0.747}, {\"0\": 0.694, \"1\": 0.954}, {\"0\": 0.709, \"1\": 0.162}, {\"0\": 0.8, \"1\": 0.76}, {\"0\": 0.255, \"1\": 0.669}, {\"0\": 0.304, \"1\": 0.168}, {\"0\": 0.239, \"1\": 0.827}, {\"0\": 0.183, \"1\": 0.194}, {\"0\": 0.141, \"1\": 0.372}, {\"0\": 0.919, \"1\": 0.618}, {\"0\": 0.0591, \"1\": 0.581}, {\"0\": 0.761, \"1\": 0.73}, {\"0\": 0.45, \"1\": 0.243}, {\"0\": 0.473, \"1\": 0.794}, {\"0\": 0.508, \"1\": 0.291}, {\"0\": 0.0727, \"1\": 0.207}, {\"0\": 0.54, \"1\": 0.898}, {\"0\": 0.568, \"1\": 0.322}, {\"0\": 0.759, \"1\": 0.781}, {\"0\": 0.604, \"1\": 0.564}, {\"0\": 0.797, \"1\": 0.52}, {\"0\": 0.458, \"1\": 0.492}, {\"0\": 0.668, \"1\": 0.804}, {\"0\": 0.52, \"1\": 0.752}, {\"0\": 0.119, \"1\": 0.606}, {\"0\": 0.123, \"1\": 0.207}, {\"0\": 0.852, \"1\": 0.404}, {\"0\": 0.208, \"1\": 0.151}, {\"0\": 0.953, \"1\": 0.767}, {\"0\": 0.399, \"1\": 0.886}, {\"0\": 0.33, \"1\": 0.715}, {\"0\": 0.581, \"1\": 0.956}, {\"0\": 0.814, \"1\": 0.718}, {\"0\": 0.38, \"1\": 0.364}, {\"0\": 0.667, \"1\": 0.122}, {\"0\": 0.441, \"1\": 0.61}, {\"0\": 0.584, \"1\": 0.357}, {\"0\": 0.548, \"1\": 0.195}, {\"0\": 0.349, \"1\": 0.337}, {\"0\": 0.792, \"1\": 0.381}, {\"0\": 0.688, \"1\": 0.516}, {\"0\": 0.134, \"1\": 0.753}, {\"0\": 0.331, \"1\": 0.848}, {\"0\": 0.524, \"1\": 0.291}, {\"0\": 0.698, \"1\": 0.55}, {\"0\": 0.464, \"1\": 0.468}, {\"0\": 0.586, \"1\": 0.36}, {\"0\": 0.741, \"1\": 0.438}, {\"0\": 0.851, \"1\": 0.389}, {\"0\": 0.447, \"1\": 0.193}, {\"0\": 0.864, \"1\": 0.808}, {\"0\": 0.82, \"1\": 0.93}, {\"0\": 0.171, \"1\": 0.287}, {\"0\": 0.772, \"1\": 0.571}, {\"0\": 0.684, \"1\": 0.189}, {\"0\": 0.122, \"1\": 0.807}, {\"0\": 0.5, \"1\": 0.233}, {\"0\": 0.361, \"1\": 0.775}, {\"0\": 0.201, \"1\": 0.696}, {\"0\": 0.638, \"1\": 0.887}, {\"0\": 0.673, \"1\": 0.653}, {\"0\": 0.869, \"1\": 0.372}, {\"0\": 0.26, \"1\": 0.176}, {\"0\": 0.286, \"1\": 0.555}, {\"0\": 0.679, \"1\": 0.664}, {\"0\": 0.963, \"1\": 0.45}, {\"0\": 0.806, \"1\": 0.635}, {\"0\": 0.585, \"1\": 0.832}, {\"0\": 0.311, \"1\": 0.396}, {\"0\": 0.686, \"1\": 0.463}, {\"0\": 0.129, \"1\": 0.533}, {\"0\": 0.161, \"1\": 0.886}, {\"0\": 0.495, \"1\": 0.262}, {\"0\": 0.461, \"1\": 0.836}, {\"0\": 0.39, \"1\": 0.44}, {\"0\": 0.431, \"1\": 0.577}, {\"0\": 0.296, \"1\": 0.497}, {\"0\": 0.554, \"1\": 0.86}, {\"0\": 0.789, \"1\": 0.734}, {\"0\": 0.24, \"1\": 0.407}, {\"0\": 0.195, \"1\": 0.967}, {\"0\": 0.763, \"1\": 0.31}, {\"0\": 0.463, \"1\": 0.338}, {\"0\": 0.732, \"1\": 0.249}, {\"0\": 0.0951, \"1\": 0.373}, {\"0\": 0.47, \"1\": 0.821}, {\"0\": 0.451, \"1\": 0.915}, {\"0\": 0.719, \"1\": 0.614}, {\"0\": 0.692, \"1\": 0.574}, {\"0\": 0.592, \"1\": 0.297}, {\"0\": 0.322, \"1\": 0.422}, {\"0\": 0.415, \"1\": 0.262}, {\"0\": 0.044, \"1\": 0.962}, {\"0\": 0.679, \"1\": 0.156}, {\"0\": 0.769, \"1\": 0.163}, {\"0\": 0.682, \"1\": 0.677}, {\"0\": 0.716, \"1\": 0.639}, {\"0\": 0.644, \"1\": 0.602}, {\"0\": 0.934, \"1\": 0.43}, {\"0\": 0.78, \"1\": 0.91}, {\"0\": 0.428, \"1\": 0.855}, {\"0\": 0.16, \"1\": 0.378}, {\"0\": 0.486, \"1\": 0.802}, {\"0\": 0.558, \"1\": 0.264}, {\"0\": 0.494, \"1\": 0.713}, {\"0\": 0.666, \"1\": 0.556}, {\"0\": 0.262, \"1\": 0.144}, {\"0\": 0.776, \"1\": 0.48}, {\"0\": 0.722, \"1\": 0.794}, {\"0\": 0.938, \"1\": 0.67}, {\"0\": 0.281, \"1\": 0.824}, {\"0\": 0.707, \"1\": 0.707}, {\"0\": 0.421, \"1\": 0.0768}, {\"0\": 0.655, \"1\": 0.648}, {\"0\": 0.292, \"1\": 0.778}, {\"0\": 0.714, \"1\": 0.374}, {\"0\": 0.887, \"1\": 0.273}, {\"0\": 0.468, \"1\": 0.0933}, {\"0\": 0.575, \"1\": 0.444}, {\"0\": 0.251, \"1\": 0.644}, {\"0\": 0.0974, \"1\": 0.138}, {\"0\": 0.117, \"1\": 0.256}, {\"0\": 0.394, \"1\": 0.779}, {\"0\": 0.893, \"1\": 0.432}, {\"0\": 0.387, \"1\": 0.779}, {\"0\": 0.596, \"1\": 0.386}, {\"0\": 0.223, \"1\": 0.16}, {\"0\": 0.458, \"1\": 0.313}, {\"0\": 0.402, \"1\": 0.399}, {\"0\": 0.545, \"1\": 0.617}, {\"0\": 0.5, \"1\": 0.944}, {\"0\": 0.668, \"1\": 0.261}, {\"0\": 0.26, \"1\": 0.486}, {\"0\": 0.646, \"1\": 0.825}, {\"0\": 0.352, \"1\": 0.869}, {\"0\": 0.6, \"1\": 0.32}, {\"0\": 0.205, \"1\": 0.86}, {\"0\": 0.69, \"1\": 0.583}, {\"0\": 0.0728, \"1\": 0.367}, {\"0\": 0.656, \"1\": 0.209}, {\"0\": 0.461, \"1\": 0.451}, {\"0\": 0.531, \"1\": 0.204}, {\"0\": 0.332, \"1\": 0.108}, {\"0\": 0.502, \"1\": 0.428}, {\"0\": 0.73, \"1\": 0.846}, {\"0\": 0.685, \"1\": 0.664}, {\"0\": 0.551, \"1\": 0.426}, {\"0\": 0.376, \"1\": 0.664}, {\"0\": 0.8, \"1\": 0.57}, {\"0\": 0.16, \"1\": 0.422}, {\"0\": 0.23, \"1\": 0.474}, {\"0\": 0.266, \"1\": 0.642}, {\"0\": 0.327, \"1\": 0.196}, {\"0\": 0.527, \"1\": 0.821}, {\"0\": 0.532, \"1\": 0.592}, {\"0\": 0.35, \"1\": 0.967}, {\"0\": 0.655, \"1\": 0.0635}, {\"0\": 0.132, \"1\": 0.106}, {\"0\": 0.819, \"1\": 0.193}, {\"0\": 0.199, \"1\": 0.0944}, {\"0\": 0.231, \"1\": 0.601}, {\"0\": 0.645, \"1\": 0.678}, {\"0\": 0.22, \"1\": 0.532}, {\"0\": 0.307, \"1\": 0.337}, {\"0\": 0.192, \"1\": 0.538}, {\"0\": 0.495, \"1\": 0.627}, {\"0\": 0.246, \"1\": 0.69}, {\"0\": 0.273, \"1\": 0.658}, {\"0\": 0.205, \"1\": 0.173}, {\"0\": 0.549, \"1\": 0.783}, {\"0\": 0.335, \"1\": 0.405}, {\"0\": 0.524, \"1\": 0.227}, {\"0\": 0.545, \"1\": 0.557}, {\"0\": 0.853, \"1\": 0.591}, {\"0\": 0.313, \"1\": 0.947}, {\"0\": 0.734, \"1\": 0.27}, {\"0\": 0.263, \"1\": 0.704}, {\"0\": 0.312, \"1\": 0.782}, {\"0\": 0.564, \"1\": 0.667}, {\"0\": 0.965, \"1\": 0.0549}, {\"0\": 0.623, \"1\": 0.566}, {\"0\": 0.364, \"1\": 0.752}, {\"0\": 0.184, \"1\": 0.817}, {\"0\": 0.113, \"1\": 0.923}, {\"0\": 0.283, \"1\": 0.33}, {\"0\": 0.323, \"1\": 0.822}, {\"0\": 0.615, \"1\": 0.243}, {\"0\": 0.906, \"1\": 0.579}, {\"0\": 0.533, \"1\": 0.324}, {\"0\": null, \"1\": 0.851}, {\"0\": null, \"1\": 0.865}, {\"0\": null, \"1\": 0.296}, {\"0\": null, \"1\": 0.226}, {\"0\": null, \"1\": 0.311}, {\"0\": null, \"1\": 0.429}, {\"0\": null, \"1\": 0.786}, {\"0\": null, \"1\": 0.355}, {\"0\": null, \"1\": 0.193}, {\"0\": null, \"1\": 0.709}, {\"0\": null, \"1\": 0.46}, {\"0\": null, \"1\": 0.16}, {\"0\": null, \"1\": 0.365}, {\"0\": null, \"1\": 0.62}, {\"0\": null, \"1\": 0.773}, {\"0\": null, \"1\": 0.814}, {\"0\": null, \"1\": 0.504}, {\"0\": null, \"1\": 0.324}, {\"0\": null, \"1\": 0.912}, {\"0\": null, \"1\": 0.295}, {\"0\": null, \"1\": 0.231}, {\"0\": null, \"1\": 0.952}, {\"0\": null, \"1\": 0.351}, {\"0\": null, \"1\": 0.854}, {\"0\": null, \"1\": 0.334}, {\"0\": null, \"1\": 0.87}, {\"0\": null, \"1\": 0.549}, {\"0\": null, \"1\": 0.376}, {\"0\": null, \"1\": 0.357}, {\"0\": null, \"1\": 0.17}, {\"0\": null, \"1\": 0.822}, {\"0\": null, \"1\": 0.356}, {\"0\": null, \"1\": 0.261}, {\"0\": null, \"1\": 0.676}, {\"0\": null, \"1\": 0.337}, {\"0\": null, \"1\": 0.26}, {\"0\": null, \"1\": 0.748}, {\"0\": null, \"1\": 0.52}, {\"0\": null, \"1\": 0.311}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.VConcatChart(...)"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# citations for this question:\n",
    "# [1] concatenating series into DataFrame with renamed columns: https://stackoverflow.com/questions/39047915/concat-series-onto-dataframe-with-column-name\n",
    "# [2] layered altair histogram: https://altair-viz.github.io/gallery/layered_histogram.html\n",
    "# [3] plotting altair charts in a for loop: https://github.com/altair-viz/altair/issues/1281#issuecomment-450783636\n",
    "# [4] display one legend for each concatenated chart: https://stackoverflow.com/questions/60328943/how-to-display-two-different-legends-in-hconcat-chart-using-altair\n",
    "\n",
    "negative_examples = df_train.query(\"target == 0\")\n",
    "positive_examples = df_train.query(\"target == 1\")\n",
    "selected_features = [\"danceability\", \"tempo\", \"energy\", \"valence\"]\n",
    "negative_selected = negative_examples[selected_features]\n",
    "positive_selected = positive_examples[selected_features]\n",
    "\n",
    "charts = []\n",
    "for i in range(len(negative_selected.columns)):\n",
    "    curr_negative = negative_selected.iloc[:, i].reset_index(drop=True)\n",
    "    curr_positive = positive_selected.iloc[:, i].reset_index(drop=True)\n",
    "    feature_data_source = pd.concat([curr_negative.rename(\"0\"), curr_positive.rename(\"1\")], axis=1)   # [1]\n",
    "    \n",
    "    altair_chart = alt.Chart(feature_data_source).transform_fold(                                     # [2]\n",
    "        [\"0\", \"1\"],\n",
    "        as_=['Feature target class', curr_negative.name]\n",
    "    ).mark_area(\n",
    "        opacity=0.5,\n",
    "        interpolate='step'\n",
    "    ).properties(\n",
    "        title=\"Histogram of %s by target class\" % curr_negative.name\n",
    "    ).encode(\n",
    "        alt.X(curr_negative.name + ':Q', bin=alt.Bin(maxbins=50)),\n",
    "        alt.Y('count()', stack=None),\n",
    "        alt.Color('Feature target class:N')\n",
    "    )\n",
    "    charts.append(altair_chart)                                                                       # [3]\n",
    "\n",
    "charts_res = alt.vconcat(*charts).resolve_scale(color='independent')                                               # [3][4]\n",
    "charts_res\n",
    "# charts do not show on gradescope for some reason although they show in my jupyter notebook environment :( please see my attached screenshots\n",
    "# I followed section 20 of this module: https://ml-learn.mds.ubc.ca/en/module3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1(e)\n",
    "rubric={points:4}\n",
    "\n",
    "Let's say you had to make a decision stump (decision tree with depth 1), _by hand_, to predict the target class. Just from looking at the plots above, describe a reasonable split (feature name and threshold) and what class you would predict in the two cases. For example, in the loudness histogram provided earlier on, it seems that very large values of loudness are generally disliked (more blue on the right side of the histogram), so you might answer something like this: \"A reasonable split would be to predict 0 if loudness > -5 (and predict 1 otherwise).\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reasonable split would be to predict 1 if danceability is > 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1(f)\n",
    "rubric={points:2}\n",
    "\n",
    "Let's say that, for a particular feature, the histograms of that feature are identical for the two target classes. Does that mean the feature is not useful for predicting the target class?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the histogram of that feature is identical for the two target classes, it means that the number of records corresponding to each feature value is split evenly.\n",
    "It does not mean that the feature is not useful - it just means that if a model is trained with that feature, there is a 50% chance of each target class being observed given any value that falls within the observed values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86f9e0c649669daf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 1(g) \n",
    "rubric={points:2}\n",
    "\n",
    "Note that the dataset includes two free text features labeled `song_title` and `artist`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_title</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>WTF (Where They From) [feat. Pharrell Williams]</td>\n",
       "      <td>Missy Elliott</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1286</th>\n",
       "      <td>10,000 Reasons (Bless the Lord) [Radio Version]</td>\n",
       "      <td>Matt Redman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>American Dream</td>\n",
       "      <td>Chelsea Grin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>Feel This Moment</td>\n",
       "      <td>Pitbull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Trap Queen</td>\n",
       "      <td>Fetty Wap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           song_title         artist\n",
       "260   WTF (Where They From) [feat. Pharrell Williams]  Missy Elliott\n",
       "1286  10,000 Reasons (Bless the Lord) [Radio Version]    Matt Redman\n",
       "1344                                   American Dream   Chelsea Grin\n",
       "1197                                 Feel This Moment        Pitbull\n",
       "119                                        Trap Queen      Fetty Wap"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[[\"song_title\", \"artist\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do you think these features could be useful in predicting whether the user liked the song or not? \n",
    "- Would there be any difficulty in using them in your model?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-dce517defdc16360",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "They would be useful in predicting whether or not the user liked the song, because people may like songs written by certain artists, or songs that contain a special word in the title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1440876fbc49ead5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2: Using sklearn to build a decision tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-706403e72adade4b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### 2(a) \n",
    "rubric={points:2}\n",
    "\n",
    "- Create `X_train` and `y_train` and `X_test` and `y_test` from `df_train` and `df_test` above. Skip the `song_title` and `artist` features for now. \n",
    "- Fit a `DecisionTreeClassifier` on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-859d4a70667da85d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=321)"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df_train.drop([\"target\", \"song_title\", \"artist\"], axis=1)\n",
    "X_test = df_test.drop([\"target\", \"song_title\", \"artist\"], axis=1)\n",
    "y_train = df_train[\"target\"]\n",
    "y_test = df_test[\"target\"]\n",
    "model = DecisionTreeClassifier(random_state=321)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-43ac6f91bc3bd9da",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### 2(b)\n",
    "rubric={points:2}\n",
    "\n",
    "Use the `predict` method to predict the class of the first example in your `X_train`. Is the prediction correct? That is, does it match with the corresponding class in `y_train`?  \n",
    "\n",
    "> Hint: you can grab the first example with `X_train.iloc[[0]]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction is correct; it matches with the corresponding y_train (as the target column within df_train). See the last column:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>key</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>valence</th>\n",
       "      <th>target</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.932</td>\n",
       "      <td>192773</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>-3.484</td>\n",
       "      <td>0</td>\n",
       "      <td>0.203</td>\n",
       "      <td>119.941</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.552</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acousticness  danceability  duration_ms  energy  instrumentalness  key  \\\n",
       "0        0.0181         0.932       192773   0.819          0.000007    8   \n",
       "\n",
       "   liveness  loudness  mode  speechiness    tempo  time_signature  valence  \\\n",
       "0    0.0577    -3.484     0        0.203  119.941             4.0    0.552   \n",
       "\n",
       "   target  predicted  \n",
       "0       1          1  "
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = model.predict(X_train.iloc[[0]])\n",
    "display_predicted = pd.concat([df_train.drop([\"song_title\", \"artist\"], axis=1).iloc[[0]].reset_index(drop=True), pd.Series(predicted, name='predicted')], axis=1)\n",
    "print('The prediction is correct; it matches with the corresponding y_train (as the target column within df_train). See the last column:')\n",
    "display_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2(c) \n",
    "rubric={points:2}\n",
    "\n",
    "Use the `cross_val_score` function on your training set to compute the 10-fold cross-validation accuracy of your tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7037037 , 0.62962963, 0.66049383, 0.71428571, 0.7515528 ,\n",
       "       0.61490683, 0.70186335, 0.74534161, 0.63354037, 0.68944099])"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_score = cross_val_score(model, X_train, y_train, cv=10)\n",
    "cv_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2(d)\n",
    "rubric={points:2}\n",
    "\n",
    "The above is useful, but we would like to see the training accuracy as well. \n",
    "\n",
    "- Compute the 10-fold cross-validation again but this time using the `cross_validate` function with `return_train_score=True`. \n",
    "- Print out both the cross-validation score and the training score.\n",
    "- Is your cross-validation score exactly the same as what you got in the previous part? Very briefly discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross-validation scores for each fold:\n",
      "    fit_time  score_time  test_score  train_score\n",
      "0  0.027605    0.003588    0.703704     0.999311\n",
      "1  0.030504    0.003922    0.629630     0.998622\n",
      "2  0.028457    0.002266    0.660494     0.999311\n",
      "3  0.022920    0.001636    0.714286     0.998623\n",
      "4  0.016269    0.002088    0.751553     0.998623\n",
      "5  0.016572    0.001564    0.614907     0.998623\n",
      "6  0.015616    0.002036    0.701863     0.998623\n",
      "7  0.015579    0.002294    0.745342     0.999311\n",
      "8  0.017138    0.001534    0.633540     0.999311\n",
      "9  0.014733    0.001499    0.689441     0.998623\n",
      "\n",
      "The training score: 0.998760\n",
      "\n",
      "=================\n",
      "\n",
      "The mean cross-validation scores in this question 0.684476:\n",
      "\n",
      "The mean cross-validation score from 2c: 0.684476\n",
      "\n",
      "The cross-validation scores (represented by test_score) in this question is the same as the cross-validation score in the previous question.\n",
      "Both cross_validate and cross_val_score are using the same random_state=321, which means that the scores are computed from the same splits (example rows).\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validate(model, X_train, y_train, cv=10, return_train_score=True)\n",
    "scores_df = pd.DataFrame(scores)\n",
    "scores_df_mean = scores_df.mean()\n",
    "training_score = model.score(X_train, y_train)\n",
    "print(\"The cross-validation scores for each fold:\\n\", scores_df)\n",
    "print(\"\\nThe training score: %0.6f\" % training_score)\n",
    "print(\"\\n=================\")\n",
    "\n",
    "scores_df_mean_test_score = scores_df_mean.iloc[2]\n",
    "cv_score_mean = cv_score.mean()\n",
    "print(\"\\nThe mean cross-validation scores in this question %0.6f:\" % scores_df_mean_test_score)\n",
    "print(\"\\nThe mean cross-validation score from 2c: %0.6f\" % cv_score_mean)\n",
    "print(\"\\nThe cross-validation scores (represented by test_score) in this question is the same as the cross-validation score in the previous question.\")\n",
    "print('Both cross_validate and cross_val_score are using the same random_state=321, which means that the scores are computed from the same splits (example rows).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2(e)\n",
    "rubric={points:1}\n",
    "\n",
    "Do you see a significant difference between the training score and the cross-validation score? Briefly discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a89757274fc5586f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score calculated in 2(d): 0.998760\n",
      "Average of 10-fold cross-validation scores calculated in 2(d): 0.6844763\n",
      "Compared to the training score calculated in 2(d), the cross-validation score is quite far away.\n",
      "The mean cross-validation score from all the splits is around 0.3 points lower than the training score - this suggests that we're likely overfitting.\n"
     ]
    }
   ],
   "source": [
    "print('Training score calculated in 2(d): %f' % training_score)\n",
    "print('Average of 10-fold cross-validation scores calculated in 2(d): %f3' % scores_df_mean_test_score)\n",
    "print('Compared to the training score calculated in 2(d), the cross-validation score is quite far away.')\n",
    "print(\"The mean cross-validation score from all the splits is around 0.3 points lower than the training score - this suggests that we're likely overfitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2(f)\n",
    "rubric={points:1}\n",
    "\n",
    "Inspect the 10 sub-scores from the 10 folds of cross-validation. How does this inform the trustworthiness of your cross validation score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a89757274fc5586f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 sub-scores from the 10 folds of cross-validation (mean = 0.684476) is same as the cross-validation score (mean of cross-validation scores: 0.684476) - this means that the dataset is sufficiently large and insensitive to splitting behaviour, making the cross-validation score trustworthy.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.027605</td>\n",
       "      <td>0.003588</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.999311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.030504</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.998622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.028457</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>0.660494</td>\n",
       "      <td>0.999311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022920</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.998623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016269</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>0.751553</td>\n",
       "      <td>0.998623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.016572</td>\n",
       "      <td>0.001564</td>\n",
       "      <td>0.614907</td>\n",
       "      <td>0.998623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.015616</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.701863</td>\n",
       "      <td>0.998623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.015579</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>0.745342</td>\n",
       "      <td>0.999311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.017138</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>0.633540</td>\n",
       "      <td>0.999311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.014733</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.689441</td>\n",
       "      <td>0.998623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_score  train_score\n",
       "0  0.027605    0.003588    0.703704     0.999311\n",
       "1  0.030504    0.003922    0.629630     0.998622\n",
       "2  0.028457    0.002266    0.660494     0.999311\n",
       "3  0.022920    0.001636    0.714286     0.998623\n",
       "4  0.016269    0.002088    0.751553     0.998623\n",
       "5  0.016572    0.001564    0.614907     0.998623\n",
       "6  0.015616    0.002036    0.701863     0.998623\n",
       "7  0.015579    0.002294    0.745342     0.999311\n",
       "8  0.017138    0.001534    0.633540     0.999311\n",
       "9  0.014733    0.001499    0.689441     0.998623"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The 10 sub-scores from the 10 folds of cross-validation (mean = %f) is same as the cross-validation score (mean of cross-validation scores: %f) - this means that the dataset is sufficiently large and insensitive to splitting behaviour, making the cross-validation score trustworthy.' % (scores_df_mean_test_score, cv_score_mean))\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4150979c1845a18c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 3: Hyperparameters \n",
    "rubric={points:10}\n",
    "\n",
    "In this exercise, you'll experiment with the `max_depth` hyperparameter of the decision tree classifier. See the [`DecisionTreeClassifier` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) for more details.\n",
    "\n",
    "- Explore the `max_depth` hyperparameter. Run 10-fold cross-validation for trees with different values of `max_depth` (at least 10 different values in the range 1 to 25).\n",
    "- For each `max_depth`, get both the train accuracy and the cross-validation accuracy.\n",
    "- Make a plot with `max_depth` on the *x*-axis and the train and cross-validation scores on the *y*-axis. That is, your plot should have two curves, one for train and one for cross-validation. Include a legend to specify which is which.\n",
    "- Discuss how changing the `max_depth` hyperparameter affects the training and cross-validation accuracy. From these results, what depth would you pick as the optimal depth? \n",
    "- Do you think that the depth you chose would generalize to other \"spotify\" datasets (i.e., data on other spotify users)?\n",
    "\n",
    "> Note: generally speaking (for all assignments) you are welcome to copy/paste code directly from the lecture notes, though I ask that you add a small citation (e.g. \"Adapted from lecture 2\") if you do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How changing the max_depth hyperparameter affects the training and cross-validation accuracy:\n",
      "Increasing the max_depth hyperparameter increases training accuracy, but once the max_depth of the decision tree gets too deep, increasing the depth rather reduces the cross-validation accuracy.\n",
      "For example, as the max_depth increases beyond 4.0, the cross-validation accuracy starts to decrease from around 0.7328 to 0.71.\n",
      "This is not a significant decrease, but it means that when the depth of the tree increases, although the training accuracy increases, the model starts to pick up on more quirks of the training dataset.\n",
      "Once the decision tree's decisions get too complex, we risk overfitting because the patterns we picked up from the training sets are too specific to be used for the validation sets, increasing the validation error.\n",
      "\n",
      "Optimal depth: 4.0\n",
      "\n",
      "Would this optimal depth generalize to other spotify datasets: \n",
      "If this model is generalized on other datasets significantly larger in size, our model may not generalize well - there are millions of spotify users, and the size of our training data is only in the thousands (n = 1613.000000). \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-79267feac3e34b09b0610e738184ab89\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-79267feac3e34b09b0610e738184ab89\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-79267feac3e34b09b0610e738184ab89\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-8ac6b69275ae73e49fb3ded24b5dd9de\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"split\", \"scale\": {\"domain\": [\"mean_train_score\", \"mean_cv_score\"], \"range\": [\"purple\", \"gold\"]}}, \"x\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"Max Tree Depth\"}, \"field\": \"depth\"}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"Train and Cross-Validation Accuracy\"}, \"field\": \"score\", \"scale\": {\"domain\": [0.5, 1.0]}}}, \"title\": \"Max Depth of DecisionTreeClassifier vs. Train and Cross-Validation Accuracy\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-8ac6b69275ae73e49fb3ded24b5dd9de\": [{\"depth\": 1, \"split\": \"mean_train_score\", \"score\": 0.6455193340585861}, {\"depth\": 2, \"split\": \"mean_train_score\", \"score\": 0.7179862183010481}, {\"depth\": 3, \"split\": \"mean_train_score\", \"score\": 0.7316246703612783}, {\"depth\": 4, \"split\": \"mean_train_score\", \"score\": 0.7643454310032219}, {\"depth\": 5, \"split\": \"mean_train_score\", \"score\": 0.7926579560405762}, {\"depth\": 6, \"split\": \"mean_train_score\", \"score\": 0.8231743852914205}, {\"depth\": 7, \"split\": \"mean_train_score\", \"score\": 0.8583048073618841}, {\"depth\": 8, \"split\": \"mean_train_score\", \"score\": 0.887579763552447}, {\"depth\": 9, \"split\": \"mean_train_score\", \"score\": 0.9151331939785046}, {\"depth\": 10, \"split\": \"mean_train_score\", \"score\": 0.9376579845190834}, {\"depth\": 11, \"split\": \"mean_train_score\", \"score\": 0.9586680507221201}, {\"depth\": 12, \"split\": \"mean_train_score\", \"score\": 0.9725140636361737}, {\"depth\": 13, \"split\": \"mean_train_score\", \"score\": 0.9817452293753904}, {\"depth\": 14, \"split\": \"mean_train_score\", \"score\": 0.9882200553242468}, {\"depth\": 15, \"split\": \"mean_train_score\", \"score\": 0.9928356144617657}, {\"depth\": 16, \"split\": \"mean_train_score\", \"score\": 0.9957979962522284}, {\"depth\": 17, \"split\": \"mean_train_score\", \"score\": 0.9975201390510581}, {\"depth\": 18, \"split\": \"mean_train_score\", \"score\": 0.9983467277245863}, {\"depth\": 19, \"split\": \"mean_train_score\", \"score\": 0.9986222572824289}, {\"depth\": 20, \"split\": \"mean_train_score\", \"score\": 0.9987600457934398}, {\"depth\": 21, \"split\": \"mean_train_score\", \"score\": 0.9988289637810347}, {\"depth\": 22, \"split\": \"mean_train_score\", \"score\": 0.9988978817686291}, {\"depth\": 23, \"split\": \"mean_train_score\", \"score\": 0.9988978817686291}, {\"depth\": 24, \"split\": \"mean_train_score\", \"score\": 0.9988978817686291}, {\"depth\": 1, \"split\": \"mean_cv_score\", \"score\": 0.6410628019323672}, {\"depth\": 2, \"split\": \"mean_cv_score\", \"score\": 0.710505329345909}, {\"depth\": 3, \"split\": \"mean_cv_score\", \"score\": 0.7098880453952918}, {\"depth\": 4, \"split\": \"mean_cv_score\", \"score\": 0.7327812284334023}, {\"depth\": 5, \"split\": \"mean_cv_score\", \"score\": 0.7098918794570968}, {\"depth\": 6, \"split\": \"mean_cv_score\", \"score\": 0.7173299593589448}, {\"depth\": 7, \"split\": \"mean_cv_score\", \"score\": 0.7154742734452879}, {\"depth\": 8, \"split\": \"mean_cv_score\", \"score\": 0.7161068936431254}, {\"depth\": 9, \"split\": \"mean_cv_score\", \"score\": 0.7105244996549345}, {\"depth\": 10, \"split\": \"mean_cv_score\", \"score\": 0.7074150755310175}, {\"depth\": 11, \"split\": \"mean_cv_score\", \"score\": 0.704313319530711}, {\"depth\": 12, \"split\": \"mean_cv_score\", \"score\": 0.7031055900621117}, {\"depth\": 13, \"split\": \"mean_cv_score\", \"score\": 0.6906832298136647}, {\"depth\": 14, \"split\": \"mean_cv_score\", \"score\": 0.687585307875163}, {\"depth\": 15, \"split\": \"mean_cv_score\", \"score\": 0.6925274135419063}, {\"depth\": 16, \"split\": \"mean_cv_score\", \"score\": 0.6807721800475423}, {\"depth\": 17, \"split\": \"mean_cv_score\", \"score\": 0.6776589218618203}, {\"depth\": 18, \"split\": \"mean_cv_score\", \"score\": 0.6838624338624337}, {\"depth\": 19, \"split\": \"mean_cv_score\", \"score\": 0.6869641898627405}, {\"depth\": 20, \"split\": \"mean_cv_score\", \"score\": 0.6832413158500115}, {\"depth\": 21, \"split\": \"mean_cv_score\", \"score\": 0.6826240318993941}, {\"depth\": 22, \"split\": \"mean_cv_score\", \"score\": 0.684475883751246}, {\"depth\": 23, \"split\": \"mean_cv_score\", \"score\": 0.684475883751246}, {\"depth\": 24, \"split\": \"mean_cv_score\", \"score\": 0.684475883751246}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [citation] Adapted from Introduction to Machine Learning, Module 3, section 20: https://ml-learn.mds.ubc.ca/en/module3\n",
    "\n",
    "res = {'depth': [], 'mean_train_score': [], 'mean_cv_score': []}\n",
    "max_cv_score = -sys.maxsize\n",
    "max_depth = 1;\n",
    "for i in range(1, 25):\n",
    "    model = DecisionTreeClassifier(max_depth=i, random_state=321)\n",
    "    scores = cross_validate(model, X_train, y_train, cv=10, return_train_score=True)\n",
    "    res['depth'].append(i)\n",
    "    res['mean_train_score'].append(scores['train_score'].mean())\n",
    "    mean_cv_score = scores['test_score'].mean()\n",
    "    res['mean_cv_score'].append(mean_cv_score)\n",
    "    if (mean_cv_score > max_cv_score):\n",
    "        max_cv_score = mean_cv_score\n",
    "        max_depth = i\n",
    "\n",
    "res_df = pd.DataFrame(res).melt(id_vars=['depth'],\n",
    "                                value_vars=['mean_train_score',\n",
    "                                            'mean_cv_score'],\n",
    "                                var_name='split',\n",
    "                                value_name='score')\n",
    "\n",
    "chart = alt.Chart(res_df).mark_line().properties(\n",
    "            title='Max Depth of DecisionTreeClassifier vs. Train and Cross-Validation Accuracy'\n",
    "        ).encode(\n",
    "            alt.X('depth:Q', axis=alt.Axis(title='Max Tree Depth')),\n",
    "            alt.Y('score:Q', axis=alt.Axis(title='Train and Cross-Validation Accuracy'),\n",
    "                            scale=alt.Scale(domain=[.50, 1.00])),\n",
    "            alt.Color('split:N', scale=alt.Scale(domain=['mean_train_score',\n",
    "                                                         'mean_cv_score'],\n",
    "                                                 range=['purple', 'gold'])))\n",
    "print('How changing the max_depth hyperparameter affects the training and cross-validation accuracy:')\n",
    "print('Increasing the max_depth hyperparameter increases training accuracy, but once the max_depth of the decision tree gets too deep, increasing the depth rather reduces the cross-validation accuracy.')\n",
    "print('For example, as the max_depth increases beyond %0.1f, the cross-validation accuracy starts to decrease from around %0.4f to 0.71.' % (max_depth, max_cv_score))\n",
    "print('This is not a significant decrease, but it means that when the depth of the tree increases, although the training accuracy increases, the model starts to pick up on more quirks of the training dataset.')\n",
    "print('Once the decision tree\\'s decisions get too complex, we risk overfitting because the patterns we picked up from the training sets are too specific to be used for the validation sets, increasing the validation error.')\n",
    "\n",
    "print('\\nOptimal depth: %0.1f' % max_depth)\n",
    "\n",
    "print('\\nWould this optimal depth generalize to other spotify datasets: ')\n",
    "print('If this model is generalized on other datasets significantly larger in size, our model may not generalize well - there are millions of spotify users, and the size of our training data is only in the thousands (n = %f). \\n' % len(X_train))\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Test set\n",
    "rubric={points:4}\n",
    "\n",
    "Remember the test set you created way back at the beginning of this assignment? Let's use it now to see if our cross-validation score from the previous exercise is trustworthy. \n",
    "\n",
    "- Select your favorite `max_depth` from the previous part.\n",
    "- Train a decision tree classifier using that `max_depth` on the _entire training set_.\n",
    "- Compute and display the test score. \n",
    "- How does it compare to the cross-validation score from the previous exercise? Briefly discuss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test score of our model with max_depth=4.0: 0.693069\n",
      "\n",
      "The computed test_score (0.693069) is comparable with the cross-validation score (0.732781) obtained with max_depth=4.0\n",
      "\n",
      "Since the test set likely has some 'quirks' not found within our validation and training sets, it's expected that our model won't fit all the random patterns found in the test set.\n",
      "As a result, we expect the test accuracy to be slightly lower than the validation score (and the test error, 1-test_score, to be higher than the validation error) - this illustrates the fundamental tradeoff of supervised learning.\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(max_depth=max_depth, random_state=321)\n",
    "model.fit(X_train, y_train)\n",
    "test_score = model.score(X_test, y_test)\n",
    "print(\"The test score of our model with max_depth=%0.1f: %0.6f\" % (max_depth, test_score))\n",
    "print('\\nThe computed test_score (%0.6f) is comparable with the cross-validation score (%0.6f) obtained with max_depth=%0.1f' % (test_score, max_cv_score, max_depth))\n",
    "print('\\nSince the test set likely has some \\'quirks\\' not found within our validation and training sets, it\\'s expected that our model won\\'t fit all the random patterns found in the test set.')\n",
    "print('As a result, we expect the test accuracy to be slightly lower than the validation score (and the test error, 1-test_score, to be higher than the validation error) - this illustrates the fundamental tradeoff of supervised learning.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Conceptual questions\n",
    "rubric={points:3}\n",
    "\n",
    "Consider the dataset below, which has $6$ examples and $2$ features:\n",
    "\n",
    "$$ X = \\begin{bmatrix}5 & 2\\\\4 & 3\\\\  2 & 2\\\\ 10 & 10\\\\ 9 & -1\\\\ 9& 9\\end{bmatrix}, \\quad y = \\begin{bmatrix}-1\\\\-1\\\\+1\\\\+1\\\\+1\\\\+1\\end{bmatrix}.$$\n",
    "\n",
    "1. Say we fit a decision stump (depth 1 decision tree) and the first split is on the first feature (left column) being less than 5.5. What would we predict in the \"true\" and \"false\" cases here?\n",
    "2. What training accuracy would the above stump get on this data set?\n",
    "3. Can we obtain 100% accuracy with a single decision stump in this particular example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) What would we predict in the \"true\" and \"false\" cases?\n",
      "\n",
      "   X0  predicted\n",
      "0   5          1\n",
      "1   4          1\n",
      "2   2          1\n",
      "3  10         -1\n",
      "4   9         -1\n",
      "5   9         -1\n",
      "\n",
      "2) Training accuracy of above stump: 0.1667\n",
      "\n",
      "3) Can we obtain 100% accuracy with a single decision stump in this particular example? In this case, we can make a decision stump with the condition such that if the first feature is greater than 2 AND less than 6, we return false\n"
     ]
    }
   ],
   "source": [
    "# citations\n",
    "# [1] concatenate dataframes: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\n",
    "# [2] update values in pandas dataframe: https://www.kite.com/python/answers/how-to-update-a-value-in-each-row-of-a-pandas-dataframe-in-python#:~:text=Update%20elements%20of%20a%20column,the%20column%20to%20be%20changed.\n",
    "df_5 = pd.DataFrame({'X0': [5,4,2,10,9,9], 'X1': [2,3,2,10,-1,9], 'target': [-1,-1,1,1,1,1]})\n",
    "X_train = df_5.drop(['target', 'X1'], axis=1)\n",
    "y_train = df_5['target']\n",
    "df_w_predicted = pd.DataFrame({\"predicted\": [0,0,0,0,0,0]})\n",
    "df_w_predicted = pd.concat([X_train, df_w_predicted], axis=1, join='inner')\n",
    "for i in range(len(X_train)):\n",
    "    predicted = 1 if df_w_predicted.at[i, \"X0\"] < 5.5 else -1\n",
    "    df_w_predicted.at[i, \"predicted\"] = predicted\n",
    "df_w_predicted\n",
    "\n",
    "num_correct = 0;\n",
    "for i in range(len(X_train)):\n",
    "    if (df_w_predicted.at[i, \"predicted\"] == y_train[[i]].values[0]):\n",
    "        num_correct+=1\n",
    "training_accuracy = (num_correct / len(X_train))\n",
    "\n",
    "print('1) What would we predict in the \"true\" and \"false\" cases?\\n')\n",
    "print(df_w_predicted)\n",
    "\n",
    "print('\\n2) Training accuracy of above stump: ' + '%0.4f' % training_accuracy)\n",
    "print('\\n3) Can we obtain 100% accuracy with a single decision stump in this particular example? In this case, we can make a decision stump with the condition such that if the first feature is greater than 2 AND less than 6, we return false')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions \n",
    "\n",
    "**PLEASE READ:** When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from “1” will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using Gradescope's drag and drop tool. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
