{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 6: Putting it all together \n",
    "### Associated lectures: All material till lecture 13 \n",
    "\n",
    "**Due date: Monday, November 15, 2021 at 11:59pm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "- [Submission instructions](#si)\n",
    "- [Understanding the problem](#1)\n",
    "- [Data splitting](#2)\n",
    "- [EDA](#3)\n",
    "- (Optional) [Feature engineering](#4)\n",
    "- [Preprocessing and transformations](#5)\n",
    "- [Baseline model](#6)\n",
    "- [Linear models](#7)\n",
    "- [Different classifiers](#8)\n",
    "- (Optional) [Feature selection](#9)\n",
    "- [Hyperparameter optimization](#10)\n",
    "- [Interpretation and feature importances](#11)\n",
    "- [Results on the test set](#12)\n",
    "- (Optional) [Explaining predictions](#13)\n",
    "- [Summary of the results](#14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, RidgeCV\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    make_scorer,\n",
    "    plot_confusion_matrix,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions \n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330/blob/master/docs/homework_instructions.md). \n",
    "\n",
    "**You may work on this homework in a group and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "- The maximum group size is 3. \n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"in\"></a>\n",
    "<hr>\n",
    "\n",
    "At this point we are at the end of supervised machine learning part of the course. So in this homework, you will be working on an open-ended mini-project, where you will put all the different things you have learned so far together to solve an interesting problem.\n",
    "\n",
    "A few notes and tips when you work on this mini-project: \n",
    "\n",
    "#### Tips\n",
    "\n",
    "1. This mini-project is open-ended, and while working on it, there might be some situations where you'll have to use your own judgment and make your own decisions (as you would be doing when you work as a data scientist). Make sure you explain your decisions whenever necessary. \n",
    "2. **Do not include everything you ever tried in your submission** -- it's fine just to have your final code. That said, your code should be reproducible and well-documented. For example, if you chose your hyperparameters based on some hyperparameter optimization experiment, you should leave in the code for that experiment so that someone else could re-run it and obtain the same hyperparameters, rather than mysteriously just setting the hyperparameters to some (carefully chosen) values in your code. \n",
    "3. If you realize that you are repeating a lot of code try to organize it in functions. Clear presentation of your code, experiments, and results is the key to be successful in this lab. You may use code from lecture notes or previous lab solutions with appropriate attributions. \n",
    "4. If you are having trouble running models on your laptop because of the size of the dataset, you can create your train/test split in such a way that you have less data in the train split. If you end up doing this, please write a note to the grader in the submission explaining why you are doing it.  \n",
    "\n",
    "#### Assessment\n",
    "\n",
    "We plan to grade fairly and leniently. We don't have some secret target score that you need to achieve to get a good grade. **You'll be assessed on demonstration of mastery of course topics, clear presentation, and the quality of your analysis and results.** For example, if you just have a bunch of code and no text or figures, that's not good. If you do a bunch of sane things and get a lower accuracy than your friend, don't sweat it.\n",
    "\n",
    "#### A final note\n",
    "\n",
    "Finally, this style of this \"project\" question is different from other assignments. It'll be up to you to decide when you're \"done\" -- in fact, this is one of the hardest parts of real projects. But please don't spend WAY too much time on this... perhaps \"a few hours\" (2-8 hours???) is a good guideline for a typical submission. Of course if you're having fun you're welcome to spend as much time as you want! But, if so, try not to do it out of perfectionism or getting the best possible grade. Do it because you're learning and enjoying it. Students from the past cohorts have found such kind of labs useful and fun and I hope you enjoy it as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the problem <a name=\"1\"></a>\n",
    "<hr>\n",
    "rubric={points:4}\n",
    "\n",
    "In this mini project, you will be working on a classification problem of predicting whether a credit card client will default or not. \n",
    "For this problem, you will use [Default of Credit Card Clients Dataset](https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset). In this data set, there are 30,000 examples and 24 features, and the goal is to estimate whether a person will default (fail to pay) their credit card bills; this column is labeled \"default.payment.next.month\" in the data. The rest of the columns can be used as features. You may take some ideas and compare your results with [the associated research paper](https://www.sciencedirect.com/science/article/pii/S0957417407006719), which is available through [the UBC library](https://www.library.ubc.ca/). \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Spend some time understanding the problem and what each feature means. You can find this information in the documentation on [the dataset page on Kaggle](https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset). Write a few sentences on your initial thoughts on the problem and the dataset. \n",
    "2. Download the dataset and read it as a pandas dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. My initial thoughts, after exploring the features, is that `PAY_[0-6]` are strong predictors of default payment. Intuitively, when a client consistently delays their payments, they are less likely to be able to pay off their debt, and thus will likely default. Another pattern I saw from examining the data is that clients who consistently had `PAY_AMT[1-6]` = 0 with `BILL_AMT[1-6]` > 0 were also likely to default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default.payment.next.month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272.0</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>3261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331.0</td>\n",
       "      <td>14948.0</td>\n",
       "      <td>15549.0</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314.0</td>\n",
       "      <td>28959.0</td>\n",
       "      <td>29547.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940.0</td>\n",
       "      <td>19146.0</td>\n",
       "      <td>19131.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>36681.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>29996</td>\n",
       "      <td>220000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>88004.0</td>\n",
       "      <td>31237.0</td>\n",
       "      <td>15980.0</td>\n",
       "      <td>8500.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>5003.0</td>\n",
       "      <td>3047.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>29997</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>8979.0</td>\n",
       "      <td>5190.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1837.0</td>\n",
       "      <td>3526.0</td>\n",
       "      <td>8998.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>29998</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>20878.0</td>\n",
       "      <td>20582.0</td>\n",
       "      <td>19357.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>29999</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>52774.0</td>\n",
       "      <td>11855.0</td>\n",
       "      <td>48944.0</td>\n",
       "      <td>85900.0</td>\n",
       "      <td>3409.0</td>\n",
       "      <td>1178.0</td>\n",
       "      <td>1926.0</td>\n",
       "      <td>52964.0</td>\n",
       "      <td>1804.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>30000</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>36535.0</td>\n",
       "      <td>32428.0</td>\n",
       "      <td>15313.0</td>\n",
       "      <td>2078.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  \\\n",
       "0          1    20000.0    2          2         1   24      2      2     -1   \n",
       "1          2   120000.0    2          2         2   26     -1      2      0   \n",
       "2          3    90000.0    2          2         2   34      0      0      0   \n",
       "3          4    50000.0    2          2         1   37      0      0      0   \n",
       "4          5    50000.0    1          2         1   57     -1      0     -1   \n",
       "...      ...        ...  ...        ...       ...  ...    ...    ...    ...   \n",
       "29995  29996   220000.0    1          3         1   39      0      0      0   \n",
       "29996  29997   150000.0    1          3         2   43     -1     -1     -1   \n",
       "29997  29998    30000.0    1          2         2   37      4      3      2   \n",
       "29998  29999    80000.0    1          3         1   41      1     -1      0   \n",
       "29999  30000    50000.0    1          2         1   46      0      0      0   \n",
       "\n",
       "       PAY_4  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  \\\n",
       "0         -1  ...        0.0        0.0        0.0       0.0     689.0   \n",
       "1          0  ...     3272.0     3455.0     3261.0       0.0    1000.0   \n",
       "2          0  ...    14331.0    14948.0    15549.0    1518.0    1500.0   \n",
       "3          0  ...    28314.0    28959.0    29547.0    2000.0    2019.0   \n",
       "4          0  ...    20940.0    19146.0    19131.0    2000.0   36681.0   \n",
       "...      ...  ...        ...        ...        ...       ...       ...   \n",
       "29995      0  ...    88004.0    31237.0    15980.0    8500.0   20000.0   \n",
       "29996     -1  ...     8979.0     5190.0        0.0    1837.0    3526.0   \n",
       "29997     -1  ...    20878.0    20582.0    19357.0       0.0       0.0   \n",
       "29998      0  ...    52774.0    11855.0    48944.0   85900.0    3409.0   \n",
       "29999      0  ...    36535.0    32428.0    15313.0    2078.0    1800.0   \n",
       "\n",
       "       PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  default.payment.next.month  \n",
       "0           0.0       0.0       0.0       0.0                           1  \n",
       "1        1000.0    1000.0       0.0    2000.0                           1  \n",
       "2        1000.0    1000.0    1000.0    5000.0                           0  \n",
       "3        1200.0    1100.0    1069.0    1000.0                           0  \n",
       "4       10000.0    9000.0     689.0     679.0                           0  \n",
       "...         ...       ...       ...       ...                         ...  \n",
       "29995    5003.0    3047.0    5000.0    1000.0                           0  \n",
       "29996    8998.0     129.0       0.0       0.0                           0  \n",
       "29997   22000.0    4200.0    2000.0    3100.0                           1  \n",
       "29998    1178.0    1926.0   52964.0    1804.0                           1  \n",
       "29999    1430.0    1000.0    1000.0    1000.0                           1  \n",
       "\n",
       "[30000 rows x 25 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./UCI_Credit_Card.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data splitting <a name=\"2\"></a>\n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Split the data into train and test portions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=123)\n",
    "\n",
    "X_train = train_df.drop(columns=[\"default.payment.next.month\"])\n",
    "y_train = train_df[\"default.payment.next.month\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"default.payment.next.month\"])\n",
    "y_test = test_df[\"default.payment.next.month\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EDA <a name=\"3\"></a>\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Perform exploratory data analysis on the train set.\n",
    "2. Include at least two summary statistics and two visualizations that you find useful, and accompany each one with a sentence explaining it.\n",
    "3. Summarize your initial observations about the data. \n",
    "4. Pick appropriate metric/metrics for assessment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Perform exploratory data analysis on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default.payment.next.month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>24000.000000</td>\n",
       "      <td>24000.000000</td>\n",
       "      <td>24000.000000</td>\n",
       "      <td>24000.000000</td>\n",
       "      <td>24000.000000</td>\n",
       "      <td>24000.000000</td>\n",
       "      <td>24000.000000</td>\n",
       "      <td>24000.000000</td>\n",
       "      <td>24000.000000</td>\n",
       "      <td>24000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>24000.000000</td>\n",
       "      <td>24000.000000</td>\n",
       "      <td>24000.000000</td>\n",
       "      <td>24000.000000</td>\n",
       "      <td>2.400000e+04</td>\n",
       "      <td>24000.000000</td>\n",
       "      <td>24000.000000</td>\n",
       "      <td>24000.000000</td>\n",
       "      <td>24000.000000</td>\n",
       "      <td>24000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14964.174292</td>\n",
       "      <td>167893.486667</td>\n",
       "      <td>1.603125</td>\n",
       "      <td>1.851958</td>\n",
       "      <td>1.553375</td>\n",
       "      <td>35.488458</td>\n",
       "      <td>-0.017542</td>\n",
       "      <td>-0.135292</td>\n",
       "      <td>-0.170042</td>\n",
       "      <td>-0.224292</td>\n",
       "      <td>...</td>\n",
       "      <td>43389.105625</td>\n",
       "      <td>40297.970375</td>\n",
       "      <td>38708.777542</td>\n",
       "      <td>5656.319917</td>\n",
       "      <td>5.910454e+03</td>\n",
       "      <td>5280.658708</td>\n",
       "      <td>4763.854250</td>\n",
       "      <td>4805.837667</td>\n",
       "      <td>5277.577958</td>\n",
       "      <td>0.222167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8660.479272</td>\n",
       "      <td>130109.666875</td>\n",
       "      <td>0.489260</td>\n",
       "      <td>0.790560</td>\n",
       "      <td>0.521452</td>\n",
       "      <td>9.217424</td>\n",
       "      <td>1.125331</td>\n",
       "      <td>1.199812</td>\n",
       "      <td>1.201709</td>\n",
       "      <td>1.170630</td>\n",
       "      <td>...</td>\n",
       "      <td>64572.844994</td>\n",
       "      <td>60878.153831</td>\n",
       "      <td>59355.284889</td>\n",
       "      <td>16757.718059</td>\n",
       "      <td>2.134743e+04</td>\n",
       "      <td>17973.951980</td>\n",
       "      <td>15162.056345</td>\n",
       "      <td>15251.828322</td>\n",
       "      <td>18222.046645</td>\n",
       "      <td>0.415711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-65167.000000</td>\n",
       "      <td>-61372.000000</td>\n",
       "      <td>-339603.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7467.750000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2310.000000</td>\n",
       "      <td>1744.250000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>8.150000e+02</td>\n",
       "      <td>390.000000</td>\n",
       "      <td>281.750000</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>110.750000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>14975.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>19032.000000</td>\n",
       "      <td>18019.000000</td>\n",
       "      <td>16812.500000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2.010000e+03</td>\n",
       "      <td>1801.500000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>22460.250000</td>\n",
       "      <td>240000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>54591.500000</td>\n",
       "      <td>50237.250000</td>\n",
       "      <td>49132.750000</td>\n",
       "      <td>5009.000000</td>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4026.000000</td>\n",
       "      <td>4009.250000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>30000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>891586.000000</td>\n",
       "      <td>927171.000000</td>\n",
       "      <td>961664.000000</td>\n",
       "      <td>873552.000000</td>\n",
       "      <td>1.227082e+06</td>\n",
       "      <td>896040.000000</td>\n",
       "      <td>621000.000000</td>\n",
       "      <td>426529.000000</td>\n",
       "      <td>528666.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID       LIMIT_BAL           SEX     EDUCATION      MARRIAGE  \\\n",
       "count  24000.000000    24000.000000  24000.000000  24000.000000  24000.000000   \n",
       "mean   14964.174292   167893.486667      1.603125      1.851958      1.553375   \n",
       "std     8660.479272   130109.666875      0.489260      0.790560      0.521452   \n",
       "min        1.000000    10000.000000      1.000000      0.000000      0.000000   \n",
       "25%     7467.750000    50000.000000      1.000000      1.000000      1.000000   \n",
       "50%    14975.000000   140000.000000      2.000000      2.000000      2.000000   \n",
       "75%    22460.250000   240000.000000      2.000000      2.000000      2.000000   \n",
       "max    30000.000000  1000000.000000      2.000000      6.000000      3.000000   \n",
       "\n",
       "                AGE         PAY_0         PAY_2         PAY_3         PAY_4  \\\n",
       "count  24000.000000  24000.000000  24000.000000  24000.000000  24000.000000   \n",
       "mean      35.488458     -0.017542     -0.135292     -0.170042     -0.224292   \n",
       "std        9.217424      1.125331      1.199812      1.201709      1.170630   \n",
       "min       21.000000     -2.000000     -2.000000     -2.000000     -2.000000   \n",
       "25%       28.000000     -1.000000     -1.000000     -1.000000     -1.000000   \n",
       "50%       34.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%       41.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max       79.000000      8.000000      8.000000      8.000000      8.000000   \n",
       "\n",
       "       ...      BILL_AMT4      BILL_AMT5      BILL_AMT6       PAY_AMT1  \\\n",
       "count  ...   24000.000000   24000.000000   24000.000000   24000.000000   \n",
       "mean   ...   43389.105625   40297.970375   38708.777542    5656.319917   \n",
       "std    ...   64572.844994   60878.153831   59355.284889   16757.718059   \n",
       "min    ...  -65167.000000  -61372.000000 -339603.000000       0.000000   \n",
       "25%    ...    2310.000000    1744.250000    1200.000000     990.000000   \n",
       "50%    ...   19032.000000   18019.000000   16812.500000    2100.000000   \n",
       "75%    ...   54591.500000   50237.250000   49132.750000    5009.000000   \n",
       "max    ...  891586.000000  927171.000000  961664.000000  873552.000000   \n",
       "\n",
       "           PAY_AMT2       PAY_AMT3       PAY_AMT4       PAY_AMT5  \\\n",
       "count  2.400000e+04   24000.000000   24000.000000   24000.000000   \n",
       "mean   5.910454e+03    5280.658708    4763.854250    4805.837667   \n",
       "std    2.134743e+04   17973.951980   15162.056345   15251.828322   \n",
       "min    0.000000e+00       0.000000       0.000000       0.000000   \n",
       "25%    8.150000e+02     390.000000     281.750000     234.000000   \n",
       "50%    2.010000e+03    1801.500000    1500.000000    1500.000000   \n",
       "75%    5.000000e+03    4600.000000    4026.000000    4009.250000   \n",
       "max    1.227082e+06  896040.000000  621000.000000  426529.000000   \n",
       "\n",
       "            PAY_AMT6  default.payment.next.month  \n",
       "count   24000.000000                24000.000000  \n",
       "mean     5277.577958                    0.222167  \n",
       "std     18222.046645                    0.415711  \n",
       "min         0.000000                    0.000000  \n",
       "25%       110.750000                    0.000000  \n",
       "50%      1500.000000                    0.000000  \n",
       "75%      4000.000000                    0.000000  \n",
       "max    528666.000000                    1.000000  \n",
       "\n",
       "[8 rows x 25 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial insights: we may want to get rid of the `ID` column (since it just seems to be an identifier). We may also consider getting rid of the `SEX` column, since we don't want our model to be biased towards a certain gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 24000 entries, 19682 to 19966\n",
      "Data columns (total 25 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   ID                          24000 non-null  int64  \n",
      " 1   LIMIT_BAL                   24000 non-null  float64\n",
      " 2   SEX                         24000 non-null  int64  \n",
      " 3   EDUCATION                   24000 non-null  int64  \n",
      " 4   MARRIAGE                    24000 non-null  int64  \n",
      " 5   AGE                         24000 non-null  int64  \n",
      " 6   PAY_0                       24000 non-null  int64  \n",
      " 7   PAY_2                       24000 non-null  int64  \n",
      " 8   PAY_3                       24000 non-null  int64  \n",
      " 9   PAY_4                       24000 non-null  int64  \n",
      " 10  PAY_5                       24000 non-null  int64  \n",
      " 11  PAY_6                       24000 non-null  int64  \n",
      " 12  BILL_AMT1                   24000 non-null  float64\n",
      " 13  BILL_AMT2                   24000 non-null  float64\n",
      " 14  BILL_AMT3                   24000 non-null  float64\n",
      " 15  BILL_AMT4                   24000 non-null  float64\n",
      " 16  BILL_AMT5                   24000 non-null  float64\n",
      " 17  BILL_AMT6                   24000 non-null  float64\n",
      " 18  PAY_AMT1                    24000 non-null  float64\n",
      " 19  PAY_AMT2                    24000 non-null  float64\n",
      " 20  PAY_AMT3                    24000 non-null  float64\n",
      " 21  PAY_AMT4                    24000 non-null  float64\n",
      " 22  PAY_AMT5                    24000 non-null  float64\n",
      " 23  PAY_AMT6                    24000 non-null  float64\n",
      " 24  default.payment.next.month  24000 non-null  int64  \n",
      "dtypes: float64(13), int64(12)\n",
      "memory usage: 4.8 MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the values in the `Non-Null`, it seems that no column is missing any values. We most likely won't need to impute values, but having an imputation step won't hurt (in case there are some missing values in the test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, -1,  1,  4,  2, -2,  8,  3,  5,  6,  7])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"PAY_0\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Include at least two summary statistics and two visualizations that you find useful, and accompany each one with a sentence explaining it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: which summary stats to choose from .describe()?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting code below is inspired by the similar histogram question in hw2\n",
    "def plot_single_hist(x_list, bins, labels, title, xlabel, alpha=1.0, histtype=\"bar\"):\n",
    "    for i in range(len(x_list)):\n",
    "        plt.hist(x=x_list[i], bins=bins, alpha=alpha, label=labels[i], histtype=histtype)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, -1,  1,  4,  2, -2,  8,  3,  5,  6,  7])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[\"PAY_0\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram on target values to show class dominance\n",
    "# histograms on PAY_[0-6] and PAY_AMT[0-6] and their target values\n",
    "# histogram of limit balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot histogram of the target class variable to check for class imbalance\n",
    "plot_single_hist([y_train], 2, [\"Default Status\"], \"Histogram of Default Statuses\", \"Default Status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram shows the distribution of the values of the target class variable which represents whether a client defaulted or not. From this histogram, we can observe that there is a clear class imbalance as there are many more examples of the negative class than of the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# retrieve all examples with negative and positive class labels separately\n",
    "negative_examples = train_df[train_df[\"default.payment.next.month\"] == 0]\n",
    "positive_examples = train_df[train_df[\"default.payment.next.month\"] == 1]\n",
    "\n",
    "# plot histogram of Repayment Status in Sept 2005 for both target classes\n",
    "plot_single_hist(\n",
    "    [negative_examples[\"PAY_0\"], positive_examples[\"PAY_0\"]],\n",
    "    10,\n",
    "    [\"Repayment Status (September, 2005) for Non-Defaults\",\n",
    "     \"Repayment Status (September, 2005) for Defaults\"],\n",
    "    \"Histogram of Repayment Statuses (September, 2005)\",\n",
    "    \"Repayment Status\",\n",
    "    0.5,\n",
    "    \"barstacked\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram shows the repayment status of clients in Sept, 2005 for both non-default and defaulted clients. A quick observation from this histogram is that clients that had more months of payment delays tend to default more than the clients that have less or zero payment delays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Summarize your initial observations about the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this dataset, the positive class is when a client defaults, and the negative class is when a client does not default.\n",
    "- False negatives (failing to predict default) could be more damaging, because the bank will not be able to recover the money they loaned the client in the form of credit.\n",
    "   - In this case, measuring recall could be important because we do not want someone who will default to slip by the predictions (e.g. we want to minimize the number of false negatives - increase recall)\n",
    "   - However, we also want to ensure that we favour predicting default correctly, therefore, prediction is also important.\n",
    "- There is class imbalance, where non-default examples dominate. We can also see this by noticing that the mean of `default.payment.next.month` is closer to 0 than 1\n",
    "   - Therefore, accuracy is likely not a good metric for assessment   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Pick appropriate metric/metrics for assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will primarily use **f1** as a metric for assessment.\n",
    "- While recall is important as it allows us to measure the proportion of positive examples that are identified, we also need to ensure that our model correctly identifies positive examples, measured using precision. \n",
    "- This is because it is possible for our model to be extremely cautious and identify every client as likely to default, predicting all examples as positive; \n",
    "    - in such case, some predictions will undoubtedly be true positives (meaning TP > 0), and because we did not predict any negative cases, FN and TN will both be 0. \n",
    "    - Recall = TP / (TP + FN), so if TP > 0 and FN = 0, it means that recall = 1 when every client is predicted to have high default risk. \n",
    "    - If everyone is positive, then no one is positive, and our predictions would be meaningless. \n",
    "- We need to strike a balance between predicting positive samples (recall) and _correctly_ predicting positive samples (precision) - f1 would allow us to do that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) 4. Feature engineering <a name=\"4\"></a>\n",
    "<hr>\n",
    "rubric={points:1}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Carry out feature engineering. In other words, extract new features relevant for the problem and work with your new feature set in the following exercises. You may have to go back and forth between feature engineering and preprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing and transformations <a name=\"5\"></a>\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Identify different feature types and the transformations you would apply on each feature type. \n",
    "2. Define a column transformer, if necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Identify different feature types and the transformations you would apply on each feature type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNote that we do not need ordinal encoder for PAY_[0-6] because the encoder will just change the ordering to integers, which is ordering that the columns already have.\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_features = [\n",
    "    \"ID\",\n",
    "    \"SEX\"\n",
    "]\n",
    "numeric_features = [\n",
    "    \"LIMIT_BAL\",\n",
    "    \"AGE\", \n",
    "    \"BILL_AMT1\",\n",
    "    \"BILL_AMT2\",\n",
    "    \"BILL_AMT3\",\n",
    "    \"BILL_AMT4\",\n",
    "    \"BILL_AMT5\",\n",
    "    \"BILL_AMT6\",\n",
    "    \"PAY_AMT1\",\n",
    "    \"PAY_AMT2\",\n",
    "    \"PAY_AMT3\",\n",
    "    \"PAY_AMT4\",\n",
    "    \"PAY_AMT5\",\n",
    "    \"PAY_AMT6\",\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"EDUCATION\",\n",
    "    \"MARRIAGE\",\n",
    "    \"PAY_0\",\n",
    "    \"PAY_2\",\n",
    "    \"PAY_3\",\n",
    "    \"PAY_4\",\n",
    "    \"PAY_5\",\n",
    "    \"PAY_6\",\n",
    "]\n",
    "\"\"\"\n",
    "Note that we do not need ordinal encoder for PAY_[0-6] because the encoder will just change the ordering to integers, which is ordering that the columns already have.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Define a column transformer, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "preprocessor = make_column_transformer(\n",
    "    (\"drop\", drop_features),\n",
    "    (numeric_transformer, numeric_features),\n",
    "    (categorical_transformer, categorical_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Baseline model <a name=\"6\"></a>\n",
    "<hr>\n",
    "\n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Try `scikit-learn`'s baseline model and report results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = [\n",
    "    \"accuracy\",\n",
    "    \"f1\",\n",
    "    \"recall\",\n",
    "    \"precision\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002852</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.777917</td>\n",
       "      <td>0.777824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004113</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.777917</td>\n",
       "      <td>0.777824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004891</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>0.777917</td>\n",
       "      <td>0.777824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005005</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.777917</td>\n",
       "      <td>0.777824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003315</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.777917</td>\n",
       "      <td>0.777824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.003510</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.777917</td>\n",
       "      <td>0.777824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.003309</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.777917</td>\n",
       "      <td>0.777824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.002983</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.777917</td>\n",
       "      <td>0.777824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.002920</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.777500</td>\n",
       "      <td>0.777870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.004509</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.777500</td>\n",
       "      <td>0.777870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_score  train_score\n",
       "0  0.002852    0.000419    0.777917     0.777824\n",
       "1  0.004113    0.000612    0.777917     0.777824\n",
       "2  0.004891    0.000869    0.777917     0.777824\n",
       "3  0.005005    0.000933    0.777917     0.777824\n",
       "4  0.003315    0.001018    0.777917     0.777824\n",
       "5  0.003510    0.000500    0.777917     0.777824\n",
       "6  0.003309    0.000499    0.777917     0.777824\n",
       "7  0.002983    0.000501    0.777917     0.777824\n",
       "8  0.002920    0.000472    0.777500     0.777870\n",
       "9  0.004509    0.000876    0.777500     0.777870"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = DummyClassifier()\n",
    "dummy_scores = cross_validate(dummy, X_train, y_train, cv=10, return_train_score=True)\n",
    "pd.DataFrame(dummy_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean train accuracy = 0.7778333333333333\n",
      "Mean validation accuracy = 0.7778333333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean train accuracy =\", dummy_scores[\"train_score\"].mean())\n",
    "print(\"Mean validation accuracy =\", dummy_scores[\"test_score\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train/validation accuracies of the baseline model is high, but because default cases are rare and there are many more examples of non-default cases, accuracy is not an informative metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Linear models <a name=\"7\"></a>\n",
    "<hr>\n",
    "rubric={points:12}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Try logistic regression as a first real attempt. \n",
    "2. Carry out hyperparameter tuning to explore different values for the complexity hyperparameter `C`. \n",
    "3. Report validation scores along with standard deviation. \n",
    "4. Summarize your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Try logistic regression as a first real attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly pasted from HW3's provided code\n",
    "def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns mean and std of cross validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model :\n",
    "        scikit-learn model\n",
    "    X_train : numpy array or pandas DataFrame\n",
    "        X in the training data\n",
    "    y_train :\n",
    "        y in the training data\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        pandas Series with mean scores from cross_validation\n",
    "    \"\"\"\n",
    "\n",
    "    scores = cross_validate(model, X_train, y_train, **kwargs)\n",
    "\n",
    "    mean_scores = pd.DataFrame(scores).mean()\n",
    "    std_scores = pd.DataFrame(scores).std()\n",
    "    out_col = []\n",
    "\n",
    "    for i in range(len(mean_scores)):\n",
    "        out_col.append((f\"%0.6f (+/- %0.6f)\" % (mean_scores[i], std_scores[i])))\n",
    "\n",
    "    return pd.Series(data=out_col, index=mean_scores.index)\n",
    "\n",
    "def get_mean(text, delimiter):\n",
    "    return float(text.split(delimiter, 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>train_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.807464 (+/- 0.047429)</td>\n",
       "      <td>0.014016 (+/- 0.003823)</td>\n",
       "      <td>0.820083 (+/- 0.005457)</td>\n",
       "      <td>0.821653 (+/- 0.000613)</td>\n",
       "      <td>0.466374 (+/- 0.018640)</td>\n",
       "      <td>0.471249 (+/- 0.002402)</td>\n",
       "      <td>0.354089 (+/- 0.017980)</td>\n",
       "      <td>0.357735 (+/- 0.002356)</td>\n",
       "      <td>0.683955 (+/- 0.024612)</td>\n",
       "      <td>0.690297 (+/- 0.002378)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  fit_time               score_time            test_accuracy  \\\n",
       "0  0.807464 (+/- 0.047429)  0.014016 (+/- 0.003823)  0.820083 (+/- 0.005457)   \n",
       "\n",
       "            train_accuracy                  test_f1                 train_f1  \\\n",
       "0  0.821653 (+/- 0.000613)  0.466374 (+/- 0.018640)  0.471249 (+/- 0.002402)   \n",
       "\n",
       "               test_recall             train_recall           test_precision  \\\n",
       "0  0.354089 (+/- 0.017980)  0.357735 (+/- 0.002356)  0.683955 (+/- 0.024612)   \n",
       "\n",
       "           train_precision  \n",
       "0  0.690297 (+/- 0.002378)  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_pipe = make_pipeline(preprocessor, LogisticRegression(max_iter=2000))\n",
    "lr_results = mean_std_cross_val_scores(\n",
    "    lr_pipe, X_train, y_train, cv=10, return_train_score=True, scoring=scoring, \n",
    ")\n",
    "pd.DataFrame(lr_results).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Carry out hyperparameter tuning to explore different values for the complexity hyperparameter `C` & 7.3 Report validation scores along with standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we initially tried a `C` range of [0.0001, 10000] (`10.0 ** np.arange(-4, 6, 1)`). This was inspired by the range we used in lecture 7 during the hyperparameter tuning of `C`; however, the runtime of this range of particularly slow, and we found that \n",
    "- `C=0.0001` resulted in a precision, recall, and f1 scores of 0.0\n",
    "- For `C<0.01` and `C>1000`, the f1 scores didn't seem to improve. \n",
    "\n",
    "**Therefore, to reduce runtime, we have chosen to tune C with a smaller range of [0.1, 1000] (`10.0 ** np.arange(-1, 4, 1)`).**\n",
    "The `C` with the best performance stayed consistent after we reduced the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>train_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>0.414843 (+/- 0.051817)</td>\n",
       "      <td>0.014696 (+/- 0.001855)</td>\n",
       "      <td>0.820042 (+/- 0.004924)</td>\n",
       "      <td>0.820889 (+/- 0.000602)</td>\n",
       "      <td>0.465635 (+/- 0.017848)</td>\n",
       "      <td>0.468959 (+/- 0.002986)</td>\n",
       "      <td>0.353152 (+/- 0.017887)</td>\n",
       "      <td>0.355985 (+/- 0.003156)</td>\n",
       "      <td>0.684354 (+/- 0.022037)</td>\n",
       "      <td>0.687000 (+/- 0.001878)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.850398 (+/- 0.067259)</td>\n",
       "      <td>0.013829 (+/- 0.001040)</td>\n",
       "      <td>0.820083 (+/- 0.005457)</td>\n",
       "      <td>0.821653 (+/- 0.000613)</td>\n",
       "      <td>0.466374 (+/- 0.018640)</td>\n",
       "      <td>0.471249 (+/- 0.002402)</td>\n",
       "      <td>0.354089 (+/- 0.017980)</td>\n",
       "      <td>0.357735 (+/- 0.002356)</td>\n",
       "      <td>0.683955 (+/- 0.024612)</td>\n",
       "      <td>0.690297 (+/- 0.002378)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>1.563020 (+/- 0.237691)</td>\n",
       "      <td>0.013821 (+/- 0.001822)</td>\n",
       "      <td>0.820333 (+/- 0.005457)</td>\n",
       "      <td>0.822046 (+/- 0.000617)</td>\n",
       "      <td>0.467088 (+/- 0.019232)</td>\n",
       "      <td>0.472481 (+/- 0.002407)</td>\n",
       "      <td>0.354652 (+/- 0.018890)</td>\n",
       "      <td>0.358715 (+/- 0.002343)</td>\n",
       "      <td>0.685039 (+/- 0.024393)</td>\n",
       "      <td>0.691936 (+/- 0.002413)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.0</th>\n",
       "      <td>2.595462 (+/- 0.302564)</td>\n",
       "      <td>0.014238 (+/- 0.002923)</td>\n",
       "      <td>0.820333 (+/- 0.005503)</td>\n",
       "      <td>0.822074 (+/- 0.000558)</td>\n",
       "      <td>0.466965 (+/- 0.019181)</td>\n",
       "      <td>0.472360 (+/- 0.002376)</td>\n",
       "      <td>0.354465 (+/- 0.018671)</td>\n",
       "      <td>0.358485 (+/- 0.002397)</td>\n",
       "      <td>0.685166 (+/- 0.024610)</td>\n",
       "      <td>0.692272 (+/- 0.002028)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000.0</th>\n",
       "      <td>3.258042 (+/- 0.260430)</td>\n",
       "      <td>0.013198 (+/- 0.001530)</td>\n",
       "      <td>0.820292 (+/- 0.005556)</td>\n",
       "      <td>0.822079 (+/- 0.000566)</td>\n",
       "      <td>0.466776 (+/- 0.019323)</td>\n",
       "      <td>0.472395 (+/- 0.002424)</td>\n",
       "      <td>0.354277 (+/- 0.018742)</td>\n",
       "      <td>0.358527 (+/- 0.002449)</td>\n",
       "      <td>0.685044 (+/- 0.024792)</td>\n",
       "      <td>0.692269 (+/- 0.002039)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       fit_time               score_time  \\\n",
       "0.1     0.414843 (+/- 0.051817)  0.014696 (+/- 0.001855)   \n",
       "1.0     0.850398 (+/- 0.067259)  0.013829 (+/- 0.001040)   \n",
       "10.0    1.563020 (+/- 0.237691)  0.013821 (+/- 0.001822)   \n",
       "100.0   2.595462 (+/- 0.302564)  0.014238 (+/- 0.002923)   \n",
       "1000.0  3.258042 (+/- 0.260430)  0.013198 (+/- 0.001530)   \n",
       "\n",
       "                  test_accuracy           train_accuracy  \\\n",
       "0.1     0.820042 (+/- 0.004924)  0.820889 (+/- 0.000602)   \n",
       "1.0     0.820083 (+/- 0.005457)  0.821653 (+/- 0.000613)   \n",
       "10.0    0.820333 (+/- 0.005457)  0.822046 (+/- 0.000617)   \n",
       "100.0   0.820333 (+/- 0.005503)  0.822074 (+/- 0.000558)   \n",
       "1000.0  0.820292 (+/- 0.005556)  0.822079 (+/- 0.000566)   \n",
       "\n",
       "                        test_f1                 train_f1  \\\n",
       "0.1     0.465635 (+/- 0.017848)  0.468959 (+/- 0.002986)   \n",
       "1.0     0.466374 (+/- 0.018640)  0.471249 (+/- 0.002402)   \n",
       "10.0    0.467088 (+/- 0.019232)  0.472481 (+/- 0.002407)   \n",
       "100.0   0.466965 (+/- 0.019181)  0.472360 (+/- 0.002376)   \n",
       "1000.0  0.466776 (+/- 0.019323)  0.472395 (+/- 0.002424)   \n",
       "\n",
       "                    test_recall             train_recall  \\\n",
       "0.1     0.353152 (+/- 0.017887)  0.355985 (+/- 0.003156)   \n",
       "1.0     0.354089 (+/- 0.017980)  0.357735 (+/- 0.002356)   \n",
       "10.0    0.354652 (+/- 0.018890)  0.358715 (+/- 0.002343)   \n",
       "100.0   0.354465 (+/- 0.018671)  0.358485 (+/- 0.002397)   \n",
       "1000.0  0.354277 (+/- 0.018742)  0.358527 (+/- 0.002449)   \n",
       "\n",
       "                 test_precision          train_precision  \n",
       "0.1     0.684354 (+/- 0.022037)  0.687000 (+/- 0.001878)  \n",
       "1.0     0.683955 (+/- 0.024612)  0.690297 (+/- 0.002378)  \n",
       "10.0    0.685039 (+/- 0.024393)  0.691936 (+/- 0.002413)  \n",
       "100.0   0.685166 (+/- 0.024610)  0.692272 (+/- 0.002028)  \n",
       "1000.0  0.685044 (+/- 0.024792)  0.692269 (+/- 0.002039)  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_values = 10.0 ** np.arange(-1, 4, 1)\n",
    "lr_C_results = {}\n",
    "highest_test_f1 = -1\n",
    "best_C = -1\n",
    "for c in C_values:\n",
    "#     print(\"Cross validating on\", c)\n",
    "    lr_C_pipe = make_pipeline(preprocessor, LogisticRegression(C=c, max_iter=2000))\n",
    "    lr_C_results[c] = mean_std_cross_val_scores(\n",
    "        lr_C_pipe, X_train, y_train, cv=10, return_train_score=True, scoring=scoring, \n",
    "    )\n",
    "    mean = get_mean(lr_C_results[c][\"test_f1\"], \" \")\n",
    "    if mean > highest_test_f1:\n",
    "        best_C = c\n",
    "        highest_test_f1 = mean\n",
    "pd.DataFrame(lr_C_results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameter C is 10.0 with a mean test_f1 value of 0.467088\n",
      "The difference between the f1-scores when C=10.0 and the logistic regression model in 7.1 with default C=1.0 test_f1=0.466374 is 0.000714\n"
     ]
    }
   ],
   "source": [
    "print(f'The best hyperparameter C is {best_C} with a mean test_f1 value of %f' % highest_test_f1)\n",
    "print(f'The difference between the f1-scores when C={best_C} and the logistic regression model in 7.1 with default C=1.0 test_f1={get_mean(lr_results[\"test_f1\"], \" \")} is %f' % (highest_test_f1 - get_mean(lr_results[\"test_f1\"], \" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Summarize your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the train and validation scores, the complexity parameter `C` with the best performance is 10.\n",
    "- Compared to the model with default complexity `C=1` in part 7.1, `C=10` resulted in a marginally higher validation f1 score.\n",
    "- However, we are not satisfied with the model with `C=10` because its f1 score is still quite low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Different classifiers <a name=\"8\"></a>\n",
    "<hr>\n",
    "rubric={points:15}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Try at least 3 other models aside from logistic regression. At least one of these models should be a tree-based ensemble model (e.g., lgbm, random forest, xgboost). \n",
    "2. Summarize your results. Can you beat logistic regression? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import additional models\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validating on decision_tree\n",
      "Cross validating on random_forest\n",
      "Cross validating on LightGBM\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>train_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>decision_tree</th>\n",
       "      <td>0.526248 (+/- 0.026633)</td>\n",
       "      <td>0.018989 (+/- 0.000700)</td>\n",
       "      <td>0.727333 (+/- 0.005234)</td>\n",
       "      <td>0.999010 (+/- 0.000184)</td>\n",
       "      <td>0.404174 (+/- 0.011638)</td>\n",
       "      <td>0.997768 (+/- 0.000416)</td>\n",
       "      <td>0.416351 (+/- 0.014984)</td>\n",
       "      <td>0.995686 (+/- 0.000917)</td>\n",
       "      <td>0.392799 (+/- 0.010474)</td>\n",
       "      <td>0.999859 (+/- 0.000129)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_forest</th>\n",
       "      <td>1.036007 (+/- 0.251081)</td>\n",
       "      <td>0.061041 (+/- 0.007328)</td>\n",
       "      <td>0.817458 (+/- 0.005902)</td>\n",
       "      <td>0.851958 (+/- 0.001798)</td>\n",
       "      <td>0.450760 (+/- 0.024900)</td>\n",
       "      <td>0.558294 (+/- 0.005548)</td>\n",
       "      <td>0.337580 (+/- 0.024235)</td>\n",
       "      <td>0.421136 (+/- 0.005726)</td>\n",
       "      <td>0.679354 (+/- 0.022479)</td>\n",
       "      <td>0.828116 (+/- 0.011326)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>0.298830 (+/- 0.035720)</td>\n",
       "      <td>0.028605 (+/- 0.003047)</td>\n",
       "      <td>0.819292 (+/- 0.007334)</td>\n",
       "      <td>0.849437 (+/- 0.001896)</td>\n",
       "      <td>0.477440 (+/- 0.022603)</td>\n",
       "      <td>0.566434 (+/- 0.005019)</td>\n",
       "      <td>0.371713 (+/- 0.020004)</td>\n",
       "      <td>0.442704 (+/- 0.005681)</td>\n",
       "      <td>0.667780 (+/- 0.030302)</td>\n",
       "      <td>0.786342 (+/- 0.011681)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              fit_time               score_time  \\\n",
       "decision_tree  0.526248 (+/- 0.026633)  0.018989 (+/- 0.000700)   \n",
       "random_forest  1.036007 (+/- 0.251081)  0.061041 (+/- 0.007328)   \n",
       "LightGBM       0.298830 (+/- 0.035720)  0.028605 (+/- 0.003047)   \n",
       "\n",
       "                         test_accuracy           train_accuracy  \\\n",
       "decision_tree  0.727333 (+/- 0.005234)  0.999010 (+/- 0.000184)   \n",
       "random_forest  0.817458 (+/- 0.005902)  0.851958 (+/- 0.001798)   \n",
       "LightGBM       0.819292 (+/- 0.007334)  0.849437 (+/- 0.001896)   \n",
       "\n",
       "                               test_f1                 train_f1  \\\n",
       "decision_tree  0.404174 (+/- 0.011638)  0.997768 (+/- 0.000416)   \n",
       "random_forest  0.450760 (+/- 0.024900)  0.558294 (+/- 0.005548)   \n",
       "LightGBM       0.477440 (+/- 0.022603)  0.566434 (+/- 0.005019)   \n",
       "\n",
       "                           test_recall             train_recall  \\\n",
       "decision_tree  0.416351 (+/- 0.014984)  0.995686 (+/- 0.000917)   \n",
       "random_forest  0.337580 (+/- 0.024235)  0.421136 (+/- 0.005726)   \n",
       "LightGBM       0.371713 (+/- 0.020004)  0.442704 (+/- 0.005681)   \n",
       "\n",
       "                        test_precision          train_precision  \n",
       "decision_tree  0.392799 (+/- 0.010474)  0.999859 (+/- 0.000129)  \n",
       "random_forest  0.679354 (+/- 0.022479)  0.828116 (+/- 0.011326)  \n",
       "LightGBM       0.667780 (+/- 0.030302)  0.786342 (+/- 0.011681)  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code is inspired from Lecture 11 code\n",
    "\n",
    "# create model pipelines\n",
    "pipe_dt = make_pipeline(preprocessor, DecisionTreeClassifier(random_state=123))\n",
    "pipe_rf = make_pipeline(\n",
    "    preprocessor,\n",
    "    RandomForestClassifier(random_state=123, n_jobs=-1, max_depth=10)\n",
    ")\n",
    "pipe_lgbm = make_pipeline(preprocessor, LGBMClassifier(random_state=123, verbosity=-1))\n",
    "\n",
    "# create dict mapping model name to the corresponding pipeline\n",
    "classifiers = {\n",
    "    \"decision_tree\": pipe_dt,\n",
    "    \"random_forest\": pipe_rf,\n",
    "    \"LightGBM\": pipe_lgbm,\n",
    "}\n",
    "\n",
    "# run cross-validation on all models\n",
    "results = {}\n",
    "for (name, pipeline) in classifiers.items():\n",
    "    print(\"Cross validating on\", name)\n",
    "    results[name] = mean_std_cross_val_scores(\n",
    "#         pipeline, X_train, y_train, return_train_score=True, scoring=\"recall\"\n",
    "        pipeline, X_train, y_train, return_train_score=True, scoring=scoring\n",
    "    )\n",
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: summarize results and compare to LR\n",
    "\n",
    "seems like all 3 models perform better than LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) 9. Feature selection <a name=\"9\"></a>\n",
    "<hr>\n",
    "rubric={points:1}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Make some attempts to select relevant features. You may try `RFECV` or forward selection. Do the results improve with feature selection? Summarize your results. If you see improvements in the results, keep feature selection in your pipeline. If not, you may abandon it in the next exercises. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Hyperparameter optimization <a name=\"10\"></a>\n",
    "<hr>\n",
    "rubric={points:15}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Make some attempts to optimize hyperparameters for the models you've tried and summarize your results. You may pick one of the best performing models from the previous exercise and tune hyperparameters only for that model. You may use `sklearn`'s methods for hyperparameter optimization or fancier Bayesian optimization methods. \n",
    "  - [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)   \n",
    "  - [RandomizedSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "  - [scikit-optimize](https://github.com/scikit-optimize/scikit-optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing model is LightGBM. We will carry out hyperparameter optimization of the following hyperparameters:\n",
    "- `max_depth` - controls the overall depth of the tree\n",
    "   - Having more levels could lead to a more complex model that is prone to overfitting.\n",
    "   - We have chosen the range of values to be [3, 13] with increments of 2. This is because...\n",
    "- `num_leaves` - controls the number of decision leaves in a single tree\n",
    "   - `num_leaves` depends on the `max_depth`. Acording to the LGBM documentation, `num_leaves` should theoretically be `2^(max_depth)`, however, sticking to this max value could lead to overfitting.\n",
    "   - LGBM documentation suggests letting `num_leaves` to be smaller; for example, when `max_depth=7`, the max value of `num_leaves` would be `2^7=127`, but we should set it to `70` or `80`. \n",
    "   - Therefore, for the first two `max_depth=3` and `5`, we let `num_leaves` be `2^(max_depth)`, but for `max_depth>5`, we don't strictly stick to the formula `2^(max_depth)`, but instead use `num_leaves=10*max_depth`.\n",
    "- `min_data_in_leaf` - specifies the minimum number of of observations that fit the decision criteria in a leaf.\n",
    "   - The optimal value depends on the size of the dataset and `num_leaves`.\n",
    "   - For a large dataset, the LGBM documentation advises setting it to hundreds or thousands.\n",
    "\n",
    "\n",
    "Sources: \n",
    "- [LightGBM Docs | Parameters Tuning](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html)\n",
    "- [towards data science | LightGBM Hyperparameter Tuning](https://towardsdatascience.com/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score\n",
    "scorers = {\n",
    "    \"f1\": make_scorer(f1_score),\n",
    "    \"precision\": make_scorer(precision_score),\n",
    "    \"recall\": make_scorer(recall_score),    \n",
    "}\n",
    "param_grid = {\n",
    "    \"lgbmclassifier__max_depth\": [3, 5, 7, 9, 11, 13],\n",
    "    \"lgbmclassifier__num_leaves\": [8, 32, 70, 90, 130],\n",
    "    \"lgbmclassifier__min_data_in_leaf\": [100, 300, 500, 700, 900, 1100],\n",
    "}\n",
    "# Optimization code from lecture 8\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipe_lgbm, param_distributions=param_grid, n_jobs=-1, n_iter=10, cv=10, scoring=scorers, random_state=123, refit=\"f1\", verbose=-1\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "pd.DataFrame(random_search.cv_results_)[\n",
    "    [\n",
    "        \"mean_test_f1\",\n",
    "        \"mean_test_recall\",\n",
    "        \"mean_test_precision\",\n",
    "        \"param_lgbmclassifier__min_data_in_leaf\",\n",
    "        \"param_lgbmclassifier__max_depth\",\n",
    "        \"param_lgbmclassifier__num_leaves\",\n",
    "        \"mean_fit_time\",\n",
    "        \"rank_test_f1\",\n",
    "    ]\n",
    "].set_index(\"rank_test_f1\").sort_index().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score from RandomizedSearchCV is 0.47894642069457116\n"
     ]
    }
   ],
   "source": [
    "print(f'The best score from RandomizedSearchCV is {random_search.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    pipe_lgbm, param_grid=param_grid, n_jobs=-1, cv=10, scoring=scorers, refit=\"f1\"\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "pd.DataFrame(grid_search.cv_results_)[\n",
    "    [\n",
    "        \"mean_test_f1\",\n",
    "        \"mean_test_recall\",\n",
    "        \"mean_test_precision\",\n",
    "        \"param_lgbmclassifier__min_data_in_leaf\",\n",
    "        \"param_lgbmclassifier__max_depth\",\n",
    "        \"param_lgbmclassifier__num_leaves\",\n",
    "        \"mean_fit_time\",\n",
    "        \"rank_test_f1\",\n",
    "    ]\n",
    "].set_index(\"rank_test_f1\").sort_index().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score from RandomizedSearchCV is 0.48031841869944814\n"
     ]
    }
   ],
   "source": [
    "print(f'The best score from RandomizedSearchCV is {grid_search.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerations/things we've tried\n",
    "- We tried using larger values of `max_depth`, `num_leaves`, `min_data_in_leaf` separately, but they all resulted in a lower max `mean_test_f1`\n",
    "- As recommended by LGBM docs, we also tried increasing `max_bin` from its default `255` to `260` and above, but it also resulted in a lower f1 score.\n",
    "- We also tried decreasing the number of folds to 5 in hopes of increasing the sample size of each fold. This also resulted in a lower f1 score.\n",
    "\n",
    "Summary\n",
    "- `GridSearchCV` yielded better results for the f1 score _and_ was faster, which was a bit surprising - we thought that `RandomizedSearchCV` would be faster, because in `RandomizedSearchCV`, adding parameters that do not influence performance does not affect efficiency\n",
    "- We will use the hyperparameter values produced by `GridSearchCV` since it yielded a slightly higher mean f1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Interpretation and feature importances <a name=\"1\"></a>\n",
    "<hr>\n",
    "rubric={points:15}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Use the methods we saw in class (e.g., `eli5`, `shap`) (or any other methods of your choice) to explain feature importances of one of the best performing models. Summarize your observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Results on the test set <a name=\"12\"></a>\n",
    "<hr>\n",
    "\n",
    "rubric={points:5}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Try your best performing model on the test data and report test scores. \n",
    "2. Do the test scores agree with the validation scores from before? To what extent do you trust your results? Do you think you've had issues with optimization bias? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) 13. Explaining predictions \n",
    "rubric={points:1}\n",
    "\n",
    "**Your tasks**\n",
    "\n",
    "1. Take one or two test predictions and explain them with SHAP force plots.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary of results <a name=\"13\"></a>\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Report your final test score along with the metric you used. \n",
    "2. Write concluding remarks.\n",
    "3. Discuss other ideas that you did not try but could potentially improve the performance/interpretability . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.3 Discuss other ideas that you did not try but could potentially improve the performance/interpretability ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to hyperparameter optimization on our best-performing model, we could try fancier methods such as `scikit-optimise` to predict what hyperparameters will be good. Since there were several hyperparameters and many possible values to tune, this could potentially give us some parameters that lead to a better-performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions \n",
    "\n",
    "**PLEASE READ:** When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from “1” will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using Gradescope's drag and drop tool. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
